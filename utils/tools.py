import os
import shutil
import subprocess
import logging
import hashlib
import re
import json
import ast
import zipfile
from datetime import datetime
from typing import Dict, Tuple, List, Any

logger = logging.getLogger("GortexTools")

def get_file_hash(path: str) -> str:
    """íŒŒì¼ ë‚´ìš©ì˜ MD5 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        if not os.path.exists(path):
            return ""
        with open(path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception:
        return ""

def write_file_with_hash(path: str, content: str) -> Tuple[str, str]:
    """íŒŒì¼ì„ ì‘ì„±í•˜ê³  ìƒˆë¡œìš´ í•´ì‹œë¥¼ ë°˜í™˜í•˜ëŠ” í†µí•© í•¨ìˆ˜."""
    write_result = write_file(path, content)
    new_hash = get_file_hash(path)
    return write_result, new_hash

def write_file(path: str, content: str) -> str:
    """ì•ˆì „í•œ ì›ìì  íŒŒì¼ ì“°ê¸° ë° ìë™ ë²„ì „ ì•„ì¹´ì´ë¹™."""
    try:
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
        if os.path.exists(path):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # 1. ì¼ë°˜ ë°±ì—…
            backup_dir = "logs/backups"
            os.makedirs(backup_dir, exist_ok=True)
            backup_path = os.path.join(backup_dir, f"{os.path.basename(path)}.{timestamp}.bak")
            shutil.copy2(path, backup_path)
            
            # 2. íƒ€ì„ë¨¸ì‹  ë²„ì „ ì•„ì¹´ì´ë¹™
            version_dir = os.path.join("logs/versions", path.replace("/", "_"))
            os.makedirs(version_dir, exist_ok=True)
            ext = os.path.splitext(path)[1]
            version_path = os.path.join(version_dir, f"v_{timestamp}{ext}")
            shutil.copy2(path, version_path)
            logger.info(f"ğŸ•°ï¸ File version archived: {version_path}")
        
        tmp_path = path + ".tmp"
        with open(tmp_path, 'w', encoding='utf-8') as f:
            f.write(content)
        os.replace(tmp_path, path)
        
        # [DISTRIBUTED SYNC] ë³€ê²½ ì‚¬í•­ ì „íŒŒ
        try:
            from gortex.core.mq import mq_bus
            if mq_bus.is_connected:
                file_hash = hashlib.md5(content.encode()).hexdigest()
                mq_bus.broadcast_file_change(path, content, file_hash)
                logger.debug(f"ğŸŒ Broadcasted file change: {path}")
        except Exception as sync_e:
            logger.warning(f"Failed to broadcast file change: {sync_e}")
            
        return f"Successfully wrote to {path}"
    except Exception as e:
        return f"Error writing file {path}: {str(e)}"

def list_files(directory: str = ".") -> str:
    """í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ íŒŒì¼ ëª©ë¡ ë°˜í™˜."""
    try:
        files = []
        ignore_dirs = {'.git', 'venv', '__pycache__', '.DS_Store', 'logs', 'site-packages'}
        for root, dirs, filenames in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            for f in filenames:
                if f in ignore_dirs:
                    continue
                rel_path = os.path.relpath(os.path.join(root, f), directory)
                if '.git' in rel_path:
                    continue
                files.append(rel_path)
        return "\n".join(sorted(files))
    except Exception as e:
        return f"Error: {str(e)}"

def execute_shell(command: str, timeout: int = 300) -> str:
    """ì…¸ ëª…ë ¹ì–´ ì•ˆì „ ì‹¤í–‰ ë° íŒŒì¼ ì‹œìŠ¤í…œ ë³€ê²½ ê°ì§€, ì˜ì¡´ì„± ìë™ ì—…ë°ì´íŠ¸."""
    forbidden_commands = ["rm -rf", "mkfs", "dd if=", ":(){ :|:& };:"]
    for cmd in forbidden_commands:
        if cmd in command:
            return f"âŒ Security Alert: Forbidden command detected ('{cmd}'). Execution blocked."

    try:
        # ì‹¤í–‰ ì „ ìŠ¤ëƒ…ìƒ·
        files_before = set(os.listdir("."))
        
        result = subprocess.run(
            command, shell=True, capture_output=True, text=True, timeout=timeout
        )
        
        # ì‹¤í–‰ í›„ ìŠ¤ëƒ…ìƒ·
        files_after = set(os.listdir("."))
        fs_changed = files_before != files_after

        # [AUTO-DEPENDENCY] pip install ê°ì§€ ì‹œ requirements.txt ì—…ë°ì´íŠ¸
        if "pip install" in command and result.returncode == 0:
            try:
                parts = command.split()
                if len(parts) >= 3:
                    package_name = parts[-1]
                    req_path = "requirements.txt"
                    existing_reqs = []
                    if os.path.exists(req_path):
                        with open(req_path, "r") as f:
                            existing_reqs = [line.strip().split('==')[0].lower() for line in f if line.strip()]
                    
                    if package_name.lower() not in existing_reqs:
                        with open(req_path, "a") as f:
                            f.write(f"\n{package_name}")
                        logger.info(f"âœ… Automatically added '{package_name}' to requirements.txt")
                        
                        # [INTEGRITY] ì„œëª… ê°±ì‹  íŠ¸ë¦¬ê±°
                        try:
                            from gortex.utils.integrity import guard
                            guard.generate_master_signature()
                            logger.info("ğŸ›¡ï¸ Master system signature refreshed after environment change.")
                        except Exception: pass
            except Exception as e:
                logger.warning(f"Failed to update requirements.txt: {e}")

        def truncate(text: str, limit: int = 2000) -> str:
            if len(text) <= limit:
                return text
            return text[:1000] + "\n... <truncated> ...\n" + text[-1000:]

        output = f"Exit Code: {result.returncode}\nSTDOUT:\n{truncate(result.stdout)}"
        if result.stderr:
            output += f"\nSTDERR:\n{truncate(result.stderr)}"
        
        if fs_changed:
            output += "\n\n[SYSTEM HINT: File system has changed. Consider using 'list_files' or 'read_file' to update cache.]"
        return output
    except subprocess.TimeoutExpired:
        return "âŒ Error: Command timed out."
    except Exception as e:
        return f"âŒ Error: {str(e)}"

def read_file(path: str, limit: int = None, offset: int = 0) -> str:
    """íŒŒì¼ ë‚´ìš© ì½ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)."""
    try:
        if not os.path.exists(path):
            return f"Error: File not found at {path}"
        with open(path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        total_lines = len(lines)
        if offset > 0:
            lines = lines[offset:]
            
        truncated = False
        if limit is not None and len(lines) > limit:
            lines = lines[:limit]
            truncated = True
            
        content = "".join(lines)
        if truncated:
            content += f"\n... (truncated, total {total_lines} lines) ...\n(truncated)"
        return content
    except Exception as e:
        return f"Error: {str(e)}"

def deep_integrity_check(working_dir: str, current_cache: Dict[str, str]) -> Tuple[Dict[str, str], List[str]]:
    """í”„ë¡œì íŠ¸ ì „ì²´ íŒŒì¼ì˜ ë¬´ê²°ì„±ì„ ê²€ì‚¬í•˜ê³  ì—…ë°ì´íŠ¸ëœ ìºì‹œì™€ ë³€ê²½ëœ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    updated_cache = current_cache.copy()
    changed_files = []
    ignore_dirs = {'.git', 'venv', '__pycache__', '.DS_Store', 'logs', 'site-packages'}
    
    for root, dirs, filenames in os.walk(working_dir):
        dirs[:] = [d for d in dirs if d not in ignore_dirs]
        for f in filenames:
            file_path = os.path.join(root, f)
            rel_path = os.path.relpath(file_path, working_dir)
            actual_hash = get_file_hash(file_path)
            cached_hash = updated_cache.get(rel_path)
            if actual_hash != cached_hash:
                updated_cache[rel_path] = actual_hash
                changed_files.append(rel_path)
                
    deleted_files = []
    for path in list(updated_cache.keys()):
        if not os.path.exists(os.path.join(working_dir, path)):
            del updated_cache[path]
            deleted_files.append(path)
    return updated_cache, changed_files + [f"(deleted) {p}" for p in deleted_files]

def get_changed_files(working_dir: str, current_cache: Dict[str, str]) -> List[str]:
    """í˜„ì¬ ìºì‹œì™€ ëŒ€ì¡°í•˜ì—¬ ë³€ê²½ëœ íŒŒì¼ ëª©ë¡ë§Œ ì¶”ì¶œ"""
    changed = []
    ignore_dirs = {'.git', 'venv', '__pycache__', 'logs', 'site-packages'}
    for root, dirs, filenames in os.walk(working_dir):
        dirs[:] = [d for d in dirs if d not in ignore_dirs]
        for f in filenames:
            file_path = os.path.join(root, f)
            rel_path = os.path.relpath(file_path, working_dir)
            actual_hash = get_file_hash(file_path)
            if actual_hash != current_cache.get(rel_path):
                changed.append(rel_path)
    return list(set(changed))

def apply_patch(path: str, start_line: int, end_line: int, new_content: str) -> str:
    """íŒŒì¼ì˜ íŠ¹ì • ë²”ìœ„ë¥¼ ìƒˆë¡œìš´ ë‚´ìš©ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤."""
    try:
        if not os.path.exists(path):
            return f"Error: File not found at {path}"
        with open(path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        if start_line < 1 or end_line > len(lines) or start_line > end_line:
            return f"Error: Invalid line range {start_line}-{end_line}"
        new_lines = lines[:start_line-1] + [new_content + "\n"] + lines[end_line:]
        write_file(path, "".join(new_lines))
        return f"Successfully applied patch to {path}."
    except Exception as e:
        return f"Error applying patch: {str(e)}"

def register_new_node(node_name: str, function_name: str, file_name: str) -> str:
    """core/graph.pyë¥¼ ì •ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ë…¸ë“œë¥¼ ë“±ë¡í•©ë‹ˆë‹¤."""
    graph_path = "core/graph.py"
    try:
        with open(graph_path, "r", encoding='utf-8') as f:
            content = f.read()
        import_stmt = f"from gortex.agents.{file_name} import {function_name}\n"
        if import_stmt not in content:
            content = import_stmt + content
        node_stmt = f'    workflow.add_node("{node_name}", {function_name})\n'
        if node_stmt not in content:
            content = content.replace("# ë…¸ë“œ ì¶”ê°€", f"# ë…¸ë“œ ì¶”ê°€\n{node_stmt}")
        write_file(graph_path, content)
        return f"âœ… Registered node '{node_name}'. Reboot required."
    except Exception as e:
        return f"âŒ Failed: {e}"

def scan_security_risks(code: str) -> List[Dict[str, str]]:
    """ìƒì„±ëœ ì½”ë“œ ë‚´ì˜ ë³´ì•ˆ ì·¨ì•½ì  íŒ¨í„´ ìŠ¤ìº”"""
    risks = []
    patterns = [
        (r'''(?i)(password|passwd|secret|api_key|token)\s*=\s*['"].*['"]''', "Hardcoded Secret"),
        (r"eval\(", "Dangerous function: eval()"),
        (r"exec\(", "Dangerous function: exec()"),
        (r"os\.system\(", "Dangerous function: os.system()"),
        (r"subprocess\.Popen\(.*shell=True", "Subprocess with shell=True"),
        (r'''cursor\.execute\(f?['"].*\{.*}''', "Potential SQL Injection")
    ]
    for pattern, risk_type in patterns:
        if re.search(pattern, code):
            risks.append({"type": risk_type, "pattern": pattern})
    return risks

def archive_project_artifacts(project_name: str, version: str, files: List[str]) -> str:
    """í”„ë¡œì íŠ¸ ìƒì„±ë¬¼ë“¤ì„ ë²„ì „ë³„ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ì•„ì¹´ì´ë¹™"""
    try:
        archive_root = os.path.join("logs", "archives", project_name, version)
        os.makedirs(archive_root, exist_ok=True)
        moved_count = 0
        for f_path in files:
            if os.path.exists(f_path):
                dest = os.path.join(archive_root, os.path.basename(f_path))
                shutil.move(f_path, dest)
                moved_count += 1
        return f"âœ… Archived {moved_count} artifacts to {archive_root}"
    except Exception as e:
        return f"âŒ Archive failed: {e}"

def backup_file_with_rotation(file_path: str, backup_dir: str = "logs/backups", max_versions: int = 5) -> str:
    """íŒŒì¼ì„ ë°±ì—…í•˜ê³  ì˜¤ë˜ëœ ë²„ì „ì„ íšŒì „(ì‚­ì œ)ì‹œí‚´."""
    if not os.path.exists(file_path):
        return f"Error: Source file {file_path} not found."
    
    try:
        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.basename(file_path)
        backup_path = os.path.join(backup_dir, f"{base_name}.{timestamp}.bak")
        
        # ë°±ì—… ìƒì„±
        shutil.copy2(file_path, backup_path)
        
        # íšŒì „(Rotation) ë¡œì§: í•´ë‹¹ íŒŒì¼ì˜ ë°±ì—… ëª©ë¡ ì¡°íšŒ
        backups = [os.path.join(backup_dir, f) for f in os.listdir(backup_dir) if f.startswith(base_name) and f.endswith(".bak")]
        backups.sort(key=os.path.getmtime, reverse=True) # ìµœì‹ ìˆœ ì •ë ¬
        
        # max_versions ê°œìˆ˜ ì´ˆê³¼ë¶„ ì‚­ì œ
        if len(backups) > max_versions:
            for old_backup in backups[max_versions:]:
                os.remove(old_backup)
                logger.info(f"ğŸ—‘ï¸ Rotated old backup: {old_backup}")
                
        return f"Successfully backed up {file_path} to {backup_path}"
    except Exception as e:
        return f"Error during backup rotation: {e}"

def safe_bulk_delete(file_paths: List[str]) -> Dict[str, Any]:
    """ëŒ€ëŸ‰ì˜ íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì‚­ì œí•˜ê³  ê²°ê³¼ë¥¼ ë³´ê³ í•¨. í•µì‹¬ íŒŒì¼ ë³´í˜¸ ê¸°ëŠ¥ í¬í•¨."""
    results = {"success": [], "failed": [], "protected": []}
    
    # ì ˆëŒ€ ì‚­ì œí•˜ë©´ ì•ˆ ë˜ëŠ” ë³´í˜¸ íŒ¨í„´
    protected_patterns = ["experience", "shard", "trace_summary", "release_note", "MILESTONE"]
    
    for path in file_paths:
        if not os.path.exists(path):
            continue
            
        # ë³´í˜¸ ë¡œì§
        if any(p in path for p in protected_patterns):
            results["protected"].append(path)
            logger.warning(f"ğŸ›¡ï¸ Protected file deletion blocked: {path}")
            continue
            
        try:
            os.remove(path)
            results["success"].append(path)
        except Exception as e:
            results["failed"].append({"path": path, "error": str(e)})
            logger.error(f"Failed to delete {path}: {e}")
            
    logger.info(f"ğŸ§¹ Bulk cleanup: {len(results['success'])} deleted, {len(results['protected'])} protected.")
    return results

def repair_and_load_json(text: str) -> Dict[str, Any]:
    """
    ë¡œì»¬ LLMì´ ìƒì„±í•œ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•˜ê³  í”í•œ ì˜¤ë¥˜ë¥¼ ë³µêµ¬í•©ë‹ˆë‹¤.
    """
    if not text:
        return {}
    
    # 1. Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
    clean_text = re.sub(r"```json\n?|```\n?", "", text).strip()
    
    # 2. ì¶”ì¶œ ì‹œë„: ìµœëŒ€í•œ JSONì²˜ëŸ¼ ë³´ì´ëŠ” êµ¬ê°„ì„ ì°¾ìŒ
    # { ë˜ëŠ” [ ë¡œ ì‹œì‘í•˜ëŠ” ì§€ì ë¶€í„° ëê¹Œì§€ ì¶”ì¶œ (ë‹«ëŠ” ê´„í˜¸ê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ)
    match = re.search(r"(\{.*\}|\[.*\])", clean_text, re.DOTALL)
    if match:
        json_str = match.group(0)
    else:
        # ê´„í˜¸ê°€ ìŒìœ¼ë¡œ ì•ˆ ë§ì•„ë„ ì¼ë‹¨ ì‹œì‘ì ë¶€í„° ëê¹Œì§€ ì‹œë„
        match_start = re.search(r"(\{|\[).*", clean_text, re.DOTALL)
        json_str = match_start.group(0) if match_start else clean_text

    # 3. í™‘ë”°ì˜´í‘œë¥¼ ìŒë”°ì˜´í‘œë¡œ ë¨¼ì € ë³€í™˜ (í”í•œ ì˜¤ë¥˜)
    json_str = json_str.replace("'", '"')

    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        # 4. êµ¬ì¡°ì  ì˜¤ë¥˜ ë³µêµ¬ ì‹œë„
        try:
            # ë¶ˆì™„ì „í•œ ì¢…ë£Œ ì¤‘ê´„í˜¸ ë³´ì •
            open_braces = json_str.count("{")
            close_braces = json_str.count("}")
            if open_braces > close_braces:
                json_str += "}" * (open_braces - close_braces)
            
            # ë¶ˆì™„ì „í•œ ì¢…ë£Œ ëŒ€ê´„í˜¸ ë³´ì •
            open_brackets = json_str.count("[")
            close_brackets = json_str.count("]")
            if open_brackets > close_brackets:
                json_str += "]" * (open_brackets - close_brackets)
                
            # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±° (Trailing comma)
            json_str = re.sub(r",\s*(\}|\])", r"\1", json_str)
            
            return json.loads(json_str)
        except Exception as e:
            logger.error(f"JSON Recovery failed: {e}")
            return {}

def verify_patch_integrity(file_path: str) -> Dict[str, Any]:
    """
    ì ìš©ëœ íŒ¨ì¹˜ê°€ ì‹œìŠ¤í…œ ë¬´ê²°ì„±ì„ í•´ì¹˜ì§€ ì•ŠëŠ”ì§€ ê²€ì¦ (Syntax + Tests).
    """
    if not os.path.exists(file_path):
        return {"success": False, "reason": "File not found"}

    # 1. Syntax Check
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            ast.parse(f.read())
    except SyntaxError as e:
        return {"success": False, "reason": f"Syntax Error: {e}"}
    except Exception as e:
        return {"success": False, "reason": str(e)}

    # 2. Selective Test Execution
    # íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ” í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¸¡ (ì˜ˆ: core/auth.py -> tests/test_auth.py)
    base_name = os.path.basename(file_path).replace(".py", "")
    test_file = f"tests/test_{base_name}.py"
    
    if os.path.exists(test_file):
        res = execute_shell(f"python3 -m unittest {test_file}")
        if "OK" in res:
            return {"success": True, "details": "Syntax and Tests passed."}
        else:
            return {"success": False, "reason": "Tests failed after patch.", "output": res}
    
    return {"success": True, "details": "Syntax passed (No matching test found)."}

def package_release_candidate(version: str, output_dir: str = "logs/archives") -> str:
    """í˜„ì¬ ì•ˆì •ì ì¸ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë°°í¬ í›„ë³´(RC)ë¡œ íŒ¨í‚¤ì§•í•¨."""
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"Gortex_RC_{version}.zip")
    
    ignore_patterns = [
        ".git", "venv", "__pycache__", ".DS_Store", "site-packages", 
        "logs", ".idea", ".pytest_cache", "training_jobs", "gortex"
    ]
    
    return compress_directory(".", output_path, ignore_patterns=ignore_patterns)

def compress_directory(source_dir: str, output_path: str, ignore_patterns: List[str] = None) -> str:
    """ë””ë ‰í† ë¦¬ ì „ì²´ë¥¼ ZIP ì•„ì¹´ì´ë¸Œë¡œ ì••ì¶• (íŠ¹ì • íŒ¨í„´ ì œì™¸)"""
    if ignore_patterns is None:
        ignore_patterns = [".git", "venv", "__pycache__", ".DS_Store", "site-packages", "logs/archives"]
    try:
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(source_dir):
                dirs[:] = [d for d in dirs if d not in ignore_patterns]
                for file in files:
                    if any(p in file for p in ignore_patterns):
                        continue
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, source_dir)
                    zipf.write(full_path, rel_path)
        return f"âœ… Directory compressed to {output_path}"
    except Exception as e:
        return f"âŒ Compression failed: {e}"

def safe_json_extract(text: str) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ë¸”ë¡ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤."""
    if not text:
        return {}
    import re
    import json
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if not match:
        return {}
    json_str = match.group(0)
    try:
        return json.loads(json_str)
    except Exception:
        try:
            return json.loads(json_str.replace("'", '"'))
        except Exception:
            return {}
