import logging
import os
from typing import List, Any
from gortex.core.state import GortexState

from gortex.core.llm.summarizer import get_summarizer

logger = logging.getLogger("GortexMemory")

def compress_synapse(state: GortexState) -> GortexState:
    """
    ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆ ë•Œ LLMì„ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ì„ ì••ì¶•í•¨.
    """
    messages = state.get("messages", [])
    backend_type = os.getenv("LLM_BACKEND", "hybrid").lower()
    
    # ì„ê³„ê°’ ê²°ì •: Ollama(ë¡œì»¬)ì¸ ê²½ìš° ë” ì¼ì° ìš”ì•½ ì‹œì‘
    threshold = 8 if backend_type == "ollama" else 15
    
    if len(messages) < threshold:
        return state

    logger.info(f"ğŸ§  Synaptic Compression active (Threshold: {threshold})...")
    
    summarizer = get_summarizer()
    summary_text = summarizer.summarize(state)
    
    # ìƒˆë¡œìš´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    # 1. ì‹œìŠ¤í…œ ìš”ì•½ë³¸ ì£¼ì…
    new_messages = [("system", f"[PROJECT STATE SUMMARY]\n{summary_text}")]
    
    # 2. ìµœê·¼ ì¤‘ìš”í•œ ëŒ€í™” ë§¥ë½ ë³´ì¡´ (ìµœê·¼ 4ê°œ)
    if len(messages) > 4:
        new_messages.extend(messages[-4:])
    
    return {
        "messages": new_messages,
        "history_summary": summary_text
    }

def distill_messages_for_agent(state: GortexState, target_agent: str) -> str:
    """ë‹¤ìŒ ì—ì´ì „íŠ¸ì—ê²Œ í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•¨ (ì‹œëƒ…ìŠ¤ ì¦ë¥˜)"""
    messages = state.get("messages", [])
    if not messages: return ""
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    history = "\n".join([f"[{m[0]}]: {m[1]}" for m in messages if isinstance(m, tuple)])
    
    prompt = f"""You are the Synaptic Distiller. 
    Summarize the following chat history specifically for the '{target_agent}' agent.
    Extract only technical requirements, tool outputs, and constraints relevant to their role.
    Keep it extremely concise (under 150 words).
    
    [History]:
    {history[-4000:]}
    """
    try:
        from gortex.core.llm.factory import LLMFactory
        backend = LLMFactory.get_default_backend()
        summary = backend.generate("gemini-1.5-flash", [{"role": "user", "content": prompt}])
        logger.info(f"ğŸ§ª Synaptic Distillation for {target_agent} complete.")
        return summary.strip()
    except Exception as e:
        logger.error(f"Distillation failed: {e}")
        return "Failed to distill history."

class ContextPruner:
    """ë©”ì‹œì§€ì˜ ê°€ì¹˜ì™€ ê´€ë ¨ì„±ì„ ë¶„ì„í•˜ì—¬ ì„ ë³„ì ìœ¼ë¡œ ê°€ì§€ì¹˜ê¸°ë¥¼ ìˆ˜í–‰í•¨."""
    def __init__(self, state: GortexState):
        self.state = state
        self.messages = list(state.get("messages", []))
        self.pinned = state.get("pinned_messages", [])
        self.plan = state.get("plan", [])

    def get_semantic_scores(self, target_messages: List[Any]) -> List[float]:
        """AnalystAgentë¥¼ í†µí•´ ì‹œë§¨í‹± ê´€ë ¨ì„± ì ìˆ˜ íšë“"""
        from gortex.agents.analyst.base import AnalystAgent
        analyst = AnalystAgent()
        
        # ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
        formatted_msgs = []
        for m in target_messages:
            content = str(m[1] if isinstance(m, tuple) else m.content if hasattr(m, 'content') else str(m))
            formatted_msgs.append({"role": m[0] if isinstance(m, tuple) else "ai", "content": content})
            
        return analyst.rank_context_relevance(formatted_msgs, self.plan)

    def prune(self, target_count: int = 15) -> List[Any]:
        """ì‹œë§¨í‹± ê´€ë ¨ì„±ì´ ë‚®ì€ ë©”ì‹œì§€ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì œê±°"""
        if len(self.messages) <= target_count:
            return self.messages
            
        logger.info(f"âœ‚ï¸ Semantic Pruning: {len(self.messages)} -> {target_count} messages.")
        
        # ë¬´ì¡°ê±´ ë³´ì¡´ ëŒ€ìƒ: ì²« ë²ˆì§¸ ë©”ì‹œì§€, ìµœì‹  4ê°œ ë©”ì‹œì§€
        
        # í‰ê°€ ëŒ€ìƒ ì¸ë±ìŠ¤ ì¶”ì¶œ
        eval_indices = [i for i in range(1, len(self.messages)-4)]
        eval_messages = [self.messages[i] for i in eval_indices]
        
        # ì‹œë§¨í‹± ì ìˆ˜ íšë“
        scores = self.get_semantic_scores(eval_messages)
        
        eval_list = []
        for idx, i_orig in enumerate(eval_indices):
            # ì‹œë§¨í‹± ì ìˆ˜ + ìµœì‹ ì„± ë³´ë„ˆìŠ¤
            final_score = scores[idx] + (i_orig / len(self.messages) * 0.2)
            eval_list.append({"index": i_orig, "score": final_score})
            
        # ì ìˆ˜ ë‚®ì€ ìˆœ ì •ë ¬ í›„ ì‚­ì œ ëŒ€ìƒ ì„ ì •
        eval_list.sort(key=lambda x: x["score"])
        
        remove_count = len(self.messages) - target_count
        to_remove_indices = {e["index"] for e in eval_list[:remove_count]}
        
        # [MEMORY CONSOLIDATION] ì‚­ì œ ì „ ê³ ê°€ì¹˜ ë©”ì‹œì§€ ë°±ì—…
        from gortex.utils.vector_store import LongTermMemory
        ltm = LongTermMemory()
        for e in eval_list[:remove_count]:
            if e["score"] > 0.7: # ë¹„ë¡ ìˆœìœ„ìƒ ì‚­ì œë˜ì§€ë§Œ ì ˆëŒ€ì  ê°€ì¹˜ê°€ ë†’ì€ ê²½ìš°
                msg = self.messages[e["index"]]
                content = str(msg[1])
                ltm.memorize(f"Historical Context (Archived): {content}", {"type": "synaptic_archive", "original_index": e["index"]})
                logger.info(f"ğŸ’¾ Consolidated high-value message before pruning: idx {e['index']}")

        new_messages = [m for i, m in enumerate(self.messages) if i not in to_remove_indices]
        return new_messages


def prune_synapse(state: GortexState) -> GortexState:
    """ì§€ëŠ¥í˜• ê°€ì§€ì¹˜ê¸° ìˆ˜í–‰"""
    pruner = ContextPruner(state)
    backend_type = os.getenv("LLM_BACKEND", "hybrid").lower()
    limit = 15 if backend_type == "ollama" else 30
    
    new_messages = pruner.prune(target_count=limit)
    return {"messages": new_messages}


def summarizer_node(state: GortexState):
    """LangGraph node for compression & pruning"""
    # 1. ì••ì¶• ìˆ˜í–‰
    state = compress_synapse(state)
    # 2. ê°•ì œ ê°€ì§€ì¹˜ê¸°(Pruning) ìˆ˜í–‰
    state = prune_synapse(state)
    return state