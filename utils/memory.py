import gc
import logging
import os
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory

logger = logging.getLogger("GortexMemory")

def compress_synapse(state: GortexState) -> GortexState:
    """
    ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆ ë•Œ LLMì„ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ì„ ì••ì¶•í•¨.
    ì§€ëŠ¥í˜• ì‘ì—… ìƒíƒœ(Task State) ë³´ì¡´ ë¡œì§ í¬í•¨.
    """
    messages = state.get("messages", [])
    # ë©”ì‹œì§€ ê°œìˆ˜ê°€ 12ê°œ ë¯¸ë§Œì´ê³ , ì „ì²´ ìš”ì•½ì´ ì´ë¯¸ ìˆë‹¤ë©´ ìŠ¤í‚µ
    if len(messages) < 12 and not state.get("history_summary"):
        return state

    logger.info("ğŸ§  Synaptic Compression active: Structuring project state...")
    
    # LLM ë°±ì—”ë“œ íšë“
    backend = LLMFactory.get_default_backend()
    
    # ëª¨ë¸ëª… ê²°ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        # Gemini: gemini-2.5-flash-lite, Ollama: qwen2.5-coder:7b (ì˜ˆì‹œ)
        
        # LLM_BACKENDê°€ ollamaë©´ OLLAMA_DEFAULT_MODEL ì‚¬ìš©
    
    if os.getenv("LLM_BACKEND", "gemini").lower() == "ollama":
        summary_model = os.getenv("OLLAMA_DEFAULT_MODEL", "qwen2.5-coder:7b")
    else:
        summary_model = "gemini-2.5-flash-lite"

    prompt = """ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ ì •ë°€ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ [Project State Schema]ì— ë§ì¶° í˜„ì¬ ìƒí™©ì„ 'êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸'ë¡œ ìš”ì•½í•˜ë¼. 
ì´ ìš”ì•½ì€ ë‹¤ìŒ ì—ì´ì „íŠ¸ê°€ ë„ˆì˜ ì •ì²´ì„±ê³¼ ì‘ì—… ìƒíƒœë¥¼ ì™„ë²½íˆ ê³„ìŠ¹í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.

[Project State Schema]
1. IDENTITY: ì‹œìŠ¤í…œì˜ ì •ì²´ì„± ë° ì ˆëŒ€ ì¤€ìˆ˜í•´ì•¼ í•  í•µì‹¬ ê·œì¹™ (active_constraints ì°¸ì¡°)
2. GOAL: í˜„ì¬ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìµœì¢… ëª©í‘œ
3. PROGRESS: ì´ë¯¸ ì™„ë£Œëœ ì‘ì—… ëª©ë¡ (ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
4. CHALLENGES: í˜„ì¬ ì§ë©´í•œ ë¬¸ì œ ë˜ëŠ” í•´ê²°í•´ì•¼ í•  ì˜¤ë¥˜
5. NEXT_STEPS: ë‹¤ìŒì— ì¦‰ì‹œ ì‹¤í–‰í•´ì•¼ í•  í–‰ë™ ê³„íš
6. CONTEXT_VARS: ì¤‘ìš” íŒŒì¼ ê²½ë¡œ, ë³€ìˆ˜ëª…, API ì •ë³´ ë“±

[Constraint]
- ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ê³¼ ì •ì²´ì„±ì€ ìš”ì•½ë³¸ ìµœìƒë‹¨ì— ë°°ì¹˜í•˜ë¼.
- êµ°ë”ë”ê¸° ì—†ëŠ” ëª…í™•í•œ ëª…ë ¹ì¡°ë¡œ ì‘ì„±í•˜ë¼.
- ë‹µë³€ì€ ì˜¤ì§ ìš”ì•½ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ë¼."""

    # active_constraintsê°€ ìˆë‹¤ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì£¼ì…
    if state.get("active_constraints"):
        constraints = "\n".join([f"- {c}" for c in state["active_constraints"]])
        prompt += f"\n\n[Active System Constraints (MUST PERSIST)]\n{constraints}"
        
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ êµ¬ì¡°ì— ë°˜ì˜
    # LLMBackend.generateëŠ” List[Dict]ë¥¼ ë°›ìŒ
    # messages ë¦¬ìŠ¤íŠ¸ ì•ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, generate ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ìœ ë„
    # ì—¬ê¸°ì„œëŠ” messages ë¦¬ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ì—¬ ë§¨ ì•ì— system ë©”ì‹œì§€ë¡œ ì¶”ê°€
    
    context_messages = [{"role": "system", "content": prompt}]
    
    # ê¸°ì¡´ messages ë³€í™˜ (Tuple -> Dict)
    # state["messages"]ëŠ” ë³´í†µ [(role, content), ...] íŠœí”Œ ë¦¬ìŠ¤íŠ¸ì„
    for msg in messages:
        if isinstance(msg, tuple) or isinstance(msg, list):
            context_messages.append({"role": msg[0], "content": msg[1]})
        elif isinstance(msg, dict):
            context_messages.append(msg)

    try:
        # ì„¤ì • ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© (types.GenerateContentConfig ì œê±°)
        config = {"temperature": 0.0}
        
        summary_text = backend.generate(summary_model, context_messages, config)
        
        gc.collect()
        
        # ì²« ë²ˆì§¸ ë©”ì‹œì§€ëŠ” ì‹œìŠ¤í…œì˜ ì •ì²´ì„±ì„ ë‹´ì€ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
        new_messages = [("system", f"[SYNAPTIC SUMMARY - PROJECT STATE]\n{summary_text}")]
        
        # ìµœê·¼ ë©”ì‹œì§€ 3ê°œëŠ” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ë¥¼ ìœ„í•´ ë³´ì¡´
        if len(messages) > 3:
            new_messages.extend(messages[-3:])
        
        return {
            "messages": new_messages,
            "history_summary": summary_text
        }
    except Exception as e:
        logger.error(f"Synaptic compression failed: {e}")
        return state

def prune_synapse(state: GortexState, limit: int = 50) -> GortexState:
    """ë©”ì‹œì§€ê°€ ì„ê³„ê°’ì„ ë„˜ì„ ê²½ìš° ì¤‘ê°„ ë©”ì‹œì§€ë¥¼ ì‚­ì œí•˜ì—¬ í† í° ë° ë©”ëª¨ë¦¬ ìµœì í™”"""
    messages = state.get("messages", [])
    pinned = state.get("pinned_messages", [])
    
    if len(messages) <= limit:
        return state
        
    logger.info(f"âœ‚ï¸ Pruning synapse: {len(messages)} -> {limit} messages. (Pinned: {len(pinned)})")
    
    # 1. ê³ ì •ëœ ë©”ì‹œì§€(Pinned)ë¥¼ ìµœìƒë‹¨ì— ë°°ì¹˜
    pruned = list(pinned)
    
    # 2. ìš”ì•½ë³¸ì´ í¬í•¨ëœ ì²« ë²ˆì§¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë³´ì¡´
    if messages[0] not in pruned:
        pruned.append(messages[0])
    
    # 3. ì¤‘ê°„ ë©”ì‹œì§€ ì ˆì‚­ í›„ ìµœê·¼ ë©”ì‹œì§€ë“¤ë¡œ ì±„ì›€
    remaining_slots = limit - len(pruned)
    if remaining_slots > 0:
        pruned.extend(messages[-remaining_slots:])
    
    return {"messages": pruned}

def summarizer_node(state: GortexState):
    """LangGraph node for compression & pruning"""
    # 1. ì••ì¶• ìˆ˜í–‰
    state = compress_synapse(state)
    # 2. ê°•ì œ ê°€ì§€ì¹˜ê¸°(Pruning) ìˆ˜í–‰
    state = prune_synapse(state)
    return state