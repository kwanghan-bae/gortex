import ast
import os
import json
import logging
from typing import List, Dict, Any

logger = logging.getLogger("GortexIndexer")

class SynapticIndexer:
    """
    í”„ë¡œì íŠ¸ì˜ ì½”ë“œë¥¼ ì •ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í•¨ìˆ˜, í´ë˜ìŠ¤, ë³€ìˆ˜ ì •ì˜ë¥¼ ì¸ë±ì‹±í•˜ëŠ” ì—”ì§„.
    """
    def __init__(self, root_dir: str = "."):
        self.root_dir = root_dir
        self.index_path = "logs/synaptic_index.json"
        self.index = {}

    def scan_project(self):
        """í”„ë¡œì íŠ¸ ë‚´ì˜ ëª¨ë“  Python íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì¸ë±ì‹±"""
        logger.info(f"ğŸš€ Starting synaptic indexing for {self.root_dir}...")
        new_index = {}
        
        for root, dirs, files in os.walk(self.root_dir):
            # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if d not in {'.git', 'venv', '__pycache__', 'logs', 'build', 'dist'}]
            
            for file in files:
                if file.endswith(".py"):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, self.root_dir)
                    try:
                        with open(full_path, "r", encoding='utf-8') as f:
                            tree = ast.parse(f.read())
                            new_index[rel_path] = self._analyze_tree(tree)
                    except Exception as e:
                        logger.error(f"Failed to index {rel_path}: {e}")
        
        self.index = new_index
        self._save_index()
        logger.info(f"âœ… Indexing complete. Indexed {len(new_index)} files.")

    def _analyze_tree(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """ASTë¥¼ ë¶„ì„í•˜ì—¬ í´ë˜ìŠ¤, í•¨ìˆ˜, ì„í¬íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        definitions = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                definitions.append({
                    "type": "class",
                    "name": node.name,
                    "bases": [ast.unparse(b) for b in node.bases],
                    "line": node.lineno,
                    "docstring": ast.get_docstring(node)
                })
            elif isinstance(node, ast.FunctionDef):
                definitions.append({
                    "type": "function",
                    "name": node.name,
                    "line": node.lineno,
                    "args": [arg.arg for arg in node.args.args],
                    "docstring": ast.get_docstring(node)
                })
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    definitions.append({
                        "type": "import",
                        "name": alias.name,
                        "line": node.lineno
                    })
            elif isinstance(node, ast.ImportFrom):
                definitions.append({
                    "type": "import_from",
                    "module": node.module,
                    "names": [alias.name for alias in node.names],
                    "line": node.lineno
                })
        return definitions

    def generate_map(self) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ì˜ ëª¨ë“ˆê°„ ê´€ê³„ ë° í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡° ë§µ ìƒì„±"""
        proj_map = {"nodes": {}, "edges": []}
        for file_path, defs in self.index.items():
            module_name = file_path.replace("/", ".").replace(".py", "")
            proj_map["nodes"][module_name] = {
                "file": file_path,
                "classes": [d["name"] for d in defs if d["type"] == "class"],
                "functions": [d["name"] for d in defs if d["type"] == "function"]
            }
            # ì„í¬íŠ¸ ê´€ê³„ ì¶”ì¶œ
            for d in defs:
                if d["type"] == "import":
                    proj_map["edges"].append({"from": module_name, "to": d["name"], "type": "dependency"})
                elif d["type"] == "import_from" and d["module"]:
                    proj_map["edges"].append({"from": module_name, "to": d["module"], "type": "dependency"})
                elif d["type"] == "class" and d.get("bases"):
                    for base in d["bases"]:
                        proj_map["edges"].append({"from": d["name"], "to": base, "type": "inheritance"})
        return proj_map

    def generate_knowledge_graph(self) -> Dict[str, Any]:
        """ì½”ë“œ êµ¬ì¡°ì™€ ì§„í™”ì  ë©”ëª¨ë¦¬ë¥¼ ê²°í•©í•œ í†µí•© ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""
        from gortex.core.evolutionary_memory import EvolutionaryMemory
        evo_mem = EvolutionaryMemory()
        rules = evo_mem.rules
        
        # 1. ê¸°ì¡´ í”„ë¡œì íŠ¸ ë§µ(ì½”ë“œ êµ¬ì¡°) ìƒì„±
        kg = self.generate_map()
        
        # 2. ê·œì¹™ ë…¸ë“œ ì¶”ê°€
        for rule in rules:
            rule_id = rule.get("id", "UNKNOWN_RULE")
            kg["nodes"][rule_id] = {
                "type": "rule",
                "instruction": rule.get("instruction"),
                "severity": rule.get("severity"),
                "triggers": rule.get("trigger_patterns", [])
            }
            
            # 3. ê·œì¹™ê³¼ ê´€ë ¨ ì½”ë“œ ë…¸ë“œ ì—°ê²° (íŠ¸ë¦¬ê±° íŒ¨í„´ ê¸°ë°˜ ë‹¨ìˆœ ë§¤ì¹­)
            for pattern in rule.get("trigger_patterns", []):
                for node_id, node_info in kg["nodes"].items():
                    if pattern.lower() in node_id.lower():
                        kg["edges"].append({
                            "from": rule_id, 
                            "to": node_id, 
                            "type": "constrains"
                        })
        return kg

    def _save_index(self):
        """ì¸ë±ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        with open(self.index_path, "w", encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)

    def search(self, query: str) -> List[Dict[str, Any]]:
        """ì¸ë±ìŠ¤ ë‚´ì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” ì •ì˜ë¥¼ ê²€ìƒ‰"""
        results = []
        query = query.lower()
        
        for file_path, defs in self.index.items():
            for d in defs:
                if query in d["name"].lower() or (d.get("docstring") and query in d["docstring"].lower()):
                    results.append({
                        "file": file_path,
                        **d
                    })
        return results

if __name__ == "__main__":
    # ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    indexer = SynapticIndexer()
    indexer.scan_project()
    print(f"Search result for 'Gortex': {indexer.search('Gortex')}")
