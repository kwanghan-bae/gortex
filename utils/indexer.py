import ast
import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any

logger = logging.getLogger("GortexIndexer")

class SynapticIndexer:
    """
    í”„ë¡œì íŠ¸ì˜ ì½”ë“œë¥¼ ì •ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í•¨ìˆ˜, í´ë˜ìŠ¤, ë³€ìˆ˜ ì •ì˜ë¥¼ ì¸ë±ì‹±í•˜ëŠ” ì—”ì§„.
    """
    def __init__(self, root_dir: str = "."):
        self.root_dir = root_dir
        self.index_path = "logs/synaptic_index.json"
        self.index = {}

    def scan_project(self):
        """í”„ë¡œì íŠ¸ ë‚´ì˜ ëª¨ë“  Python íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì¸ë±ì‹±"""
        logger.info(f"ğŸš€ Starting synaptic indexing for {self.root_dir}...")
        new_index = {}
        
        for root, dirs, files in os.walk(self.root_dir):
            # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if d not in {'.git', 'venv', '__pycache__', 'logs', 'build', 'dist'}]
            
            for file in files:
                if file.endswith(".py"):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, self.root_dir)
                    try:
                        with open(full_path, "r", encoding='utf-8') as f:
                            tree = ast.parse(f.read())
                            new_index[rel_path] = self._analyze_tree(tree)
                    except Exception as e:
                        logger.error(f"Failed to index {rel_path}: {e}")
        
        self.index = new_index
        self._save_index()
        logger.info(f"âœ… Indexing complete. Indexed {len(new_index)} files.")

    def _analyze_tree(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """ASTë¥¼ ë¶„ì„í•˜ì—¬ í´ë˜ìŠ¤, í•¨ìˆ˜, ì„í¬íŠ¸, í˜¸ì¶œ ì •ë³´ ì¶”ì¶œ"""
        definitions = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                definitions.append({
                    "type": "class",
                    "name": node.name,
                    "bases": [ast.unparse(b) for b in node.bases],
                    "line": node.lineno,
                    "docstring": ast.get_docstring(node)
                })
            elif isinstance(node, ast.FunctionDef):
                # í•¨ìˆ˜ ë‚´ë¶€ì˜ ë‹¤ë¥¸ í•¨ìˆ˜ í˜¸ì¶œ ìˆ˜ì§‘
                calls = []
                for subnode in ast.walk(node):
                    if isinstance(subnode, ast.Call):
                        try:
                            call_name = ast.unparse(subnode.func)
                            calls.append(call_name)
                        except: pass
                
                definitions.append({
                    "type": "function",
                    "name": node.name,
                    "line": node.lineno,
                    "args": [arg.arg for arg in node.args.args],
                    "calls": list(set(calls)), # ì¤‘ë³µ ì œê±°
                    "docstring": ast.get_docstring(node)
                })
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    definitions.append({"type": "import", "name": alias.name, "line": node.lineno})
            elif isinstance(node, ast.ImportFrom):
                definitions.append({"type": "import_from", "module": node.module, "names": [alias.name for alias in node.names], "line": node.lineno})
        return definitions

    def generate_call_graph(self) -> Dict[str, Any]:
        """í•¨ìˆ˜ ê°„ í˜¸ì¶œ ê´€ê³„ ê·¸ë˜í”„ ìƒì„±"""
        nodes = {}
        edges = []
        for file_path, defs in self.index.items():
            for d in defs:
                if d["type"] == "function":
                    func_id = f"{file_path}:{d['name']}"
                    nodes[func_id] = {"name": d["name"], "file": file_path}
                    for called in d.get("calls", []):
                        # í”„ë¡œì íŠ¸ ë‚´ì˜ ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ”ì§€ ë§¤ì¹­ (ë‹¨ìˆœí™”ëœ ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­)
                        edges.append({"from": func_id, "to_name": called})
        return {"nodes": nodes, "edges": edges}

    def generate_map(self) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ì˜ ëª¨ë“ˆê°„ ê´€ê³„ ë° í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡° ë§µ ìƒì„±"""
        proj_map = {"nodes": {}, "edges": []}
        for file_path, defs in self.index.items():
            module_name = file_path.replace("/", ".").replace(".py", "")
            proj_map["nodes"][module_name] = {
                "file": file_path,
                "classes": [d["name"] for d in defs if d["type"] == "class"],
                "functions": [d["name"] for d in defs if d["type"] == "function"]
            }
            # ì„í¬íŠ¸ ê´€ê³„ ì¶”ì¶œ
            for d in defs:
                if d["type"] == "import":
                    proj_map["edges"].append({"from": module_name, "to": d["name"], "type": "dependency"})
                elif d["type"] == "import_from" and d["module"]:
                    proj_map["edges"].append({"from": module_name, "to": d["module"], "type": "dependency"})
                elif d["type"] == "class" and d.get("bases"):
                    for base in d["bases"]:
                        proj_map["edges"].append({"from": d["name"], "to": base, "type": "inheritance"})
        return proj_map

    def generate_knowledge_graph(self) -> Dict[str, Any]:
        """ì½”ë“œ êµ¬ì¡°ì™€ ì§„í™”ì  ë©”ëª¨ë¦¬ë¥¼ ê²°í•©í•œ í†µí•© ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""
        from gortex.core.evolutionary_memory import EvolutionaryMemory
        evo_mem = EvolutionaryMemory()
        rules = evo_mem.rules
        
        # 1. ê¸°ì¡´ í”„ë¡œì íŠ¸ ë§µ(ì½”ë“œ êµ¬ì¡°) ìƒì„±
        kg = self.generate_map()
        
        # 2. ê·œì¹™ ë…¸ë“œ ì¶”ê°€
        for rule in rules:
            rule_id = rule.get("id", "UNKNOWN_RULE")
            kg["nodes"][rule_id] = {
                "type": "rule",
                "instruction": rule.get("instruction"),
                "severity": rule.get("severity"),
                "triggers": rule.get("trigger_patterns", [])
            }
            
            # 3. ê·œì¹™ê³¼ ê´€ë ¨ ì½”ë“œ ë…¸ë“œ ì—°ê²° (íŠ¸ë¦¬ê±° íŒ¨í„´ ê¸°ë°˜ ë‹¨ìˆœ ë§¤ì¹­)
            for pattern in rule.get("trigger_patterns", []):
                for node_id, node_info in kg["nodes"].items():
                    if pattern.lower() in node_id.lower():
                        kg["edges"].append({
                            "from": rule_id, 
                            "to": node_id, 
                            "type": "constrains"
                        })
        return kg

    def _save_index(self):
        """ì¸ë±ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        dirname = os.path.dirname(self.index_path)
        if dirname:
            os.makedirs(dirname, exist_ok=True)
        with open(self.index_path, "w", encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)

    def search(self, query: str, normalize: bool = False) -> List[Dict[str, Any]]:
        """ì¸ë±ìŠ¤ ë‚´ì—ì„œ ê²€ìƒ‰ (ì§€ëŠ¥í˜• ì¿¼ë¦¬ ì •ê·œí™” ë° ì ìˆ˜í™” ì§€ì›)"""
        search_query = query.lower()
        
        if normalize:
            from gortex.core.auth import GortexAuth
            auth = GortexAuth()
            prompt = f"ë‹¤ìŒ ìì—°ì–´ ì§ˆë¬¸ì„ ì½”ë“œ ê²€ìƒ‰ì„ ìœ„í•œ í•µì‹¬ ê¸°ìˆ  í‚¤ì›Œë“œ(í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª… ë“±)ë¡œ ë³€í™˜í•˜ë¼: {query}"
            try:
                response = auth.generate("gemini-1.5-flash", [("user", prompt)], None)
                search_query = response.text.strip().lower()
                logger.info(f"ğŸ” Normalized query: '{query}' -> '{search_query}'")
            except:
                pass

        results = []
        for file_path, defs in self.index.items():
            for d in defs:
                symbol_name = d.get("name", "").lower()
                # 1. ì‹¬ë³¼ëª… ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 100)
                name_match = search_query in symbol_name
                # 2. ë…ìŠ¤íŠ¸ë§ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 50)
                doc_match = d.get("docstring") and search_query in d["docstring"].lower()
                
                if name_match or doc_match:
                    results.append({
                        "file": file_path,
                        "score": 100 if name_match else 50,
                        **d
                    })
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        results.sort(key=lambda x: x["score"], reverse=True)
        return results

    def generate_dependency_graph(self) -> List[Dict[str, str]]:
        """ëª¨ë“ˆ ê°„ì˜ ì„í¬íŠ¸ ì˜ì¡´ì„± ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (A -> B)"""
        if not self.index:
            self.scan_project()
            
        dependencies = []
        for file_path, defs in self.index.items():
            source_mod = file_path.replace("/", ".").replace(".py", "")
            for d in defs:
                target_mod = None
                if d["type"] == "import":
                    target_mod = d["name"]
                elif d["type"] == "import_from":
                    target_mod = d["module"]
                
                if target_mod and "gortex" in target_mod:
                    dependencies.append({"source": source_mod, "target": target_mod})
        return dependencies

    def get_impact_radius(self, target_file: str) -> Dict[str, List[str]]:
        """íŠ¹ì • íŒŒì¼ ìˆ˜ì • ì‹œ ì˜í–¥ì„ ë°›ëŠ” ì§ì ‘/ê°„ì ‘ ëª¨ë“ˆ ë¶„ì„"""
        if not self.index:
            self.scan_project()
            
        target_module = target_file.replace("/", ".").replace(".py", "")
        direct_impact = []
        indirect_impact = []
        
        # 1ë‹¨ê³„: ì§ì ‘ ì„í¬íŠ¸ ë˜ëŠ” í˜¸ì¶œí•˜ëŠ” ëª¨ë“ˆ ì°¾ê¸°
        for file_path, defs in self.index.items():
            if file_path == target_file: continue
            
            is_direct = False
            for d in defs:
                # ì„í¬íŠ¸ í™•ì¸
                if d["type"] == "import" and target_module in d["name"]:
                    is_direct = True
                elif d["type"] == "import_from" and d["module"] and target_module.endswith(d["module"]):
                    is_direct = True
                # í•¨ìˆ˜ í˜¸ì¶œ í™•ì¸ (ë‹¨ìˆœ ì´ë¦„ ê¸°ë°˜)
                target_funcs = [def_item["name"] for def_item in self.index.get(target_file, []) if def_item["type"] == "function"]
                if d["type"] == "function" and any(tf in d.get("calls", []) for tf in target_funcs):
                    is_direct = True
                    
            if is_direct:
                direct_impact.append(file_path)

        # 2ë‹¨ê³„: ê°„ì ‘ ì˜í–¥(ì§ì ‘ ì˜í–¥ ë°›ëŠ” ëª¨ë“ˆì„ ë‹¤ì‹œ ì°¸ì¡°í•˜ëŠ” ëª¨ë“ˆ)
        for file_path, defs in self.index.items():
            if file_path == target_file or file_path in direct_impact: continue
            
            for direct in direct_impact:
                direct_mod = direct.replace("/", ".").replace(".py", "")
                if any(d["type"] in ["import", "import_from"] and (direct_mod in str(d.get("name", "")) or direct_mod in str(d.get("module", ""))) for d in defs):
                    indirect_impact.append(file_path)
                    break
                    
        return {
            "target": target_file,
            "direct": list(set(direct_impact)),
            "indirect": list(set(indirect_impact))
        }

    def find_reverse_dependencies(self, symbol_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì‹¬ë³¼ì„ í˜¸ì¶œí•˜ê±°ë‚˜ ì°¸ì¡°í•˜ëŠ” ëª¨ë“  ìœ„ì¹˜ë¥¼ ì—­ì¶”ì í•¨."""
        if not self.index:
            self.scan_project()
            
        dependents = []
        for file_path, defs in self.index.items():
            for d in defs:
                # 1. í•¨ìˆ˜ í˜¸ì¶œ ì¶”ì 
                if d["type"] == "function" and symbol_name in d.get("calls", []):
                    dependents.append({
                        "file": file_path,
                        "type": "call",
                        "caller": d["name"],
                        "line": d["line"]
                    })
                # 2. í´ë˜ìŠ¤ ìƒì† ì¶”ì 
                elif d["type"] == "class" and symbol_name in d.get("bases", []):
                    dependents.append({
                        "file": file_path,
                        "type": "inheritance",
                        "caller": d["name"],
                        "line": d["line"]
                    })
                # 3. ëª…ì‹œì  ì„í¬íŠ¸ ì¶”ì  (ImportFrom)
                elif d["type"] == "import_from" and symbol_name in d.get("names", []):
                    dependents.append({
                        "file": file_path,
                        "type": "import",
                        "caller": "module_scope",
                        "line": d["line"]
                    })
                    
        return dependents

    def calculate_intelligence_index(self) -> Dict[str, float]:
        """ëª¨ë“ˆë³„ ì§€ëŠ¥ ì§€ìˆ˜(Intelligence Index) ì‚°ì¶œ"""
        if not self.index:
            self.scan_project()
            
        from gortex.core.evolutionary_memory import EvolutionaryMemory
        evo_mem = EvolutionaryMemory()
        rules = evo_mem.memory
        
        intel_index = {}
        for file_path, defs in self.index.items():
            # 1. ê¸°ë³¸ êµ¬ì¡° ì ìˆ˜ (í´ë˜ìŠ¤/í•¨ìˆ˜ ë°€ë„)
            symbol_count = len([d for d in defs if d["type"] in ["class", "function"]])
            
            # 2. ì§€ì‹ ë°€ë„ ì ìˆ˜ (í•´ë‹¹ ëª¨ë“ˆê³¼ ê´€ë ¨ëœ ê·œì¹™ ìˆ˜)
            matched_rules = 0
            for r in rules:
                patterns = r.get("trigger_patterns", [])
                if any(p.lower() in file_path.lower() for p in patterns):
                    matched_rules += 1
            
            # 3. ì¢…í•© ì§€ìˆ˜ ê³„ì‚°
            # (ì‹¬ë³¼ ìˆ˜ * 0.4) + (ê´€ë ¨ ê·œì¹™ ìˆ˜ * 2.0) -> ì§€ëŠ¥ ë°€ë„
            # ê·œì¹™ì´ ë§ì„ìˆ˜ë¡ í•´ë‹¹ ëª¨ë“ˆì— ëŒ€í•œ ì‹œìŠ¤í…œì˜ 'ì´í•´'ì™€ 'ì œì•½'ì´ ê¹ŠìŒì„ ì˜ë¯¸
            score = (symbol_count * 0.4) + (matched_rules * 2.0)
            intel_index[file_path] = round(score, 2)
            
        return dict(sorted(intel_index.items(), key=lambda x: x[1], reverse=True))

    def calculate_health_score(self) -> Dict[str, Any]:
        """
        ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê±´ê°•ë„(Health Score) ì‚°ì¶œ.
        ì§€ëŠ¥ ì§€ìˆ˜, ì˜ì¡´ì„± ë³µì¡ë„, ë ˆì´ì–´ ìœ„ë°˜ ê±´ìˆ˜ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤.
        """
        from gortex.agents.analyst import AnalystAgent
        analyst = AnalystAgent()
        
        # 1. ë ˆì´ì–´ ìœ„ë°˜ ê±´ìˆ˜ íšë“
        violations = analyst.audit_architecture()
        violation_penalty = len(violations) * 5.0 # ê±´ë‹¹ 5ì  ê°ì 
        
        # 2. ì½”ë“œ ë³µì¡ë„ ë¶„ì„
        complexity = analyst.scan_project_complexity()
        total_complexity = sum(item.get("score", 0) for item in complexity)
        complexity_penalty = min(30, total_complexity / 10.0) # ìµœëŒ€ 30ì  ê°ì 
        
        # 3. ì§€ëŠ¥ ì„±ìˆ™ë„ ë³´ë„ˆìŠ¤
        intel_map = self.calculate_intelligence_index()
        avg_maturity = sum(intel_map.values()) / len(intel_map) if intel_map else 0
        maturity_bonus = min(20, avg_maturity / 2.0) # ìµœëŒ€ 20ì  ê°€ì‚°
        
        # ê¸°ë³¸ ì ìˆ˜ 80ì ì—ì„œ ì‹œì‘
        final_score = round(max(0, min(100, 80 - violation_penalty - complexity_penalty + maturity_bonus)), 1)
        
        return {
            "score": final_score,
            "violation_count": len(violations),
            "total_complexity": total_complexity,
            "avg_maturity": round(avg_maturity, 2),
            "timestamp": datetime.now().isoformat()
        }

if __name__ == "__main__":
    # ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    indexer = SynapticIndexer()
    indexer.scan_project()
    print(f"Search result for 'Gortex': {indexer.search('Gortex')}")
