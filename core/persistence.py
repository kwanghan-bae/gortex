import json
import os
import logging
import time
from typing import Any, Dict, Optional, Iterator, List, Tuple
from langgraph.checkpoint.base import BaseCheckpointSaver, Checkpoint, CheckpointMetadata, CheckpointTuple
from collections import ChainMap
from langgraph.checkpoint.memory import MemorySaver

logger = logging.getLogger("GortexPersistence")

class DistributedSaver(BaseCheckpointSaver):
    """
    ìƒíƒœ ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì™¸ë¶€ ì €ì¥ì†Œ(Redis ë˜ëŠ” ë¯¸ëŸ¬ íŒŒì¼)ì— ë³µì œí•˜ëŠ” 
    v3.0 í‘œì¤€ ë¶„ì‚° ì²´í¬í¬ì¸í„°.
    """
    def __init__(self, primary_saver: Optional[BaseCheckpointSaver] = None, mirror_path: str = "logs/state_mirror.json"):
        super().__init__()
        self.primary = primary_saver or MemorySaver()
        self.mirror_path = mirror_path
        os.makedirs(os.path.dirname(self.mirror_path), exist_ok=True)

    def put(self, config: Dict[str, Any], checkpoint: Checkpoint, metadata: CheckpointMetadata, new_versions: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì €ì¥ì†Œì— ê¸°ë¡ í›„ ì™¸ë¶€ ì €ì¥ì†Œë¡œ ì¦‰ì‹œ ë³µì œ"""
        # 1. Primary ì €ì¥ (Memory/SQLite)
        res = self.primary.put(config, checkpoint, metadata, new_versions)
        self._replicate(config, checkpoint, metadata)
        return res

    async def aput(self, config: Dict[str, Any], checkpoint: Checkpoint, metadata: CheckpointMetadata, new_versions: Dict[str, Any]) -> Dict[str, Any]:
        """ë¹„ë™ê¸°: ê¸°ë³¸ ì €ì¥ì†Œì— ê¸°ë¡ í›„ ì™¸ë¶€ ì €ì¥ì†Œë¡œ ì¦‰ì‹œ ë³µì œ"""
        if hasattr(self.primary, "aput"):
            res = await self.primary.aput(config, checkpoint, metadata, new_versions)
        else:
            res = self.primary.put(config, checkpoint, metadata, new_versions)
        self._replicate(config, checkpoint, metadata)
        return res

    def _replicate(self, config, checkpoint, metadata):
        # 2. Replication (Mirroring & Redis Sync)
        try:
            # ì§ë ¬í™” ê°€ëŠ¥í•œ ìƒíƒœë¡œ ë³€í™˜
            serializable_state = {
                "v": 3,
                "ts": time.time(),
                "config": self._make_serializable(config), 
                "checkpoint": self._make_serializable(checkpoint),
                "metadata": self._make_serializable(metadata)
            }
            
            # [Mirror 1] Local Atomic File
            tmp_path = self.mirror_path + ".tmp"
            with open(tmp_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_state, f, ensure_ascii=False, indent=2, default=str)
            os.replace(tmp_path, self.mirror_path)
            
            # [Mirror 2] Global Sync (StorageProvider)
            from gortex.core.mq import mq_bus
            thread_id = config.get("configurable", {}).get("thread_id", "global")
            redis_key = f"gortex:state:{thread_id}"
            
            try:
                mq_bus.storage.set(redis_key, json.dumps(serializable_state, default=str), ex=3600*24)
                # ìƒíƒœ ë³€ê²½ ì´ë²¤íŠ¸ ì „íŒŒ (mq_bus handles local/remote publishing)
                mq_bus.publish_event("gortex:state_updates", "Persistence", "state_synced", {"thread_id": thread_id})
            except Exception as e:
                logger.warning(f"Storage sync failed: {e}")
            
        except Exception as e:
            logger.error(f"Replication failed: {e}")

    def get_tuple(self, config: Dict[str, Any]) -> Optional[CheckpointTuple]:
        """ê¸°ë³¸ ì €ì¥ì†Œì—ì„œ ì¡°íšŒí•˜ë˜, ì‹¤íŒ¨ ì‹œ ë¯¸ëŸ¬ë¡œë¶€í„° ê°•ì œ ë³µêµ¬"""
        res = self.primary.get_tuple(config)
        if res:
            return res
        return self._recover_from_mirror()

    async def aget_tuple(self, config: Dict[str, Any]) -> Optional[CheckpointTuple]:
        """ë¹„ë™ê¸°: ê¸°ë³¸ ì €ì¥ì†Œì—ì„œ ì¡°íšŒí•˜ë˜, ì‹¤íŒ¨ ì‹œ ë¯¸ëŸ¬ë¡œë¶€í„° ê°•ì œ ë³µêµ¬"""
        if hasattr(self.primary, "aget_tuple"):
            res = await self.primary.aget_tuple(config)
        else:
            res = self.primary.get_tuple(config)
            
        if res:
            return res
        return self._recover_from_mirror()

    def _recover_from_mirror(self, config: Optional[Dict[str, Any]] = None) -> Optional[CheckpointTuple]:
        # 1. Storage Recovery Attempt (Highest Priority)
        from gortex.core.mq import mq_bus
        if config:
            thread_id = config.get("configurable", {}).get("thread_id", "global")
            redis_key = f"gortex:state:{thread_id}"
            try:
                data_str = mq_bus.storage.get(redis_key)
                if data_str:
                    logger.info(f"ğŸ“¡ Recovered state for {thread_id} from Storage.")
                    # (ì‹¤ì œ CheckpointTuple ë³µì› ë¡œì§ì€ ìŠ¤í‚¤ë§ˆ ê³ ë„í™” í•„ìš” - í˜„ì¬ëŠ” ë¡œì§ íë¦„ êµ¬ì¶•)
                    return None
            except Exception as e:
                logger.warning(f"Storage recovery failed: {e}")

        # 2. Local File Recovery (Fallback)
        if os.path.exists(self.mirror_path):
            logger.info("ğŸ“¡ Primary state lost. Recovering from local mirror...")
            try:
                with open(self.mirror_path, 'r', encoding='utf-8') as f:
                    _ = json.load(f)
                return None 
            except Exception:
                return None
        return None

    def list(self, config: Optional[Dict[str, Any]] = None, *, filter: Optional[Dict[str, Any]] = None, before: Optional[Dict[str, Any]] = None, limit: Optional[int] = None) -> Iterator[CheckpointTuple]:
        return self.primary.list(config, filter=filter, before=before, limit=limit)

    async def alist(self, config: Optional[Dict[str, Any]] = None, *, filter: Optional[Dict[str, Any]] = None, before: Optional[Dict[str, Any]] = None, limit: Optional[int] = None) -> Iterator[CheckpointTuple]:
        if hasattr(self.primary, "alist"):
            return [c async for c in self.primary.alist(config, filter=filter, before=before, limit=limit)]
        else:
            return self.primary.list(config, filter=filter, before=before, limit=limit)

    async def aget(self, config: Dict[str, Any]) -> Optional[Checkpoint]:
        if hasattr(self.primary, "aget"):
            return await self.primary.aget(config)
        return self.primary.get(config)

    async def adelete_thread(self, config: Dict[str, Any]) -> None:
        if hasattr(self.primary, "adelete_thread"):
            await self.primary.adelete_thread(config)
        else:
            self.primary.delete_thread(config)

    async def aput_writes(self, config: Dict[str, Any], writes: List[Tuple[str, Any]], task_id: str) -> None:
        if hasattr(self.primary, "aput_writes"):
            await self.primary.aput_writes(config, writes, task_id)
        else:
            self.primary.put_writes(config, writes, task_id)


    def _make_serializable(self, data: Any) -> Any:
        """ë°ì´í„°ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì¬ê·€ì  ë³€í™˜ (BaseMessage ë“± ì²˜ë¦¬)"""
        # [Scalar] ê¸°ë³¸ íƒ€ì…ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ê°€ì¥ ë¹ˆë²ˆí•˜ë¯€ë¡œ ìµœìƒë‹¨ ë°°ì¹˜)
        if data is None or isinstance(data, (str, int, float, bool)):
            return data
            
        if isinstance(data, dict):
            return {k: self._make_serializable(v) for k, v in data.items()}
        elif isinstance(data, ChainMap):
             # ChainMapì„ dictë¡œ ë³€í™˜ (ëª¨ë“  ë§µì„ í•©ì¹¨)
            return {k: self._make_serializable(v) for k, v in dict(data).items()}
        elif isinstance(data, (list, tuple, set)): 
            # list, tuple, set ëª¨ë‘ JSON ë°°ì—´ë¡œ ì§ë ¬í™”
            return [self._make_serializable(v) for v in data]
        elif hasattr(data, "content") and hasattr(data, "type"): # BaseMessage ëŒ€ì‘
            return {"type": data.type, "content": data.content}
        elif hasattr(data, "__dict__"):
            return str(data) # ê°ì²´ëŠ” ë¬¸ìì—´ í‘œí˜„ìœ¼ë¡œ ì €ì¥ (í•„ìš”ì‹œ __dict__ ì§ë ¬í™”ë¡œ ê³ ë„í™”)
        
        # [Fallback] ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ëª¨ë“  íƒ€ì…(Runtime, SystemObject ë“±)ì€ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        # ì´ë ‡ê²Œ í•˜ë©´ ì§ë ¬í™” ì—ëŸ¬ë¡œ ì‹œìŠ¤í…œì´ ë©ˆì¶”ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        try:
            return str(data)
        except Exception:
            return f"<Unserializable: {type(data).__name__}>"