import logging
import uuid
import json
from typing import Dict, Any, Optional
from gortex.core.graph import compile_gortex_graph
from gortex.core.state import GortexState
from gortex.utils.healing_memory import SelfHealingMemory
from gortex.utils.token_counter import count_tokens, DailyTokenTracker
from gortex.utils.resource_monitor import ResourceMonitor

logger = logging.getLogger("GortexEngine")

class GortexEngine:
    """
    Gortex ì‹œìŠ¤í…œì˜ í•µì‹¬ ì‹¤í–‰ ì—”ì§„.
    ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ë©° ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    def __init__(self, ui=None, observer=None, vocal_bridge=None, thread_id: str = None):
        self.ui = ui
        self.observer = observer
        self.vocal = vocal_bridge
        self.thread_id = thread_id or str(uuid.uuid4())
        self.config = {"configurable": {"thread_id": self.thread_id}}
        
        # 1. ì´ˆê¸° ê·¸ë˜í”„ ì»´íŒŒì¼
        self.graph = compile_gortex_graph()
        
        self.healer = SelfHealingMemory()
        self.tracker = DailyTokenTracker()
        self.monitor = ResourceMonitor()
        self.max_concurrency = 2 

    def refresh_graph(self):
        """ëŸ°íƒ€ì„ì— ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ì¬ì»´íŒŒì¼í•¨ (Zero-Downtime Evolution)"""
        logger.info("ğŸ§  Hot-swapping neural architecture: Re-compiling graph...")
        try:
            # ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒíƒœë¥¼ ë°˜ì˜í•˜ì—¬ ê·¸ë˜í”„ ì¬êµ¬ì¶•
            self.graph = compile_gortex_graph()
            if self.ui:
                self.ui.add_achievement("Neural Map Updated")
            logger.info("âœ… Graph successfully re-compiled and swapped.")
            return True
        except Exception as e:
            logger.error(f"Failed to refresh graph: {e}")
            return False

    def update_scaling_policy(self):
        """ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¼ ë™ì‹œ ì‹¤í–‰ í•œë„ ìŠ¤ì¼€ì¼ë§"""
        old_limit = self.max_concurrency
        self.max_concurrency = self.monitor.estimate_concurrency_limit(base_limit=2)
        
        if old_limit != self.max_concurrency:
            logger.info(f"âš–ï¸ Scaling Policy Updated: {old_limit} -> {self.max_concurrency} tasks concurrently.")
            if self.ui:
                self.ui.add_achievement(f"Scaling to {self.max_concurrency}x")

    async def run_self_defense_cycle(self):
        """ììœ¨ì ìœ¼ë¡œ ì·¨ì•½ êµ¬ì—­ì„ ì°¾ì•„ í…ŒìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°©ì–´ë ¥ì„ ë†’ì„."""
        from gortex.agents.analyst.base import AnalystAgent
        from gortex.agents.coder import CoderAgent
        import os
        
        analyst = AnalystAgent()
        coder = CoderAgent()
        
        logger.info("ğŸ›¡ï¸ Initiating Self-Defense Cycle...")
        
        # 1. í…ŒìŠ¤íŠ¸ í•«ìŠ¤íŒŸ ì‹ë³„
        hotspots = analyst.identify_test_hotspots()
        if not hotspots:
            logger.info("âœ… No urgent test hotspots found. System is well-defended.")
            return
            
        target = hotspots[0] # ê°€ì¥ ìœ„í—˜í•œ ê³³ ìš°ì„ 
        logger.info(f"ğŸ“ Target hotspot identified: {target['file']} (Risk: {target['risk_score']})")
        
        # 2. íšŒê·€ í…ŒìŠ¤íŠ¸ ìƒì„± ë° ê²€ì¦
        res = coder.generate_regression_test(target["file"], risk_info=target["reason"])
        
        if res.get("status") == "success":
            logger.info(f"âœ… Defenses strengthened: {res['file']}")
            if self.ui:
                self.ui.add_achievement(f"Defense Up: {os.path.basename(res['file'])}")
        else:
            logger.error(f"âŒ Defense generation failed for {target['file']}: {res.get('error') or res.get('reason')}")

    def select_optimal_model(self, state: GortexState, agent_name: str) -> str:
        """ì—ì´ì „íŠ¸ í‰íŒ, ì§€ê°‘ ì”ê³ , ì‘ì—… ìœ„í—˜ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì  ëª¨ë¸ ì„ íƒ"""
        risk = state.get("risk_score", 0.5)
        budget_status = self.tracker.get_budget_status()
        economy = state.get("agent_economy", {}).get(agent_name.lower(), {})
        
        points = economy.get("points", 0)
        credits = economy.get("credits", 0.0) # [NEW] ì§€ë¶ˆ ëŠ¥ë ¥ í™•ì¸
        
        # 1. ì˜ˆì‚° ê³ ê°ˆ ìƒíƒœ (ì‹œìŠ¤í…œ ì „ì²´)
        if budget_status > 0.9:
            return "ollama/llama3"
            
        # 2. [ECONOMY] ì§€ë¶ˆ ëŠ¥ë ¥ ê¸°ë°˜ í•„í„°ë§
        # Gemini ProëŠ” ìµœì†Œ $1.0ì˜ ì”ê³ ê°€ ìˆì–´ì•¼ ì‹œë„ ê°€ëŠ¥
        can_afford_pro = credits >= 1.0
        # Gemini FlashëŠ” ìµœì†Œ $0.1ì˜ ì”ê³  í•„ìš”
        can_afford_flash = credits >= 0.1

        # 3. ëª¨ë¸ í• ë‹¹ ë¡œì§
        if risk > 0.8 and points > 1000 and can_afford_pro:
            return "gemini-1.5-pro"
            
        if (points > 500 or risk > 0.4) and can_afford_flash:
            return "gemini-2.0-flash"
            
        # 4. ì”ê³  ë¶€ì¡± ì‹œ ê°•ì œ ë‹¤ìš´ê·¸ë ˆì´ë“œ
        if not can_afford_flash:
            logger.info(f"ğŸ’¸ Agent {agent_name} is under-funded (${credits:.4f}). Downgrading to Ollama.")
            
        return "ollama/llama3"

    async def process_node_output(self, node_name: str, output: Dict[str, Any], state: Dict[str, Any], latency_ms: Optional[int] = None):
        """ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ê³  ì‹¤ì‹œê°„ ê²½ì œ ì •ì‚° ë° UI ì—…ë°ì´íŠ¸ ìˆ˜í–‰"""
        # 1. í† í° ì¶”ì  ë° ë¹„ìš© ê³„ì‚°
        tokens = count_tokens(json.dumps(output))
        model = state.get("assigned_model", "flash")
        self.tracker.update_usage(tokens, model)
        
        from gortex.utils.token_counter import estimate_cost
        cost = estimate_cost(tokens, model)
        
        # 2. [ECONOMIC SOWEREIGNTY] ìë™ ê²°ì œ (Auto-Billing)
        from gortex.utils.economy import get_economy_manager
        eco_manager = get_economy_manager()
        
        # ì‚¬ìš©ë£Œ ì°¨ê°
        eco_manager.deduct_credits(state, node_name, cost)
        
        # ì„±ê³µ ì‹œ ìƒê¸ˆ ì§€ê¸‰ (ë¹„ìš©ì˜ 1.5ë°° ë³´ë„ˆìŠ¤ ë˜ëŠ” ê³ ì • ìˆ˜ìµ)
        if output.get("status") == "success" or "âŒ" not in str(output.get("messages", "")):
            reward = cost * 1.2 + 0.001 # ìµœì†Œ ìˆ˜ìµ ë³´ì¥
            eco_manager.add_credits(state, node_name, reward)
            logger.info(f"ğŸ’° Agent '{node_name}' earned ${reward:.6f} (ROI: +20%)")

        # 3. ì¸ê³¼ ê´€ê³„ ë° ê´€ì°°ì ê¸°ë¡ (ê¸°ì¡´ ë¡œì§)
        event_id = str(uuid.uuid4())
        if self.observer:
            cause_id = state.get("last_event_id")
            res_id = self.observer.log_event(
                agent=node_name, 
                event="node_complete", 
                payload=output, 
                latency_ms=latency_ms,
                cause_id=cause_id
            )
            state["last_event_id"] = res_id or event_id
        
        # UI ì—…ë°ì´íŠ¸
        if self.ui:
            self.ui.update_thought(output.get("thought", ""), agent_name=node_name)
            if "ui_mode" in output:
                self.ui.set_mode(output["ui_mode"])
            
            # [NEW] AI ë©”ì‹œì§€ë¥¼ UI ì±„íŒ… ê¸°ë¡ì— ë°˜ì˜ (í•„í„°ë§ ê°•í™”)
            if "messages" in output:
                for msg in output["messages"]:
                    if isinstance(msg, (list, tuple)) and msg[0] == "ai":
                        content = str(msg[1])
                        # ë‚´ë¶€ ê¸°ìˆ ì  ì—ëŸ¬ë‚˜ ë‹¨ìˆœ ì™„ë£Œ ì•Œë¦¼ì€ ë©”ì¸ ì±„íŒ…ì—ì„œ ì œì™¸
                        internal_keywords = ["Planning Error", "ë¶„ì„ ì˜¤ë¥˜", "All steps completed", "ì™„ë£Œí–ˆìŠµë‹ˆë‹¤"]
                        if not any(k in content for k in internal_keywords):
                            self.ui.chat_history.append(msg)
                        else:
                            # ë‚´ë¶€ ìƒíƒœëŠ” ë¡œê·¸ë¡œ ê¸°ë¡
                            self.ui.update_logs({"agent": node_name, "event": content})
            
            msg_str = str(output.get("messages", ""))
            if "ì™„ë£Œí–ˆìŠµë‹ˆë‹¤" in msg_str:
                self.ui.add_achievement("Goal Reached")
            
            if "âŒ" in msg_str or "security alert" in msg_str.lower():
                self.ui.add_security_event("High", "Security issue detected")
            
            if "agent_economy" in state or "agent_economy" in output:
                eco_data = output.get("agent_economy") or state.get("agent_economy")
                if eco_data:
                    self.ui.update_economy_panel(eco_data)
        
        # ìŒì„± ë¸Œë¦¿ì§€ (v9.0 ì—ì´ì „íŠ¸ ê³ ìœ  ë³´ì´ìŠ¤ ì—°ë™)
        if self.vocal and output.get("messages"):
            for m in output["messages"]:
                if isinstance(m, (list, tuple)) and m[0] == "ai":
                    last_msg = str(m[1])
                    if self.vocal.text_to_speech(last_msg, agent_name=node_name):
                        self.vocal.play_audio("logs/response.mp3")
            
        state.update(output)
        return tokens

    def run(self, user_input: str, initial_state: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì—ì´ì „íŠ¸ ë£¨í”„ ì‹¤í–‰"""
        # ì‹¤í–‰ ì „ ìŠ¤ì¼€ì¼ë§ ì •ì±… ê°±ì‹ 
        self.update_scaling_policy()
        
        state = initial_state or {
            "messages": [("user", user_input)],
            "pinned_messages": [],
            "plan": [],
            "current_step": 0,
            "working_dir": ".",
            "file_cache": {},
            "agent_energy": 100,
            "api_call_count": 0,
            "token_credits": {},
            "agent_economy": {},
            "risk_score": 0.5
        }
        
        energy = state.get("agent_energy", 100)
        if energy < 10:
            return {
                "messages": [("ai", "ğŸ”‹ **ì‹œìŠ¤í…œ ì—ë„ˆì§€ ê³ ê°ˆ**: ìœ ì§€ë³´ìˆ˜ ëª¨ë“œì…ë‹ˆë‹¤.")],
                "next_node": "__end__"
            }
        
        try:
            final_state = self.graph.invoke(state, self.config)
            return final_state
        except Exception as e:
            logger.error(f"Engine execution failed: {e}")
            return {"error": str(e), "next_node": "__end__"}

    async def run_async(self, user_input: str, initial_state: Optional[Dict[str, Any]] = None):
        return self.run(user_input, initial_state)
