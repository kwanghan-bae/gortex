import logging
import uuid
import json
from typing import Dict, Any, Optional
from gortex.core.graph import compile_gortex_graph
from gortex.core.state import GortexState
from gortex.utils.healing_memory import SelfHealingMemory
from gortex.utils.token_counter import count_tokens, DailyTokenTracker
from gortex.utils.resource_monitor import ResourceMonitor

logger = logging.getLogger("GortexEngine")

class GortexEngine:
    """
    Gortex ì‹œìŠ¤í…œì˜ í•µì‹¬ ì‹¤í–‰ ì—”ì§„.
    ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¼ ë¦¬ì†ŒìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ë©° ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    def __init__(self, ui=None, observer=None, vocal_bridge=None, thread_id: str = None):
        self.ui = ui
        self.observer = observer
        self.vocal = vocal_bridge
        self.graph = compile_gortex_graph()
        self.thread_id = thread_id or str(uuid.uuid4())
        self.config = {"configurable": {"thread_id": self.thread_id}}
        self.healer = SelfHealingMemory()
        self.tracker = DailyTokenTracker()
        self.monitor = ResourceMonitor()
        self.max_concurrency = 2 # ê¸°ë³¸ ë™ì‹œ ì‹¤í–‰ í•œë„

    def update_scaling_policy(self):
        """ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¼ ë™ì‹œ ì‹¤í–‰ í•œë„ ìŠ¤ì¼€ì¼ë§"""
        old_limit = self.max_concurrency
        self.max_concurrency = self.monitor.estimate_concurrency_limit(base_limit=2)
        
        if old_limit != self.max_concurrency:
            logger.info(f"âš–ï¸ Scaling Policy Updated: {old_limit} -> {self.max_concurrency} tasks concurrently.")
            if self.ui:
                self.ui.add_achievement(f"Scaling to {self.max_concurrency}x")

    async def run_self_defense_cycle(self):
        """ììœ¨ì ìœ¼ë¡œ ì·¨ì•½ êµ¬ì—­ì„ ì°¾ì•„ í…ŒìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°©ì–´ë ¥ì„ ë†’ì„."""
        from gortex.agents.analyst.base import AnalystAgent
        from gortex.agents.coder import CoderAgent
        import os
        
        analyst = AnalystAgent()
        coder = CoderAgent()
        
        logger.info("ğŸ›¡ï¸ Initiating Self-Defense Cycle...")
        
        # 1. í…ŒìŠ¤íŠ¸ í•«ìŠ¤íŒŸ ì‹ë³„
        hotspots = analyst.identify_test_hotspots()
        if not hotspots:
            logger.info("âœ… No urgent test hotspots found. System is well-defended.")
            return
            
        target = hotspots[0] # ê°€ì¥ ìœ„í—˜í•œ ê³³ ìš°ì„ 
        logger.info(f"ğŸ“ Target hotspot identified: {target['file']} (Risk: {target['risk_score']})")
        
        # 2. íšŒê·€ í…ŒìŠ¤íŠ¸ ìƒì„± ë° ê²€ì¦
        res = coder.generate_regression_test(target["file"], risk_info=target["reason"])
        
        if res.get("status") == "success":
            logger.info(f"âœ… Defenses strengthened: {res['file']}")
            if self.ui:
                self.ui.add_achievement(f"Defense Up: {os.path.basename(res['file'])}")
        else:
            logger.error(f"âŒ Defense generation failed for {target['file']}: {res.get('error') or res.get('reason')}")

    def select_optimal_model(self, state: GortexState, agent_name: str) -> str:
        """ì—ì´ì „íŠ¸ í‰íŒ, ì‘ì—… ìœ„í—˜ë„, ì¼ì¼ ì˜ˆì‚°ì„ ê³ ë ¤í•˜ì—¬ ìµœì  ëª¨ë¸ ì„ íƒ"""
        risk = state.get("risk_score", 0.5)
        budget_status = self.tracker.get_budget_status()
        economy = state.get("agent_economy", {}).get(agent_name, {})
        points = economy.get("points", 0)
        
        # 1. ì˜ˆì‚° ê³ ê°ˆ ìƒíƒœ (80% ì´ìƒ ì†Œëª¨) -> ê°•ì œ Ollama ë‹¤ìš´ê·¸ë ˆì´ë“œ
        if budget_status > 0.8:
            logger.warning(f"ğŸ”‹ Budget critical ({budget_status:.1%}). Downgrading to Ollama.")
            return "ollama/llama3"
            
        # 2. ê³ ìœ„í—˜/ì—í”½ ì‘ì—… + ì—˜ë¦¬íŠ¸ ì—ì´ì „íŠ¸ -> Gemini Pro
        if risk > 0.8 and points > 1000:
            return "gemini-1.5-pro"
            
        # 3. ì¼ë°˜ ì „ë¬¸ ì‘ì—… -> Gemini Flash
        if points > 500 or risk > 0.4:
            return "gemini-2.0-flash"
            
        # 4. ë‹¨ìˆœ ë°˜ë³µ ì‘ì—…/ì €í‰íŒ ì—ì´ì „íŠ¸ -> Ollama
        return "ollama/llama3"

    async def process_node_output(self, node_name: str, output: Dict[str, Any], state: Dict[str, Any]):
        """ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ê³  UI/ê´€ì°°ìì—ê²Œ ì•Œë¦¼"""
        # í† í° ì¶”ì  ì—…ë°ì´íŠ¸
        tokens = count_tokens(json.dumps(output))
        model = state.get("assigned_model", "flash")
        self.tracker.update_usage(tokens, model)
        
        # ì¸ê³¼ ê´€ê³„ ë° ê´€ì°°ì ê¸°ë¡
        event_id = str(uuid.uuid4())
        if self.observer:
            cause_id = state.get("last_event_id")
            res_id = self.observer.log_event(
                agent=node_name, 
                event="node_complete", 
                payload=output, 
                cause_id=cause_id
            )
            state["last_event_id"] = res_id or event_id
        
        # UI ì—…ë°ì´íŠ¸
        if self.ui:
            self.ui.update_thought(output.get("thought", ""), agent_name=node_name)
            if "ui_mode" in output:
                self.ui.set_layout_mode(output["ui_mode"])
            
            msg_str = str(output.get("messages", ""))
            if "ì™„ë£Œí–ˆìŠµë‹ˆë‹¤" in msg_str:
                self.ui.add_achievement("Goal Reached")
            
            if "âŒ" in msg_str or "security alert" in msg_str.lower():
                self.ui.add_security_event("High", "Security issue detected")
            
            if "agent_economy" in state or "agent_economy" in output:
                eco_data = output.get("agent_economy") or state.get("agent_economy")
                if eco_data:
                    self.ui.update_economy_panel(eco_data)
        
        # ìŒì„± ë¸Œë¦¿ì§€
        if self.vocal and output.get("messages"):
            last_msg = str(output["messages"][-1][1] if isinstance(output["messages"][-1], tuple) else output["messages"][-1])
            self.vocal.text_to_speech(last_msg)
            self.vocal.play_audio()
            
        # ìê°€ ì¹˜ìœ 
        if output.get("status") == "failed":
            hint = self.healer.get_solution_hint("Error detected in node output")
            if hint:
                logger.info(f"ğŸ©¹ Healing hint found: {hint}")

        state.update(output)
        return tokens

    def run(self, user_input: str, initial_state: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì—ì´ì „íŠ¸ ë£¨í”„ ì‹¤í–‰"""
        # ì‹¤í–‰ ì „ ìŠ¤ì¼€ì¼ë§ ì •ì±… ê°±ì‹ 
        self.update_scaling_policy()
        
        state = initial_state or {
            "messages": [("user", user_input)],
            "pinned_messages": [],
            "plan": [],
            "current_step": 0,
            "working_dir": ".",
            "file_cache": {},
            "agent_energy": 100,
            "api_call_count": 0,
            "token_credits": {},
            "agent_economy": {},
            "risk_score": 0.5
        }
        
        energy = state.get("agent_energy", 100)
        if energy < 10:
            return {
                "messages": [("ai", "ğŸ”‹ **ì‹œìŠ¤í…œ ì—ë„ˆì§€ ê³ ê°ˆ**: ìœ ì§€ë³´ìˆ˜ ëª¨ë“œì…ë‹ˆë‹¤.")],
                "next_node": "__end__"
            }
        
        try:
            final_state = self.graph.invoke(state, self.config)
            return final_state
        except Exception as e:
            logger.error(f"Engine execution failed: {e}")
            return {"error": str(e), "next_node": "__end__"}

    async def run_async(self, user_input: str, initial_state: Optional[Dict[str, Any]] = None):
        return self.run(user_input, initial_state)
