import json
import logging
import uuid
import os
import shutil
from datetime import datetime
from typing import Any, Dict, Optional, List

logger = logging.getLogger("GortexObserver")

class GortexObserver:
    """
    ì‹œìŠ¤í…œì˜ ëª¨ë“  ë™ì‘(ì‚¬ê³ , ë„êµ¬ í˜¸ì¶œ, ì˜¤ë¥˜)ì„ ê°ì‹œí•˜ê³  êµ¬ì¡°í™”ëœ ë¡œê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self, log_path: str = "logs/trace.jsonl"):
        self.log_path = log_path
        self.trace_id = str(uuid.uuid4())
        self._ensure_log_dir()
        self._rotate_logs()

    def _ensure_log_dir(self):
        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)

    def archive_and_reset_logs(self) -> str:
        """í˜„ì¬ ë¡œê·¸ë¥¼ ì••ì¶• ì•„ì¹´ì´ë¹™í•˜ê³  ì›ë³¸ì„ ì´ˆê¸°í™”í•¨ (ë¦¬ì†ŒìŠ¤ ìµœì í™”)"""
        if not os.path.exists(self.log_path) or os.path.getsize(self.log_path) == 0:
            return "Log file is already empty or missing."

        try:
            archive_dir = "logs/archives"
            os.makedirs(archive_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            zip_path = os.path.join(archive_dir, f"trace_archive_{timestamp}.zip")
            
            # ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬ í›„ ì••ì¶• (ì“°ê¸° ì ê¸ˆ ë°©ì§€)
            temp_path = self.log_path + ".tmp"
            shutil.copy2(self.log_path, temp_path)
            
            # ì••ì¶• ìˆ˜í–‰ (íŒŒì¼ í•˜ë‚˜ë§Œ ì••ì¶•í•˜ë¯€ë¡œ compress_directory í™œìš© ë˜ëŠ” ì§ì ‘ êµ¬í˜„)
            import zipfile
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                zipf.write(temp_path, os.path.basename(self.log_path))
            
            os.remove(temp_path)
            
            # ì›ë³¸ ë¡œê·¸ ì´ˆê¸°í™” (Truncate)
            with open(self.log_path, 'w') as f:
                f.write("")
                
            logger.info(f"ğŸ“ Trace logs archived to {zip_path} and reset.")
            return zip_path
        except Exception as e:
            logger.error(f"Failed to archive logs: {e}")
            return f"Error: {e}"

    def _rotate_logs(self, max_size_mb: int = 10):
        """ë¡œê·¸ íŒŒì¼ í¬ê¸°ê°€ í¬ë©´ ìë™ìœ¼ë¡œ ì•„ì¹´ì´ë¹™ íŠ¸ë¦¬ê±°"""
        if os.path.exists(self.log_path):
            if os.path.getsize(self.log_path) > max_size_mb * 1024 * 1024:
                logger.warning(f"âš ï¸ Log size exceeded {max_size_mb}MB. Triggering auto-archive.")
                self.archive_and_reset_logs()

    def get_stats(self) -> Dict[str, Any]:
        """ëˆ„ì  í†µê³„ ë°ì´í„° ë°˜í™˜"""
        total_tokens = 0
        total_cost = 0.0
        start_time = None
        event_count = 0
        
        if os.path.exists(self.log_path):
            try:
                with open(self.log_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        event_count += 1
                        
                        # í† í° ë° ë¹„ìš© í•©ì‚°
                        if "tokens" in data and data["tokens"]:
                            if isinstance(data["tokens"], dict):
                                total_tokens += data["tokens"].get("total", 0)
                            elif isinstance(data["tokens"], (int, float)):
                                total_tokens += int(data["tokens"])
                        
                        if not start_time:
                            start_time = datetime.fromisoformat(data["timestamp"])
            except Exception as e:
                logger.warning(f"Error reading stats from log: {e}")

        uptime = "N/A"
        if start_time:
            delta = datetime.now() - start_time
            uptime = str(delta).split(".")[0] # HH:MM:SS í˜•ì‹

        return {
            "total_tokens": total_tokens,
            "total_cost": round(total_tokens * 0.000002, 6),
            "uptime": uptime,
            "event_count": event_count,
            "trace_id": self.trace_id
        }

    def log_event(self, agent: str, event: str, payload: Any, latency_ms: Optional[int] = None, tokens: Optional[Dict[str, int]] = None, cause_id: Optional[str] = None):
        """ì´ë²¤íŠ¸ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ê¸°ë¡ (ì¸ê³¼ ê´€ê³„ ì¶”ì  ì§€ì›)"""
        event_id = str(uuid.uuid4())[:8]
        entry = {
            "id": event_id,
            "timestamp": datetime.now().isoformat(),
            "trace_id": self.trace_id,
            "agent": agent,
            "event": event, # 'thought', 'tool_call', 'node_complete', 'error'
            "payload": payload,
            "latency_ms": latency_ms,
            "tokens": tokens,
            "cause_id": cause_id # ì›ì¸ì´ ëœ ì´ì „ ì´ë²¤íŠ¸ ID
        }
        
        try:
            with open(self.log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"Failed to write trace log: {e}")
        return event_id

    def get_causal_chain(self, start_event_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì´ë²¤íŠ¸ IDë¡œë¶€í„° ë£¨íŠ¸ê¹Œì§€ ì¸ê³¼ ê´€ê³„ ì²´ì¸ì„ ì—­ì¶”ì """
        if not os.path.exists(self.log_path):
            return []
            
        try:
            with open(self.log_path, "r", encoding="utf-8") as f:
                logs = [json.loads(line) for line in f]
            
            # ID ê¸°ë°˜ ê²€ìƒ‰ ë§µ ìƒì„±
            log_map = {l["id"]: l for l in logs if "id" in l}
            
            chain = []
            current_id = start_event_id
            
            # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ ê¹Šì´ ì œí•œ
            for _ in range(100):
                if current_id not in log_map:
                    break
                event = log_map[current_id]
                chain.append(event)
                current_id = event.get("cause_id")
                if not current_id:
                    break
            return chain # [ìµœì‹  -> ê³¼ê±°] ìˆœì„œ
        except Exception as e:
            logger.error(f"Failed to trace causal chain: {e}")
            return []

    def get_collaboration_matrix(self, limit: int = 500) -> Dict[str, Dict[str, int]]:
        """ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ì—ì´ì „íŠ¸ ê°„ í˜¸ì¶œ ë¹ˆë„(Collaboration Matrix) ì‚°ì¶œ"""
        if not os.path.exists(self.log_path):
            return {}
            
        try:
            with open(self.log_path, "r", encoding="utf-8") as f:
                logs = [json.loads(line) for line in f][-limit:]
            
            # ID ê¸°ë°˜ ê²€ìƒ‰ ë§µ (ìµœì í™”)
            event_agent_map = {l["id"]: l["agent"] for l in logs if "id" in l}
            
            matrix = {} # {caller: {callee: count}}
            
            for l in logs:
                callee = l["agent"]
                cause_id = l.get("cause_id")
                
                if cause_id and cause_id in event_agent_map:
                    caller = event_agent_map[cause_id]
                    
                    # ìê¸° ìì‹  í˜¸ì¶œ ì œì™¸
                    if caller == callee:
                        continue
                        
                    if caller not in matrix: matrix[caller] = {}
                    matrix[caller][callee] = matrix[caller].get(callee, 0) + 1
                    
            return matrix
        except Exception as e:
            logger.error(f"Failed to generate collaboration matrix: {e}")
            return {}

    def get_causal_graph(self, limit: int = 200) -> Dict[str, Any]:
        """ì „ì²´ ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ê³¼ ê´€ê³„ ê·¸ë˜í”„(Nodes/Edges) ìƒì„±"""
        if not os.path.exists(self.log_path):
            return {"nodes": [], "edges": []}
            
        try:
            with open(self.log_path, "r", encoding="utf-8") as f:
                logs = [json.loads(line) for line in f][-limit:]
            
            nodes = []
            edges = []
            
            for l in logs:
                # ë…¸ë“œ ì •ë³´ êµ¬ì„±
                nodes.append({
                    "id": l["id"],
                    "label": f"{l['agent']}: {l['event']}",
                    "agent": l["agent"],
                    "event": l["event"],
                    "timestamp": l["timestamp"]
                })
                # ì¸ê³¼ ê´€ê³„ ì—£ì§€ êµ¬ì„±
                if l.get("cause_id"):
                    edges.append({"from": l["cause_id"], "to": l["id"]})
                    
            return {"nodes": nodes, "edges": edges}
        except Exception as e:
            logger.error(f"Failed to generate causal graph: {e}")
            return {"nodes": [], "edges": []}

# LangChain Callback í˜•ì‹ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ í˜•íƒœ ì œê³µ)
class FileLoggingCallbackHandler:
    def __init__(self, observer: GortexObserver):
        self.observer = observer

    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any):
        self.observer.log_event("Chain", "start", inputs)

    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any):
        self.observer.log_event("Tool", "start", input_str)

    def on_tool_end(self, output: str, **kwargs: Any):
        self.observer.log_event("Tool", "end", output)
