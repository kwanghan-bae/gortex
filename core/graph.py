import os
from typing import Dict, Any, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from gortex.core.state import GortexState
from gortex.utils.token_counter import count_tokens
from gortex.agents.manager import manager_node
from gortex.agents.planner import planner_node
from gortex.agents.coder import coder_node
from gortex.agents.researcher import researcher_node
from gortex.agents.analyst import analyst_node
from gortex.agents.trend_scout import trend_scout_node
from gortex.agents.optimizer import optimizer_node
from gortex.agents.swarm import swarm_node
from gortex.agents.evolution_node import evolution_node
from gortex.utils.memory import summarizer_node

def route_manager(state: GortexState) -> Literal["summarizer", "planner", "researcher", "analyst", "optimizer", "swarm", "evolution", "__end__"]:
    """Managerì˜ ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ë¼ìš°íŒ…."""
    next_node = state.get("next_node", "__end__")
    if next_node == "__end__":
        return "__end__"

    messages = state.get("messages", [])
    total_tokens = sum(count_tokens(m.content if hasattr(m, 'content') else str(m)) for m in messages)
    
    # [Dynamic Threshold] ë°±ì—”ë“œ íƒ€ì…ì— ë”°ë¥¸ ë™ì  ì„ê³„ê°’ ì ìš©
    backend_type = os.getenv("LLM_BACKEND", "hybrid").lower()
    msg_threshold = 8 if backend_type == "ollama" else 15
    token_threshold = 3000 if backend_type == "ollama" else 10000
    
    if len(messages) >= msg_threshold or total_tokens >= token_threshold:
        logger.info(f"Triggering summarizer (Messages: {len(messages)}, Tokens: {total_tokens})")
        return "summarizer"
        
    return "evolution" if next_node == "evolution" else next_node

def route_after_summary(state: GortexState) -> str:
    """ìš”ì•½ í›„ ì›ë˜ ê°€ë ¤ë˜ ë…¸ë“œë¡œ ë³µê·€"""
    return state.get("next_node", "manager")

def route_emergency(state: GortexState) -> Literal["analyst", "manager"]:
    """ì¹˜ëª…ì  ì—ëŸ¬ ê°ì§€ ì‹œ ììœ¨ ìˆ˜ë¦¬ ê²½ë¡œë¡œ ì•ˆë‚´"""
    messages = state.get("messages", [])
    if not messages: return "manager"
    
    last_msg = str(messages[-1][1] if isinstance(messages[-1], tuple) else messages[-1])
    if "âŒ" in last_msg or "error" in last_msg.lower():
        logger.warning("ğŸš¨ Emergency detected! Routing to Surgeon (Analyst).")
        return "analyst"
    return "manager"

def route_coder(state: GortexState) -> Literal["coder", "analyst", "__end__"]:
    """Coderì˜ ì‘ì—… ì™„ë£Œ ì—¬ë¶€ì— ë”°ë¼ ë¼ìš°íŒ…"""
    next_node = state.get("next_node", "manager")
    if next_node == "__end__":
        return "analyst"
    return "coder"

def compile_gortex_graph(checkpointer=None):
    """Gortex ì‹œìŠ¤í…œì˜ ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ê·¸ë˜í”„ ì»´íŒŒì¼"""
    workflow = StateGraph(GortexState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("manager", manager_node)
    workflow.add_node("planner", planner_node)
    workflow.add_node("coder", coder_node)
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("analyst", analyst_node)
    workflow.add_node("swarm", swarm_node)
    workflow.add_node("trend_scout", trend_scout_node)
    workflow.add_node("summarizer", summarizer_node)
    workflow.add_node("optimizer", optimizer_node)
    workflow.add_node("evolution", evolution_node)

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge(START, "trend_scout")
    workflow.add_edge("trend_scout", "manager")

    # Managerì˜ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "manager",
        route_manager,
        {
            "summarizer": "summarizer",
            "planner": "planner",
            "researcher": "researcher",
            "analyst": "analyst",
            "optimizer": "optimizer",
            "swarm": "swarm",
            "evolution": "evolution",
            "__end__": END
        }
    )

    # Summarizer -> Target
    workflow.add_conditional_edges(
        "summarizer",
        route_after_summary,
        {
            "planner": "planner",
            "researcher": "researcher",
            "analyst": "analyst",
            "optimizer": "optimizer",
            "swarm": "swarm",
            "evolution": "evolution",
            "manager": "manager"
        }
    )

    # Swarm, Researcher, Optimizer, Evolution ì™„ë£Œ í›„ Manager ë³µê·€
    workflow.add_edge("swarm", "manager")
    workflow.add_edge("researcher", "manager")
    workflow.add_edge("optimizer", "manager")
    workflow.add_edge("evolution", "manager")

    # Planner -> Coder
    workflow.add_edge("planner", "coder")

    # Coder ë£¨í”„ ë° ì™„ë£Œ í›„ Analyst ê²€ì¦ ë˜ëŠ” Emergency Patch
    workflow.add_conditional_edges(
        "coder",
        route_emergency,
        {
            "analyst": "analyst", # Repair mode
            "manager": "manager"  # Success
        }
    )

    # Analyst ì™„ë£Œ í›„ Manager ë³µê·€
    workflow.add_edge("analyst", "manager")

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    if checkpointer is not None:
        return workflow.compile(checkpointer=checkpointer)
    else:
        from langgraph.checkpoint.memory import MemorySaver
        return workflow.compile(checkpointer=MemorySaver())