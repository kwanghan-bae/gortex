import json
import os
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.core.llm.factory import LLMFactory

logger = logging.getLogger("GortexNeuralDistiller")

class NeuralDistiller:
    """
    ì‹œìŠ¤í…œì˜ í©ì–´ì§„ ì§€ì‹(Experience Shards)ì„ ë¶„ì„í•˜ì—¬ 
    í•µì‹¬ ì›ì¹™ìœ¼ë¡œ ì¦ë¥˜(Distillation)í•˜ê³  ìê°€ í•™ìŠµ ë°ì´í„°ì…‹ì„ ìƒì„±í•¨.
    """
    def __init__(self):
        self.memory = EvolutionaryMemory()
        self.backend = LLMFactory.get_default_backend()

    def distill_wisdom(self, category: str = "coding") -> Optional[str]:
        """íŠ¹ì • ë¶„ì•¼ì˜ ê³ ì„±ê³¼ ê·œì¹™ë“¤ì„ í•˜ë‚˜ì˜ ì •ì œëœ ì›ì¹™ìœ¼ë¡œ ì••ì¶•í•¨."""
        shard = self.memory.shards.get(category, [])
        # ì„±ê³µë¥  90% ì´ìƒ, ì‚¬ìš© 5íšŒ ì´ìƒì˜ 'ê³µì¸ëœ' ì§€ì‹ë§Œ ì„ ë³„
        certified = [r for r in shard if r.get("is_certified") or (r.get("usage_count", 0) >= 5 and (r.get("success_count", 0)/r["usage_count"]) >= 0.9)]
        
        if len(certified) < 3:
            return None
            
        logger.info(f"ğŸ”® Distilling wisdom from {len(certified)} certified rules in '{category}'...")
        
        rules_text = "\n".join([f"- {r['learned_instruction']}" for r in certified])
        prompt = f"""ë‹¹ì‹ ì€ ì§€ì‹ ì¦ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ {category} ë¶„ì•¼ì˜ ê³µì¸ëœ ì§€ì‹ë“¤ì„ ë¶„ì„í•˜ì—¬, 
        ì—ì´ì „íŠ¸ê°€ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  í•˜ë‚˜ì˜ í†µí•©ëœ 'ìµœìƒìœ„ ì›ì¹™'ìœ¼ë¡œ ìš”ì•½í•˜ì‹­ì‹œì˜¤.
        ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë¥¼ ë¹¼ê³  ë§¤ìš° ì—„ê²©í•˜ê³  ê¸°ìˆ ì ì¸ ë¬¸ì²´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        
        [Certified Knowledge]:
        {rules_text}
        """
        
        try:
            distilled = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            return distilled.strip()
        except Exception as e:
            logger.error(f"Distillation failed: {e}")
            return None

    def prepare_training_dataset(self, output_dir: str = "training_jobs"):
        """ì„±ê³µì ì¸ ë²„ê·¸ ìˆ˜ì • ë° ìµœì í™” ì‚¬ë¡€ë¥¼ LLM Fine-tuningìš© JSONLë¡œ ë³€í™˜."""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(output_dir, f"gortex_dataset_{timestamp}.jsonl")
        
        # 1. ëª¨ë“  ìƒ¤ë“œì—ì„œ ì§€ì‹ ìˆ˜ì§‘
        all_rules = self.memory.memory
        valid_samples = 0
        
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                for rule in all_rules:
                    # ë¬¸ë§¥ê³¼ í•´ë‹µì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ ì‚¬ìš©
                    if rule.get("context") and rule.get("learned_instruction"):
                        entry = {
                            "instruction": f"As a Gortex Agent, provide a solution for the following technical situation in {rule.get('category', 'general')} domain.",
                            "input": rule["context"],
                            "output": rule["learned_instruction"]
                        }
                        f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                        valid_samples += 1
            
            if valid_samples > 0:
                logger.info(f"ğŸ“‚ Created self-learning dataset with {valid_samples} samples: {output_path}")
                return output_path
            else:
                os.remove(output_path)
                return None
        except Exception as e:
            logger.error(f"Failed to prepare dataset: {e}")
            return None

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
distiller = NeuralDistiller()
