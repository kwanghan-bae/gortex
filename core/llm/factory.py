import os
import logging
from typing import Literal
from gortex.core.llm.base import LLMBackend
from gortex.core.llm.gemini_client import GeminiBackend
from gortex.core.llm.ollama_client import OllamaBackend

logger = logging.getLogger("GortexLLMFactory")

class HybridBackend(LLMBackend):
    """
    í´ë¼ìš°ë“œ(Gemini)ì™€ ë¡œì»¬(Ollama) ëª¨ë¸ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë°±ì—”ë“œ.
    Gemini ì‹¤íŒ¨ ì‹œ Ollamaë¡œ ìë™ í´ë°±í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        self.gemini = GeminiBackend()
        self.ollama = OllamaBackend()

    def generate(self, model: str, messages: List[dict], config: Optional[Dict[str, Any]] = None) -> str:
        try:
            # 1. ìš°ì„  Gemini ì‹œë„
            return self.gemini.generate(model, messages, config)
        except Exception as e:
            # 429(Quota), 500 ë“± ì¥ì•  ë°œìƒ ì‹œ Ollamaë¡œ ì „í™˜
            logger.warning(f"Gemini failed, falling back to Ollama: {e}")
            if self.ollama.is_available():
                # ë¡œì»¬ ëª¨ë¸ëª… ë§¤í•‘ (ê¸°ë³¸ê°’: llama3)
                local_model = os.getenv("OLLAMA_MODEL", "llama3")
                return self.ollama.generate(local_model, messages, config)
            else:
                logger.error("Ollama is also unavailable.")
                raise e

    def is_available(self) -> bool:
        return self.gemini.is_available() or self.ollama.is_available()

    def supports_structured_output(self) -> bool:
        return self.gemini.supports_structured_output()

    def supports_function_calling(self) -> bool:
        return self.gemini.supports_function_calling()

class LLMFactory:
    _instances = {}

    @staticmethod
    def get_model_for_grade(grade: str, daily_cost: float = 0.0, budget_limit: float = 0.5) -> str:
        """
        ì—ì´ì „íŠ¸ ë“±ê¸‰ ë° ì˜ˆì‚° ìƒí™©ì— ìµœì í™”ëœ ëª¨ë¸ í• ë‹¹.
        ì˜ˆì‚° ì†Œì§„ìœ¨ì— ë”°ë¼ ìë™ìœ¼ë¡œ ëª¨ë¸ì„ í•˜í–¥(Downgrade) ì¡°ì •í•¨.
        """
        # ë“±ê¸‰ë³„ ê¸°ë³¸ ëª¨ë¸ ì •ì˜
        grade_map = {
            "Diamond": "gemini-2.0-flash", 
            "Gold": "gemini-pro-latest",   
            "Silver": "gemini-1.5-flash",  
            "Bronze": "gemini-2.5-flash-lite" 
        }
        
        selected_model = grade_map.get(grade, "ollama/llama3")
        
        # [Economic Defense] ì˜ˆì‚° ê¸°ë°˜ í•˜í–¥ ì¡°ì •
        budget_usage = daily_cost / budget_limit if budget_limit > 0 else 0
        
        if budget_usage > 0.9: # 90% ì†Œì§„ ì‹œ
            logger.warning(f"ğŸ’¸ Budget critical ({budget_usage*100:.1f}%). Downgrading to lightweight/local models.")
            return "ollama/llama3"
        elif budget_usage > 0.7: # 70% ì†Œì§„ ì‹œ
            # í•œ ë‹¨ê³„ì”© í•˜í–¥
            downgrade_map = {
                "gemini-2.0-flash": "gemini-1.5-flash",
                "gemini-pro-latest": "gemini-1.5-flash",
                "gemini-1.5-flash": "gemini-2.5-flash-lite",
                "gemini-2.5-flash-lite": "ollama/llama3"
            }
            return downgrade_map.get(selected_model, "ollama/llama3")
            
        return selected_model

    @staticmethod
    def get_backend(backend_type: Literal["gemini", "ollama", "hybrid"] = "hybrid") -> LLMBackend:
        """
        ìš”ì²­ëœ íƒ€ì…ì— ë§ëŠ” ì‹±ê¸€í†¤ ë°±ì—”ë“œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤.
        """
        if backend_type not in LLMFactory._instances:
            logger.info(f"Initializing LLM Backend: {backend_type}")
            if backend_type == "gemini":
                LLMFactory._instances[backend_type] = GeminiBackend()
            elif backend_type == "ollama":
                LLMFactory._instances[backend_type] = OllamaBackend()
            elif backend_type == "hybrid":
                LLMFactory._instances[backend_type] = HybridBackend()
            else:
                raise ValueError(f"Unknown backend type: {backend_type}")
        
        return LLMFactory._instances[backend_type]

    @staticmethod
    def get_default_backend() -> LLMBackend:
        """
        í™˜ê²½ ë³€ìˆ˜ LLM_BACKENDì— ë”°ë¼ ê¸°ë³¸ ë°±ì—”ë“œë¥¼ ë°˜í™˜í•œë‹¤. (ê¸°ë³¸ê°’: hybrid)
        """
        backend_name = os.getenv("LLM_BACKEND", "hybrid").lower()
        if backend_name not in ["gemini", "ollama", "hybrid"]:
            logger.warning(f"Unsupported LLM_BACKEND '{backend_name}', falling back to Hybrid.")
            backend_name = "hybrid"
        
        return LLMFactory.get_backend(backend_name)  # type: ignore
