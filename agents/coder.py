import logging
import json
import time
from typing import Dict, Any, List
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.utils.tools import read_file, write_file, execute_shell, list_files, get_file_hash, apply_patch, scan_security_risks
from gortex.utils.healing_memory import SelfHealingMemory

logger = logging.getLogger("GortexCoder")

def coder_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ê°œë°œì(Coder) ë…¸ë“œ.
    Plannerê°€ ìˆ˜ë¦½í•œ ê³„íšì„ í•œ ë‹¨ê³„ì”© ì‹¤í–‰í•˜ë©°, ê²€ì¦(Verification)ì„ í†µí•´ ì½”ë“œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
    """
    auth = GortexAuth()
    healing_mem = SelfHealingMemory()
    
    # 0. ë°˜ë³µ íšŸìˆ˜ ì²´í¬
    current_iteration = state.get("coder_iteration", 0)
    if current_iteration >= 30:
        logger.warning("Coder iteration limit reached.")
        return {
            "messages": [("ai", "âŒ ì•ˆì „ì„ ìœ„í•´ Coder ë£¨í”„ë¥¼ 30íšŒì—ì„œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")],
            "next_node": "__end__"
        }
    
    # 1. í˜„ì¬ ì‹¤í–‰í•  ë‹¨ê³„ ê°€ì ¸ì˜¤ê¸°
    plan = state.get("plan", [])
    current_step_idx = state.get("current_step", 0)
    
    if current_step_idx >= len(plan):
        return {
            "messages": [("ai", "âœ… ëª¨ë“  ê³„íšëœ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")],
            "next_node": "__end__"
        }
    
    current_step_json = plan[current_step_idx]
    try:
        current_step = json.loads(current_step_json)
    except:
        current_step = {"action": "unknown", "target": "unknown"}
        
    logger.info(f"Executing Step {current_step_idx + 1}: {current_step['action']} -> {current_step['target']}")
    
    # 2. ë„êµ¬ ì‹¤í–‰
    tool_output = ""
    action = current_step["action"]
    target = current_step["target"]
    
    if action == "read_file":
        tool_output = read_file(target)
    elif action in ["write_file", "apply_patch"]:
        pass # LLMì—ì„œ ì²˜ë¦¬
    elif action == "execute_shell":
        tool_output = execute_shell(target)
        # [SELF-HEALING] ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ê°ì ì¸ í•´ê²°ì±… ê²€ìƒ‰
        if "Exit Code: 0" not in tool_output:
            instant_solution = healing_mem.find_solution(tool_output)
            if instant_solution:
                logger.info(f"ğŸ©¹ Instant healing solution found!")
                state["messages"].append(("system", f"HINT: ê³¼ê±° í•´ê²°ì±… ë°œê²¬. '{instant_solution['action']}'(target: {instant_solution['target']})ì„ ì‹œë„í•˜ì‹­ì‹œì˜¤."))
    elif action == "list_files":
        tool_output = list_files(target)
    
    # 3. Gemini í˜¸ì¶œ
    # f-string ë‚´ì˜ ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ì£¼ì˜ ({{, }})
    base_instruction = f"""ë„ˆëŠ” Gortex v1.0ì˜ ìˆ˜ì„ ê°œë°œì(Coder)ë‹¤.
í˜„ì¬ Plannerê°€ ìˆ˜ë¦½í•œ ê³„íš ì¤‘ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•´ì•¼ í•œë‹¤.

[Self-Healing]
- ì‹œìŠ¤í…œ íŒíŠ¸(HINT)ë¡œ ê³¼ê±° í•´ê²°ì±…ì´ ì œê³µë˜ë©´, ì´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì ìš©í•˜ë¼.

[Precision Editing Rules]
- íŒŒì¼ ì „ì²´ë¥¼ ë°”ê¾¸ê¸°ë³´ë‹¤ íŠ¹ì • ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë¼ë©´ `apply_patch` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë¼.

[Mental Sandbox Rules]
ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê¸° ì „, ë°˜ë“œì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ë¯¸ë¦¬ 'ì‹œë®¬ë ˆì´ì…˜'í•˜ë¼:
1. ì˜ˆìƒ ê²°ê³¼ ë° ìœ„í—˜ ë¶„ì„
2. ì•ˆì „ ê°€ë“œ: ìœ„í—˜ ì‹œ 'failed' ìƒíƒœì™€ í•¨ê»˜ ëŒ€ì•ˆ ì œì‹œ

[Standard Error Response Manual]
- ModuleNotFoundError: ì¦‰ì‹œ `execute_shell`ë¡œ `pip install <module>`ì„ ì‹¤í–‰í•˜ë¼.
- IndentationError/SyntaxError: `read_file`ë¡œ ë‹¤ì‹œ ì½ì–´ ë“¤ì—¬ì“°ê¸°ë¥¼ ì ê²€í•˜ë¼.

[Current Step]
{json.dumps(current_step, ensure_ascii=False, indent=2)}

[Tool Output / Context]
{tool_output if tool_output else "(Not executed yet)"}

[Your Mission]
1. ìœ„ ë‹¨ê³„ê°€ 'write_file' ë˜ëŠ” 'apply_patch'ë¼ë©´, í•„ìš”í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ë¼. **[Reflective Validation]**: ìˆ˜ì • ì§í›„ì—ëŠ” ë°˜ë“œì‹œ `execute_shell`ë¡œ ê´€ë ¨ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ìê°€ ê²€ì¦í•˜ë¼.
2. ì‹¤í–‰ ê²°ê³¼ì— ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ ì •ë°€ ë¶„ì„í•˜ì—¬ ìˆ˜ì •í•˜ë¼. ë°˜ë³µ ì‹¤íŒ¨ ì‹œ 'failed'ë¥¼ ë°˜í™˜í•˜ë¼.
3. ì„±ê³µ ì‹œ 'status': 'success'ë¥¼ ë°˜í™˜í•˜ë¼.

[Available Tools]
- read_file(path)
- write_file(path, content)
- apply_patch(path, start_line, end_line, new_content)
- execute_shell(command)
- list_files(path)

[Output Schema (Strict JSON)]
{{{{
  "thought": "ìƒê°ì˜ ê³¼ì •",
  "thought_tree": [ {{{{ "id": "1", "text": "...", "type": "analysis", "priority": 3, "certainty": 0.9 }}}} ],
  "simulation": {{{{ 
      "expected_outcome": "...", 
      "risk_level": "Low/Medium/High", 
      "safeguard_action": "...",
      "visual_delta": []
  }}}},
  "status": "success" | "in_progress" | "failed"
}}}} """
    
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES]\n{constraints_str}"

    config = types.GenerateContentConfig(
        system_instruction=base_instruction,
        temperature=0.0,
        response_mime_type="application/json",
        tools=[read_file, write_file, execute_shell, list_files, apply_patch],
        response_schema={
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING"},
                "thought_tree": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "STRING"},
                            "parent_id": {"type": "STRING", "nullable": True},
                            "text": {"type": "STRING"},
                            "type": {"type": "STRING", "enum": ["analysis", "action", "verification", "simulation"]},
                            "priority": {"type": "INTEGER"},
                            "certainty": {"type": "NUMBER"}
                        },
                        "required": ["id", "text", "type", "priority", "certainty"]
                    }
                },
                "simulation": {
                    "type": "OBJECT",
                    "properties": {
                        "expected_outcome": {"type": "STRING"},
                        "risk_level": {"type": "STRING", "enum": ["Low", "Medium", "High"]},
                        "safeguard_action": {"type": "STRING"},
                        "visual_delta": {
                            "type": "ARRAY",
                            "items": {
                                "type": "OBJECT",
                                "properties": {
                                    "target": {"type": "STRING"},
                                    "change": {"type": "STRING", "enum": ["added", "modified", "deleted"]}
                                },
                                "required": ["target", "change"]
                            }
                        }
                    },
                    "required": ["expected_outcome", "risk_level", "safeguard_action", "visual_delta"]
                },
                "status": {"type": "STRING", "enum": ["success", "in_progress", "failed"]}
            },
            "required": ["thought", "thought_tree", "simulation", "status"]
        }
    )
    
    response = auth.generate(model_id="gemini-3-flash-preview", contents=state["messages"], config=config)
    
    function_calls = []
    if response.candidates and response.candidates[0].content.parts:
        for part in response.candidates[0].content.parts:
            if part.function_call:
                function_calls.append(part.function_call)

    try:
        res_data = response.parsed if hasattr(response, 'parsed') else json.loads(response.text)
        coder_thought = res_data.get("thought", "")
        coder_tree = res_data.get("thought_tree", [])
        status = res_data.get("status", "in_progress")
    except:
        coder_thought = "Processing..."
        coder_tree = []
        status = "in_progress"

    if function_calls:
        fc = function_calls[0]
        fname = fc.name
        fargs = fc.args
        result_msg = ""
        new_file_cache = state.get("file_cache", {}).copy()

        # [SECURITY SCAN] ë„êµ¬ í˜¸ì¶œ ì „ ì‹¤ì‹œê°„ ë³´ì•ˆ ê²€ì‚¬
        if fname in ["write_file", "apply_patch"]:
            code_to_check = fargs.get("content") or fargs.get("new_content", "")
            risks = scan_security_risks(code_to_check)
            if risks:
                logger.warning(f"ğŸš¨ Security risks detected!")
                return {
                    "thought": f"ë³´ì•ˆ ì·¨ì•½ì  ê°ì§€: {risks[0]['type']}",
                    "thought_tree": coder_tree,
                    "coder_iteration": current_iteration + 1,
                    "messages": [
                        ("ai", f"âŒ ë³´ì•ˆ ì·¨ì•½ì ({risks[0]['type']}) ê°ì§€ë¡œ ì‹¤í–‰ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."),
                        ("system", "ë³´ì•ˆ ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í•˜ì—¬ ë‹¤ì‹œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.")
                    ],
                    "next_node": "coder"
                }

        if fname == "write_file":
            result_msg = write_file(fargs["path"], fargs["content"])
            new_file_cache[fargs["path"]] = get_file_hash(fargs["path"])
        elif fname == "apply_patch":
            result_msg = apply_patch(fargs["path"], int(fargs["start_line"]), int(fargs["end_line"]), fargs["new_content"])
            new_file_cache[fargs["path"]] = get_file_hash(fargs["path"])
        elif fname == "read_file":
            path = fargs["path"]
            current_hash = get_file_hash(path)
            if new_file_cache.get(path) == current_hash and current_hash != "":
                result_msg = "(Cache Hit) Content unchanged."
            else:
                result_msg = read_file(path)
                new_file_cache[path] = current_hash
        elif fname == "execute_shell":
            result_msg = execute_shell(fargs["command"])
            # ì„±ê³µ ì‹œ í•™ìŠµ
            if "Exit Code: 0" in result_msg and "pip install" in fargs["command"]:
                healing_mem.learn("ModuleNotFoundError", {"action": "execute_shell", "target": fargs["command"]})
        elif fname == "list_files":
            result_msg = list_files(fargs.get("directory", "."))
            
        return {
            "thought": coder_thought, "thought_tree": coder_tree,
            "coder_iteration": current_iteration + 1, "file_cache": new_file_cache,
            "messages": [("ai", f"Executed {fname}"), ("tool", result_msg)],
            "next_node": "coder"
        }

    if status == "success":
        return {
            "thought": coder_thought, "thought_tree": coder_tree,
            "current_step": current_step_idx + 1, "coder_iteration": 0,
            "next_node": "coder", "messages": [("ai", f"Step {current_step_idx+1} ì™„ë£Œ.")]
        }
    elif status == "failed":
        return {
            "thought": f"Failed: {coder_thought}", "thought_tree": coder_tree,
            "next_node": "analyst", "messages": [("ai", "âš ï¸ ë°˜ë³µ ì‹¤íŒ¨ë¡œ ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤.")]
        }
    else:
        return {
            "thought": coder_thought, "thought_tree": coder_tree,
            "coder_iteration": current_iteration + 1, "next_node": "coder"
        }
