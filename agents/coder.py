import logging
import json
import time
from typing import Dict, Any, List
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.utils.tools import read_file, write_file, execute_shell, list_files, get_file_hash, apply_patch, scan_security_risks
from gortex.utils.healing_memory import SelfHealingMemory

logger = logging.getLogger("GortexCoder")

def coder_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ê°œë°œì(Coder) ë…¸ë“œ.
    Plannerê°€ ìˆ˜ë¦½í•œ ê³„íšì„ í•œ ë‹¨ê³„ì”© ì‹¤í–‰í•˜ë©°, ê²€ì¦(Verification)ì„ í†µí•´ ì½”ë“œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
    """
    auth = GortexAuth()
    healing_mem = SelfHealingMemory()
    
    # 0. ë°˜ë³µ íšŸìˆ˜ ì²´í¬
    current_iteration = state.get("coder_iteration", 0)
    if current_iteration >= 30:
        logger.warning("Coder iteration limit reached.")
        return {
            "messages": [("ai", "âŒ ì•ˆì „ì„ ìœ„í•´ Coder ë£¨í”„ë¥¼ 30íšŒì—ì„œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")],
            "next_node": "__end__"
        }
    
    # 1. í˜„ì¬ ì‹¤í–‰í•  ë‹¨ê³„ ê°€ì ¸ì˜¤ê¸°
    plan = state.get("plan", [])
    current_step_idx = state.get("current_step", 0)
    
    if current_step_idx >= len(plan):
        return {
            "messages": [("ai", "âœ… ëª¨ë“  ê³„íšëœ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")],
            "next_node": "__end__"
        }
    
    current_step_json = plan[current_step_idx]
    try:
        current_step = json.loads(current_step_json)
    except:
        current_step = {"action": "unknown", "target": "unknown"}
        
    logger.info(f"Executing Step {current_step_idx + 1}: {current_step['action']} -> {current_step['target']}")
    
    # 2. ë„êµ¬ ì‹¤í–‰
    tool_output = ""
    action = current_step["action"]
    target = current_step["target"]
    
    if action == "read_file":
        tool_output = read_file(target)
    elif action in ["write_file", "apply_patch"]:
        pass # LLMì—ì„œ ì²˜ë¦¬
    elif action == "execute_shell":
        tool_output = execute_shell(target)
        # [SELF-HEALING] ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ê°ì ì¸ í•´ê²°ì±… ê²€ìƒ‰
        if "Exit Code: 0" not in tool_output:
            instant_solution = healing_mem.find_solution(tool_output)
            if instant_solution:
                logger.info(f"ğŸ©¹ Instant healing solution found!")
                state["messages"].append(("system", f"HINT: ê³¼ê±° í•´ê²°ì±… ë°œê²¬. '{instant_solution['action']}'(target: {instant_solution['target']})ì„ ì‹œë„í•˜ì‹­ì‹œì˜¤."))
    elif action == "list_files":
        tool_output = list_files(target)
    
    # 3. Gemini í˜¸ì¶œ (ì™¸ë¶€ í…œí”Œë¦¿ ë¡œë“œ)
    from gortex.utils.prompt_loader import loader
    base_instruction = loader.get_prompt(
        "coder", 
        current_step_json=json.dumps(current_step, ensure_ascii=False, indent=2),
        tool_output=(tool_output if tool_output else "(Not executed yet)")
    )
    
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES]\n{constraints_str}"

    config = types.GenerateContentConfig(
        system_instruction=base_instruction,
        temperature=0.0,
        response_mime_type="application/json",
        tools=[read_file, write_file, execute_shell, list_files, apply_patch],
        response_schema={
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING"},
                "thought_tree": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "STRING"},
                            "parent_id": {"type": "STRING", "nullable": True},
                            "text": {"type": "STRING"},
                            "type": {"type": "STRING", "enum": ["analysis", "action", "verification", "simulation"]},
                            "priority": {"type": "INTEGER"},
                            "certainty": {"type": "NUMBER"}
                        },
                        "required": ["id", "text", "type", "priority", "certainty"]
                    }
                },
                "simulation": {
                    "type": "OBJECT",
                    "properties": {
                        "expected_outcome": {"type": "STRING"},
                        "risk_level": {"type": "STRING", "enum": ["Low", "Medium", "High"]},
                        "safeguard_action": {"type": "STRING"},
                        "visual_delta": {
                            "type": "ARRAY",
                            "items": {
                                "type": "OBJECT",
                                "properties": {
                                    "target": {"type": "STRING"},
                                    "change": {"type": "STRING", "enum": ["added", "modified", "deleted"]}
                                },
                                "required": ["target", "change"]
                            }
                        },
                        "expected_graph_delta": {
                            "type": "OBJECT",
                            "properties": {
                                "added_nodes": {"type": "ARRAY", "items": {"type": "STRING"}},
                                "modified_nodes": {"type": "ARRAY", "items": {"type": "STRING"}},
                                "deleted_nodes": {"type": "ARRAY", "items": {"type": "STRING"}}
                            }
                        }
                    },
                    "required": ["expected_outcome", "risk_level", "safeguard_action", "visual_delta"]
                },
                "status": {"type": "STRING", "enum": ["success", "in_progress", "failed"]}
            },
            "required": ["thought", "thought_tree", "simulation", "status"]
        }
    )
    
    # [Dynamic Model] Managerê°€ í• ë‹¹í•œ ëª¨ë¸ ì‚¬ìš©
    assigned_model = state.get("assigned_model", "gemini-1.5-flash")
    logger.info(f"Coder using model: {assigned_model}")
    
    response = auth.generate(model_id=assigned_model, contents=state["messages"], config=config)
    
    function_calls = []
    if response.candidates and response.candidates[0].content.parts:
        for part in response.candidates[0].content.parts:
            if part.function_call:
                function_calls.append(part.function_call)

    try:
        res_data = response.parsed if hasattr(response, 'parsed') else json.loads(response.text)
        coder_thought = res_data.get("thought", "")
        coder_tree = res_data.get("thought_tree", [])
        status = res_data.get("status", "in_progress")
    except:
        coder_thought = "Processing..."
        coder_tree = []
        status = "in_progress"

    if function_calls:
        fc = function_calls[0]
        fname = fc.name
        fargs = fc.args
        result_msg = ""
        new_file_cache = state.get("file_cache", {}).copy()

        # [Compliance Check] ë„êµ¬ ì‹¤í–‰ ì „ ì‹¤ì‹œê°„ ì œì•½ ì¡°ê±´ ê²€ì¦
        from gortex.agents.analyst import AnalystAgent
        compliance_res = AnalystAgent().validate_constraints(
            state.get("active_constraints", []),
            {"action": fname, "target": fargs.get("path") or fargs.get("command") or fargs.get("directory"), "args": fargs}
        )
        
        if not compliance_res.get("is_valid", True):
            logger.warning(f"ğŸ›¡ï¸ Policy violation detected: {compliance_res.get('reason')}")
            return {
                "thought": f"ì •ì±… ìœ„ë°˜ ê°ì§€: {compliance_res.get('reason')}",
                "thought_tree": coder_tree,
                "coder_iteration": current_iteration + 1,
                "messages": [
                    ("ai", f"âŒ ì‹œìŠ¤í…œ ì •ì±… ìœ„ë°˜ìœ¼ë¡œ ì‹¤í–‰ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."),
                    ("system", f"ìœ„ë°˜ ê·œì¹™: {', '.join(compliance_res.get('violated_rules', []))}\nì‚¬ìœ : {compliance_res.get('reason')}\nê¶Œê³ : {compliance_res.get('remedy')}")
                ],
                "next_node": "coder"
            }

        # [SECURITY SCAN] ë„êµ¬ í˜¸ì¶œ ì „ ì‹¤ì‹œê°„ ë³´ì•ˆ ê²€ì‚¬ (ê¸°ì¡´ ë¡œì§)
        if fname in ["write_file", "apply_patch"]:
            code_to_check = fargs.get("content") or fargs.get("new_content", "")
            risks = scan_security_risks(code_to_check)
            if risks:
                logger.warning(f"ğŸš¨ Security risks detected!")
                return {
                    "thought": f"ë³´ì•ˆ ì·¨ì•½ì  ê°ì§€: {risks[0]['type']}",
                    "thought_tree": coder_tree,
                    "coder_iteration": current_iteration + 1,
                    "messages": [
                        ("ai", f"âŒ ë³´ì•ˆ ì·¨ì•½ì ({risks[0]['type']}) ê°ì§€ë¡œ ì‹¤í–‰ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."),
                        ("system", "ë³´ì•ˆ ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í•˜ì—¬ ë‹¤ì‹œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.")
                    ],
                    "next_node": "coder"
                }

        if fname == "write_file":
            result_msg = write_file(fargs["path"], fargs["content"])
            new_file_cache[fargs["path"]] = get_file_hash(fargs["path"])
        elif fname == "apply_patch":
            result_msg = apply_patch(fargs["path"], int(fargs["start_line"]), int(fargs["end_line"]), fargs["new_content"])
            new_file_cache[fargs["path"]] = get_file_hash(fargs["path"])
        elif fname == "read_file":
            path = fargs["path"]
            current_hash = get_file_hash(path)
            if new_file_cache.get(path) == current_hash and current_hash != "":
                result_msg = "(Cache Hit) Content unchanged."
            else:
                result_msg = read_file(path)
                new_file_cache[path] = current_hash
        elif fname == "execute_shell":
            result_msg = execute_shell(fargs["command"])
            # ì„±ê³µ ì‹œ í•™ìŠµ
            if "Exit Code: 0" in result_msg and "pip install" in fargs["command"]:
                healing_mem.learn("ModuleNotFoundError", {"action": "execute_shell", "target": fargs["command"]})
        elif fname == "list_files":
            result_msg = list_files(fargs.get("directory", "."))
            
        return {
            "thought": coder_thought, "thought_tree": coder_tree,
            "coder_iteration": current_iteration + 1, "file_cache": new_file_cache,
            "messages": [("ai", f"Executed {fname}"), ("tool", result_msg)],
            "next_node": "coder"
        }

    if status == "success":
        # [Autonomous Pre-Commit] ì„±ê³µ ë³´ê³  ì „ ììœ¨ ê²€ì¦ ìˆ˜í–‰
        logger.info("ğŸ§ª Running autonomous pre-commit check...")
        check_res = execute_shell("./scripts/pre_commit.sh")
        
        if "Ready to commit" in check_res:
            logger.info("âœ… Autonomous check passed.")
            return {
                "thought": coder_thought, "thought_tree": coder_tree,
                "current_step": current_step_idx + 1, "coder_iteration": 0,
                "next_node": "coder", "messages": [("ai", f"Step {current_step_idx+1} ì™„ë£Œ ë° ê²€ì¦ í†µê³¼.")]
            }
        else:
            logger.warning("âŒ Autonomous check failed. Triggering self-correction...")
            # ì‹¤íŒ¨ ë¡œê·¸ì™€ í•¨ê»˜ ë‹¤ì‹œ Coderì—ê²Œ ê¸°íšŒ ë¶€ì—¬ (ë˜ëŠ” Analystë¡œ ë¼ìš°íŒ…)
            return {
                "thought": f"Pre-commit failed after success attempt. Needs correction. Log: {check_res[:200]}",
                "thought_tree": coder_tree,
                "coder_iteration": current_iteration + 1,
                "messages": [
                    ("ai", "âŒ ììœ¨ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì¸í•´ ìê°€ ìˆ˜ì •ì„ ì‹œë„í•©ë‹ˆë‹¤."),
                    ("tool", check_res)
                ],
                "next_node": "coder"
            }
            
    elif status == "failed":
        # [Reflective Debugging] ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ë° ê·œì¹™ ìƒì„±
        from gortex.agents.analyst import AnalystAgent
        analyst = AnalystAgent()
        rule_data = analyst.generate_anti_failure_rule(tool_output, coder_thought)
        
        msg = "âš ï¸ ë°˜ë³µ ì‹¤íŒ¨ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤."
        if rule_data:
            msg += f"\nğŸ›¡ï¸ ìƒˆë¡œìš´ ë°©ì–´ ê·œì¹™ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {rule_data['instruction']}"
            
        return {
            "thought": f"Failed: {coder_thought}. Reflection complete.", "thought_tree": coder_tree,
            "next_node": "analyst", "messages": [("ai", msg)]
        }
    else:
        return {
            "thought": coder_thought, "thought_tree": coder_tree,
            "coder_iteration": current_iteration + 1, "next_node": "coder"
        }
