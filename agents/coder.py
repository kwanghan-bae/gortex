import logging
import json
import time
import re
from typing import Dict, Any, List, Optional
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.utils.tools import read_file, write_file, execute_shell, list_files, get_file_hash, apply_patch, scan_security_risks
from gortex.utils.healing_memory import SelfHealingMemory

logger = logging.getLogger("GortexCoder")

def coder_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ê°œë°œì(Coder) ë…¸ë“œ.
    Plannerê°€ ìˆ˜ë¦½í•œ ê³„íšì„ í•œ ë‹¨ê³„ì”© ì‹¤í–‰í•˜ë©°, ê²€ì¦(Verification)ì„ í†µí•´ ì½”ë“œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
    (Ollama/Gemini í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)
    """
    backend = LLMFactory.get_default_backend()
    healing_mem = SelfHealingMemory()
    
    # 0. ë°˜ë³µ íšŸìˆ˜ ì²´í¬
    current_iteration = state.get("coder_iteration", 0)
    if current_iteration >= 30:
        logger.warning("Coder iteration limit reached.")
        return {
            "messages": [("ai", "âŒ ì•ˆì „ì„ ìœ„í•´ Coder ë£¨í”„ë¥¼ 30íšŒì—ì„œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")],
            "next_node": "__end__"
        }
    
    # 1. í˜„ì¬ ì‹¤í–‰í•  ë‹¨ê³„ ê°€ì ¸ì˜¤ê¸°
    plan = state.get("plan", [])
    current_step_idx = state.get("current_step", 0)
    
    if current_step_idx >= len(plan):
        return {
            "messages": [("ai", "âœ… ëª¨ë“  ê³„íšëœ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")],
            "next_node": "__end__"
        }
    
    current_step_json = plan[current_step_idx]
    try:
        current_step = json.loads(current_step_json)
    except:
        current_step = {"action": "unknown", "target": "unknown"}
        
    logger.info(f"Executing Step {current_step_idx + 1}: {current_step['action']} -> {current_step['target']}")
    
    # 2. ë„êµ¬ ì‹¤í–‰
    tool_output = ""
    action = current_step["action"]
    target = current_step["target"]
    
    if action == "read_file":
        tool_output = read_file(target)
    elif action in ["write_file", "apply_patch"]:
        pass # LLMì—ì„œ ì²˜ë¦¬
    elif action == "execute_shell":
        tool_output = execute_shell(target)
        if "Exit Code: 0" not in tool_output:
            instant_solution = healing_mem.find_solution(tool_output)
            if instant_solution:
                logger.info(f"ğŸ©¹ Instant healing solution found!")
                state["messages"].append(("system", f"HINT: ê³¼ê±° í•´ê²°ì±… ë°œê²¬. '{instant_solution['action']}'(target: {instant_solution['target']})ì„ ì‹œë„í•˜ì‹­ì‹œì˜¤."))
    elif action == "list_files":
        tool_output = list_files(target)
    
    # 3. LLM í˜¸ì¶œ ì¤€ë¹„
    from gortex.utils.prompt_loader import loader
    base_instruction = loader.get_prompt(
        "coder", 
        persona_id=state.get("assigned_persona", "standard"),
        current_step_json=json.dumps(current_step, ensure_ascii=False, indent=2),
        tool_output=(tool_output if tool_output else "(Not executed yet)")
    )
    
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES]\n{constraints_str}"

    # ë°±ì—”ë“œ ëŠ¥ë ¥ì— ë”°ë¥¸ ì„¤ì • ë¶„ê¸°
    assigned_model = state.get("assigned_model", "gemini-1.5-flash")
    config = {"temperature": 0.0}
    
    # [Hybrid Strategy] Native ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ ë³´ê°•
    if not backend.supports_structured_output():
        base_instruction += "\n[IMPORTANT: OUTPUT FORMAT]\nYou must respond in the following JSON format ONLY. Do not include any other text outside the JSON block."
        base_instruction += "{\n  \"thought\": \"Your reasoning here\",\n  \"thought_tree\": [{\"id\": \"1\", \"text\": \"...\", \"type\": \"analysis\", \"priority\": 1, \"certainty\": 0.9}],\n  \"simulation\": {\n    \"expected_outcome\": \"...\",\n    \"risk_level\": \"Low|Medium|High\",\n    \"safeguard_action\": \"...\",\n    \"visual_delta\": [{\"target\": \"file.py\", \"change\": \"modified\"}]\n  },\n  \"action\": \"write_file|apply_patch|execute_shell|read_file|list_files|none\",\n  \"action_input\": { ... parameters for the action ... },\n  \"status\": \"success|in_progress|failed\"\n}"
    else:
        # Gemini ë“± Native ì§€ì› ì‹œ ì „ìš© ê°ì²´ êµ¬ì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€ ì‹œë„)
        from google.genai import types
        gemini_config = types.GenerateContentConfig(
            system_instruction=base_instruction,
            temperature=0.0,
            response_mime_type="application/json",
            tools=[read_file, write_file, execute_shell, list_files, apply_patch],
            # schema ìƒëµ (GeminiBackendê°€ ì²˜ë¦¬í•˜ê±°ë‚˜ ì—¬ê¸°ì„œ ë„˜ê¹€)
        )
        config = gemini_config

    # ë©”ì‹œì§€ ë³€í™˜ (LLMBackend í‘œì¤€ í¬ë§·)
    formatted_messages = []
    # ì‹œìŠ¤í…œ ì§€ì¹¨ì„ ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¡œ (ë˜ëŠ” configì— í¬í•¨)
    formatted_messages.append({"role": "system", "content": base_instruction})
    for m in state["messages"]:
        role = m[0]
        content = m[1]
        formatted_messages.append({"role": role, "content": content})

    # LLM í˜¸ì¶œ
    logger.info(f"Coder calling backend with model: {assigned_model}")
    try:
        response_text = backend.generate(model=assigned_model, messages=formatted_messages, config=config)
    except Exception as e:
        logger.error(f"LLM generation failed: {e}")
        return {
            "messages": [("system", f"ERROR: LLM í˜¸ì¶œ ì‹¤íŒ¨ - {e}")],
            "next_node": "coder",
            "coder_iteration": current_iteration + 1
        }

    # 4. ì‘ë‹µ íŒŒì‹± ë° ì‹¤í–‰
    res_data = {}
    function_calls = []

    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ (Ollama ë“± í…ìŠ¤íŠ¸ ì„ì—¬ ë‚˜ì˜¤ëŠ” ê²½ìš° ëŒ€ë¹„)
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            res_data = json.loads(json_match.group(0))
        else:
            res_data = json.loads(response_text)
            
        coder_thought = res_data.get("thought", "Processing...")
        coder_tree = res_data.get("thought_tree", [])
        status = res_data.get("status", "in_progress")
        
        # Native Function Callì´ ì•„ë‹Œ ê²½ìš° action í•„ë“œ í™•ì¸
        if "action" in res_data and res_data["action"] != "none":
            # ê°€ìƒ Function Call ê°ì²´ ìƒì„±
            fname = res_data["action"]
            fargs = res_data.get("action_input", {})
            function_calls.append(type('obj', (object,), {'name': fname, 'args': fargs}))
            
    except Exception as e:
        logger.warning(f"Failed to parse LLM response: {e}")
        coder_thought = "Response parsing failed."
        coder_tree = []
        status = "failed"

    # [Compatibility] Gemini Backendì˜ ê²½ìš° function_callsê°€ ë³„ë„ë¡œ ìˆì„ ìˆ˜ ìˆìŒ
    # (í˜„ì¬ backend.generateëŠ” textë§Œ ë¦¬í„´í•˜ë¯€ë¡œ, ì¶”í›„ backend ì¸í„°í˜ì´ìŠ¤ ê³ ë„í™” í•„ìš”)
    # ì¼ë‹¨ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ í†µì¼í•˜ê±°ë‚˜ Gemini ì „ìš© ë¡œì§ ë³´ê°•

    if function_calls:
        fc = function_calls[0]
        fname = fc.name
        fargs = fc.args
        result_msg = ""
        new_file_cache = state.get("file_cache", {}).copy()

        # [Compliance & Security Check] (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        from gortex.agents.analyst import AnalystAgent
        compliance_res = AnalystAgent().validate_constraints(
            state.get("active_constraints", []),
            {"action": fname, "target": fargs.get("path") or fargs.get("command") or fargs.get("directory"), "args": fargs}
        )
        
        if not compliance_res.get("is_valid", True):
            return {
                "thought": f"ì •ì±… ìœ„ë°˜: {compliance_res.get('reason')}",
                "coder_iteration": current_iteration + 1,
                "messages": [("ai", "âŒ ì •ì±… ìœ„ë°˜ìœ¼ë¡œ ì°¨ë‹¨ë¨"), ("system", compliance_res.get('reason'))],
                "next_node": "coder"
            }

        if fname == "write_file":
            result_msg = write_file(fargs["path"], fargs["content"])
            new_file_cache[fargs["path"]] = get_file_hash(fargs["path"])
        elif fname == "apply_patch":
            result_msg = apply_patch(fargs["path"], int(fargs["start_line"]), int(fargs["end_line"]), fargs["new_content"])
            new_file_cache[fargs["path"]] = get_file_hash(fargs["path"])
        elif fname == "execute_shell":
            result_msg = execute_shell(fargs["command"])
        elif fname == "read_file":
            result_msg = read_file(fargs["path"])
        elif fname == "list_files":
            result_msg = list_files(fargs.get("directory", "."))
            
        return {
            "thought": coder_thought, "thought_tree": coder_tree,
            "coder_iteration": current_iteration + 1, "file_cache": new_file_cache,
            "messages": [("ai", f"Executed {fname}"), ("tool", result_msg)],
            "next_node": "coder"
        }

    if status == "success":
        # [Recursion Guard] pre-commitì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
        import os
        if os.environ.get("GORTEX_PRE_COMMIT_ACTIVE") == "true":
            logger.info("Pre-commit guard active: Skipping recursive check.")
            return {
                "current_step": current_step_idx + 1, "coder_iteration": 0,
                "next_node": "coder", "messages": [("ai", f"âœ… Step {current_step_idx+1} ì™„ë£Œ (Guard Active)")]
            }

        # ê²€ì¦ ë£¨í”„ (ê¸°ì¡´ ë¡œì§)
        from gortex.utils.tools import get_changed_files
        changed_files = get_changed_files(state.get("working_dir", "."), state.get("file_cache", {}))
        
        # ì‰˜ ëª…ë ¹ì–´ ë ˆë²¨ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ ì‹¤í–‰
        check_res = execute_shell(f"GORTEX_PRE_COMMIT_ACTIVE=true ./scripts/pre_commit.sh --selective {' '.join(changed_files)}")
        
        if "Ready to commit" in check_res:
            return {
                "current_step": current_step_idx + 1, "coder_iteration": 0,
                "next_node": "coder", "messages": [("ai", f"âœ… Step {current_step_idx+1} ì™„ë£Œ")]
            }
        else:
            return {
                "thought": "Correction needed.", "coder_iteration": current_iteration + 1,
                "messages": [("ai", "âŒ ê²€ì¦ ì‹¤íŒ¨"), ("tool", check_res)],
                "next_node": "coder"
            }
            
    return {
        "thought": coder_thought, "thought_tree": coder_tree,
        "coder_iteration": current_iteration + 1, "next_node": "coder"
    }