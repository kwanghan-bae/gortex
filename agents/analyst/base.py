import logging
import json
import os
import re
import math
from typing import Dict, Any, List, Optional
from datetime import datetime
from gortex.core.state import GortexState
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.vector_store import LongTermMemory

from gortex.agents.base import BaseAgent
from gortex.core.registry import AgentMetadata

logger = logging.getLogger("GortexAnalystBase")

class AnalystAgent(BaseAgent):
    """Gortex ÏãúÏä§ÌÖúÏùò Î∂ÑÏÑù Î∞è ÏßÑÌôî Îã¥Îãπ ÏóêÏù¥Ï†ÑÌä∏ (Base Class)"""
    def __init__(self):
        super().__init__()
        self.memory = EvolutionaryMemory()
        self.ltm = LongTermMemory()

    @property
    def metadata(self) -> AgentMetadata:
        return AgentMetadata(
            name="Analyst",
            role="Analyst",
            description="Analyzes work quality, audits architecture, and curates knowledge base.",
            tools=["scan_complexity", "audit_architecture", "optimize_knowledge"],
            version="3.0.0"
        )

    def run(self, state: GortexState) -> Dict[str, Any]:
        """Í∏∞Î≥∏ Î∂ÑÏÑù Î£®Ìã¥: ÌíàÏßà ÌèâÍ∞Ä ÎòêÎäî Î¶¨ÏÑúÏπò Í≤∞Í≥º ÏöîÏïΩ"""
        # [INTEGRATION] Update Skill Points on Success
        from gortex.utils.economy import get_economy_manager
        eco_manager = get_economy_manager()
        
        eco_manager.update_skill_points(
            state, 
            self.metadata.name, 
            category="Analysis", 
            quality_score=1.0, 
            difficulty=1.0
        )
        
        # (Í∏∞Î≥∏ Íµ¨ÌòÑ: managerÎ°ú Î≥µÍ∑ÄÌïòÎ©∞ ÏÑ±Í≥º Î¶¨Ìè¨Ìä∏)
        return {
            "next_node": "manager", 
            "thought": "Analysis routine complete.",
            "agent_economy": state.get("agent_economy")
        }

    def calculate_efficiency_score(self, success: bool, tokens: int, latency_ms: int, energy_cost: int) -> float:
        if not success: return 0.0
        cost = (tokens * 0.01) + (latency_ms * 0.005) + (energy_cost * 2.0)
        score = 100.0 / (1.0 + math.log1p(cost / 5.0))
        return round(min(100.0, score), 1)

    def identify_tool_gap(self, failure_context: str) -> Optional[Dict[str, Any]]:
        """ÏûëÏóÖ Ïã§Ìå® Îß•ÎùΩÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÌïÑÏöîÌïú Ïã†Í∑ú ÎèÑÍµ¨(Tool)Î•º ÏÑ§Í≥ÑÌï®."""
        prompt = f"""You are the Master ToolSmith. 
        Analyze the following failure and design a NEW Python tool (function) that would prevent this in the future.
        
        [Failure Context]:
        {failure_context}
        
        [Current Tools]: {tool_registry.list_tools()}
        
        Design a reusable tool. Return JSON ONLY:
        {{
            "tool_name": "reusable_tool_name",
            "description": "What it does",
            "parameters": {{"param1": "type", "param2": "type"}},
            "logic_blueprint": "Step-by-step logic for the function",
            "target_agent": "Which agent should receive this tool"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Tool gap analysis failed: {e}")
            return None

    def resolve_knowledge_conflict(self, conflict: Dict[str, Any], model_id: str = "gemini-2.0-flash") -> Optional[Dict[str, Any]]:
        """Îëê ÏÉ§Îìú Í∞ÑÏùò ÏÉÅÏ∂©ÎêòÎäî ÏßÄÏãùÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©ÌïòÍ±∞ÎÇò Ïö∞ÏÑ†ÏàúÏúÑÎ•º Í≤∞Ï†ïÌï®."""
        rule_a = conflict["rule_a"]
        rule_b = conflict["rule_b"]
        
        logger.info(f"‚öñÔ∏è Resolving conflict between {rule_a['id']} and {rule_b['id']}...")
        
        # 1. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÏûêÎèô Ìï¥Í≤∞ ÏãúÎèÑ
        score_a = (rule_a.get("success_count", 0) + 1) * rule_a.get("reinforcement_count", 1)
        score_b = (rule_b.get("success_count", 0) + 1) * rule_b.get("reinforcement_count", 1)
        
        # Ï†êÏàò Ï∞®Ïù¥Í∞Ä ÌÅ¨Î©¥ (Ïòà: 3Î∞∞ Ïù¥ÏÉÅ) Ïö∞ÏÑ∏Ìïú Ï™ΩÏùÑ ÏÑ†ÌÉù
        if score_a > score_b * 3:
            logger.info(f"‚úÖ Auto-resolved: {rule_a['id']} wins by performance score.")
            return rule_a
        elif score_b > score_a * 3:
            logger.info(f"‚úÖ Auto-resolved: {rule_b['id']} wins by performance score.")
            return rule_b

        # 2. Ï†êÏàòÍ∞Ä ÎπÑÏä∑ÌïòÎ©¥ LLMÏùÑ ÌÜµÌï¥ ÌÜµÌï©(Synthesis) ÏãúÎèÑ
        prompt = f"""ÎãπÏã†ÏùÄ ÏãúÏä§ÌÖúÏùò ÏùºÍ¥ÄÏÑ±ÏùÑ Í¥ÄÎ¶¨ÌïòÎäî ÏßÄÏãù Ï°∞Ï†ïÏûêÏûÖÎãàÎã§. Îã§Ïùå Îëê ÏÉÅÏ∂©ÎêòÎäî Í∑úÏπôÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÌïòÎÇòÏùò ÏµúÏ†ÅÌôîÎêú Í∑úÏπôÏúºÎ°ú ÌÜµÌï©ÌïòÏã≠ÏãúÏò§.
        
        [Rule A (Category: {rule_a['category']})]: {rule_a['learned_instruction']}
        [Rule B (Category: {rule_b['category']})]: {rule_b['learned_instruction']}
        
        ÌÜµÌï© ÏõêÏπô:
        1. Î™®ÏàúÎêòÎäî Î∂ÄÎ∂ÑÏùÄ Îçî ÌòÑÎåÄÏ†ÅÏù¥Í≥† ÏïàÏ†ÑÌïú Í∏∞Ïà†Ï†Å Í¥ÄÏ†êÏùÑ Îî∞Î•¥Ïã≠ÏãúÏò§.
        2. Îëê Î∂ÑÏïºÏùò Îß•ÎùΩÏùÑ Î™®Îëê ÏàòÏö©Ìï† Ïàò ÏûàÎäî Î≤îÏö©Ï†ÅÏù∏ ÏßÄÏπ®ÏùÑ ÎßåÎìúÏã≠ÏãúÏò§.
        
        Í≤∞Í≥ºÎäî JSON ÌòïÏãùÏúºÎ°úÎßå Î∞òÌôòÌïòÏã≠ÏãúÏò§:
        {{ "instruction": "ÌÜµÌï©Îêú ÏßÄÏπ® ÎÇ¥Ïö©", "trigger_patterns": ["Ìå®ÌÑ¥1", "Ìå®ÌÑ¥2"], "severity": 1~5, "target_category": "Ïñ¥Îäê ÏÉ§ÎìúÎ°ú Î≥¥ÎÇºÏßÄ" }}
        """
        
        try:
            response_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            return {
                "learned_instruction": res_data["instruction"],
                "trigger_patterns": res_data["trigger_patterns"],
                "severity": res_data.get("severity", 3),
                "category": res_data.get("target_category", rule_a["category"])
            }
        except Exception as e:
            logger.error(f"Semantic conflict resolution failed: {e}")
            return rule_a if score_a >= score_b else rule_b # ÏµúÏïÖÏùò Í≤ΩÏö∞ ÏÑ±Í≥º Ï¢ãÏùÄ Ï™Ω Ïú†ÏßÄ

    def identify_capability_gap(self, error_log: str = "", unresolved_task: str = "") -> Optional[Dict[str, Any]]:
        """
        ÏãúÏä§ÌÖúÏù¥ Ï≤òÎ¶¨ÌïòÏßÄ Î™ªÌïú Í≥ºÏ†úÎÇò ÏóêÎü¨Î•º Î∂ÑÏÑùÌïòÏó¨ ÌïÑÏöîÌïú ÏÉàÎ°úÏö¥ Ï†ÑÎ¨∏Í∞Ä ÏóêÏù¥Ï†ÑÌä∏ Î™ÖÏÑ∏Î•º Ï†úÏïàÌï®.
        """
        prompt = f"""You are the Intelligence Growth Strategist. 
        Analyze the following failure/unresolved task and design a NEW specialized agent to handle it.
        
        [Failure/Task]: {error_log or unresolved_task}
        
        Design an agent that inherits from 'BaseAgent'.
        Return JSON ONLY:
        {{
            "agent_name": "UniqueNameAgent",
            "role": "Specific role title",
            "description": "What this agent does better than others",
            "required_tools": ["tool1", "tool2"],
            "version": "1.0.0",
            "logic_strategy": "How its 'run' method should behave"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Capability gap analysis failed: {e}")
            return None

    def synthesize_debug_consensus(self, error_log: str, debate_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Ïó¨Îü¨ ÏóêÏù¥Ï†ÑÌä∏Ïùò ÎîîÎ≤ÑÍπÖ Í∞ÄÏÑ§Í≥º ÌÜ†Î°† ÎÇ¥Ïö©ÏùÑ Ï¢ÖÌï©ÌïòÏó¨ ÏµúÏ¢Ö ÏàòÎ¶¨ Í≥ÑÌöçÏùÑ ÌôïÏ†ïÌï®.
        """
        prompt = f"""You are the Chief Surgeon. Synthesize the following debugging debate into one final, authoritative fix plan.
        
        [Original Error]:
        {error_log}
        
        [Debate History]:
        {json.dumps(debate_history, indent=2, ensure_ascii=False)}
        
        Analyze the pros and cons of each hypothesis and output the best combined solution.
        Return JSON ONLY:
        {{
            "diagnosis": "Final root cause identification",
            "fix_strategy": "Authoritative fix strategy",
            "action_plan": ["Step 1", "Step 2"],
            "verification_method": "How to verify the fix"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Debug consensus synthesis failed: {e}")
            return {"diagnosis": "Failed to synthesize", "fix_strategy": str(e), "action_plan": []}

    def summarize_system_trace(self, log_path: str = "logs/trace.jsonl") -> str:
        """Í±∞ÎåÄÌïú ÏãúÏä§ÌÖú Î°úÍ∑∏Î•º Î∂ÑÏÑùÌïòÏó¨ ÌïµÏã¨ ÌÉÄÏûÑÎùºÏù∏Í≥º ÌÜµÏ∞∞ÏùÑ ÏöîÏïΩÌï®."""
        if not os.path.exists(log_path):
            return "No trace logs available for summarization."

        try:
            with open(log_path, "r", encoding="utf-8") as f:
                logs = [json.loads(line) for line in f][-300:] # ÏµúÍ∑º 300Í∞ú Ïù¥Î≤§Ìä∏ ÎåÄÏÉÅ
            
            # Ï§ëÏöî Ïù¥Î≤§Ìä∏Îßå Ï∂îÏ∂ú (ÏóêÎü¨, ÎÖ∏Îìú ÏôÑÎ£å, ÎèÑÍµ¨ Í≤∞Í≥º Îì±)
            significant_events = []
            for l in logs:
                if l.get("event") in ["error", "node_complete", "tool_call"] or "‚ùå" in str(l.get("payload")):
                    significant_events.append({
                        "agent": l.get("agent"),
                        "event": l.get("event"),
                        "time": l.get("timestamp"),
                        "info": str(l.get("payload"))[:200]
                    })

            prompt = f"""Îã§ÏùåÏùÄ Gortex ÏãúÏä§ÌÖúÏùò ÏµúÍ∑º Ïã§Ìñâ Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞Îã§.
            Ïù¥ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ ÏãúÏä§ÌÖúÏùò 'ÏµúÍ∑º Ïó≠ÏÇ¨'Î•º ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏúºÎ°ú ÏöîÏïΩÌïòÎùº.
            
            [Î∂ÑÏÑù Ìï≠Î™©]
            1. Ï£ºÏöî ÎßàÏùºÏä§ÌÜ§: ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎêú ÌÅ∞ ÏûëÏóÖÎì§
            2. ÏúÑÍ∏∞ Î∞è Ìï¥Í≤∞: Î∞úÏÉùÌñàÎçò ÏπòÎ™ÖÏ†Å ÏóêÎü¨ÏôÄ ÏûêÏú® ÏàòÎ¶¨ Í≤∞Í≥º
            3. ÌòëÏóÖ Ìå®ÌÑ¥: Í∞ÄÏû• ÌôúÎ∞úÌñàÎçò ÏóêÏù¥Ï†ÑÌä∏ Í∞ÑÏùò Í¥ÄÍ≥Ñ
            4. Í∞úÏÑ† Í∂åÍ≥†: Î°úÍ∑∏Î•º ÌÜµÌï¥ Î≥∏ ÏïÑÌÇ§ÌÖçÏ≤òÏ†Å ÏïΩÏ†ê
            
            [Raw Data]
            {json.dumps(significant_events, ensure_ascii=False)}
            """
            
            summary = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            summary_path = "logs/trace_summary.md"
            from gortex.utils.tools import write_file
            write_file(summary_path, f"# üìú Gortex Historical Trace Summary\n\n> Generated: {datetime.now()}\n\n{summary}")
            return summary
            
        except Exception as e:
            logger.error(f"Trace summarization failed: {e}")
            return f"Error: {e}"

    def apply_consensus_result(self, debate_result: Dict[str, Any], conflicting_rules: List[Dict[str, Any]]):
        """SwarmÏùò Ìï©Ïùò Í≤∞Í≥ºÎ•º ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê ÏòÅÍµ¨ Î∞òÏòÅÌï®."""
        unified = debate_result.get("unified_rule")
        if not unified:
            logger.warning("No unified rule found in consensus result. Skipping integration.")
            return

        # 1. ÏÉàÎ°úÏö¥ Ï†ÑÏó≠ Í∑úÏπô ÏÉùÏÑ± (Í≥ÑÎ≥¥ Ïó∞Í≤∞)
        parent_ids = [r["id"] for r in conflicting_rules]
        new_rule_id = self.memory.save_rule(
            instruction=unified["instruction"],
            trigger_patterns=unified["trigger_patterns"],
            category=unified.get("category", "general"),
            severity=unified.get("severity", 3),
            context=f"Consensus achieved via Swarm Intelligence. Rationale: {debate_result.get('rationale')}"
        )
        
        # 2. Í≥ÑÎ≥¥(Lineage) Ï†ïÎ≥¥ Ï∂îÍ∞Ä ÏóÖÎç∞Ïù¥Ìä∏ (save_rule Ïù¥ÌõÑ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï)
        # shardÎ•º ÏßÅÏ†ë Ï∞æÏïÑ parent_rules Ï£ºÏûÖ
        cat = unified.get("category", "general")
        for rule in self.memory.shards.get(cat, []):
            if rule["id"] == new_rule_id:
                rule["parent_rules"] = parent_ids
                rule["is_super_rule"] = True
                break
        self.memory._persist_shard(cat)

        # 3. Í∏∞Ï°¥ Í∞àÎì± Í∑úÏπôÎì§ Ï†ïÎ¶¨ (Soft-delete ÎòêÎäî Flag Ï≤òÎ¶¨)
        # ÌòÑÏû¨Îäî Îã®ÏàúÌïòÍ≤å ÏÉà Í∑úÏπôÏúºÎ°ú ÎåÄÏ≤¥ÌïòÎäî Î∞©ÏãùÏúºÎ°ú Ïö¥ÏòÅ (Ï§ëÎ≥µ Ï†úÍ±∞ Î£®Ìã¥ÏóêÏÑú Ï∂îÌõÑ ÏôÑÏ†Ñ ÏÜåÍ±∞)
        logger.info(f"‚ú® Unified rule {new_rule_id} created from parents: {parent_ids}")

    def generate_impact_map(self, symbol_name: str) -> str:
        """ÌäπÏ†ï Ïã¨Î≥º Î≥ÄÍ≤Ω ÏãúÏùò ÏòÅÌñ•Î†• ÏßÄÎèÑÎ•º Mermaid ÌòïÏãùÏúºÎ°ú ÏÉùÏÑ±Ìï®."""
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        # ÏµúÏã† ÏΩîÎìú ÏÉÅÌÉú Î∞òÏòÅÏùÑ ÏúÑÌïú Ïä§Ï∫î Í∞ïÏ†ú Ïã§Ìñâ
        indexer.scan_project()
        deps = indexer.find_reverse_dependencies(symbol_name)
        
        if not deps:
            return f"graph TD\n  {symbol_name} -->|No Direct Dependents| Safe"
            
        diagram = f"graph RL\n  %% Impact map for {symbol_name}\n"
        diagram += f"  Target(({symbol_name})):::target\n"
        
        for idx, d in enumerate(deps):
            caller_label = f"{d['file']}\\n({d['caller']})"
            diagram += f"  Dep{idx}[{caller_label}] --> Target\n"
            
        diagram += "\n  classDef target fill:#f96,stroke:#333,stroke-width:4px;"
        return diagram

    def analyze_workflow_bottlenecks(self) -> List[Dict[str, Any]]:
        """ÏóêÏù¥Ï†ÑÌä∏ Í∞Ñ ÌòëÏóÖ Îß§Ìä∏Î¶≠Ïä§Î•º Î∂ÑÏÑùÌïòÏó¨ ÎπÑÌö®Ïú®Ï†ÅÏù∏ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ìå®ÌÑ¥ÏùÑ ÏãùÎ≥ÑÌï®."""
        from gortex.core.observer import GortexObserver
        observer = GortexObserver()
        matrix = observer.get_collaboration_matrix(limit=1000)
        
        bottlenecks = []
        if not matrix: return []
        
        # 1. ÌïëÌêÅ ÌòÑÏÉÅ Í∞êÏßÄ (A -> B -> A Î∞òÎ≥µ)
        for caller, callees in matrix.items():
            for callee, count in callees.items():
                if count > 5: # ÏûÑÍ≥ÑÏπò: 5Ìöå Ïù¥ÏÉÅ Ìò∏Ï∂ú
                    # Ïó≠Î∞©Ìñ• Ìò∏Ï∂ú ÌôïÏù∏
                    back_count = matrix.get(callee, {}).get(caller, 0)
                    if back_count > 5:
                        bottlenecks.append({
                            "type": "ping_pong",
                            "agents": [caller, callee],
                            "severity": "High" if min(count, back_count) > 10 else "Medium",
                            "reason": f"{caller}ÏôÄ {callee} Í∞ÑÏùò Ïû¶ÏùÄ ÌïëÌêÅ({count}:{back_count})Ïù¥ Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§.",
                            "suggestion": f"{caller}Ïùò ÌéòÎ•¥ÏÜåÎÇò ÏßÄÏπ®ÏùÑ Í∞ïÌôîÌïòÏó¨ Îã®Î≤àÏóê Ìï¥Í≤∞ÌïòÎèÑÎ°ù Í∞úÏÑ†ÌïòÍ±∞ÎÇò, Ï§ëÍ∞Ñ Í≤ÄÏ¶ù Î°úÏßÅÏùÑ Îã®ÏàúÌôîÌïòÏã≠ÏãúÏò§."
                        })
        
        # 2. Í≥†Î∂ÄÌïò ÎÖ∏Îìú Í∞êÏßÄ (In-degreeÍ∞Ä ÎÑàÎ¨¥ ÎÜíÏùÄ Í≤ΩÏö∞)
        node_load = {}
        for caller, callees in matrix.items():
            for callee, count in callees.items():
                node_load[callee] = node_load.get(callee, 0) + count
                
        for node, load in node_load.items():
            if load > 50: # Í≥ºÎ∂ÄÌïò ÏûÑÍ≥ÑÏπò
                bottlenecks.append({
                    "type": "hotspot",
                    "agent": node,
                    "severity": "Medium",
                    "reason": f"'{node}' ÎÖ∏ÎìúÍ∞Ä ÏãúÏä§ÌÖú Î∂ÄÌïòÏùò Ï§ëÏã¨({load} calls)Ïù¥ ÎêòÍ≥† ÏûàÏäµÎãàÎã§.",
                    "suggestion": f"'{node}'Ïùò Ïó≠Ìï†ÏùÑ Ïó¨Îü¨ Ï†ÑÎ¨∏Í∞ÄÎ°ú Î∂ÑÎ¶¨(Role Splitting)ÌïòÏó¨ Î≥ëÎ†¨ Ï≤òÎ¶¨Î•º Ïú†ÎèÑÌïòÏã≠ÏãúÏò§."
                })
                
        return bottlenecks

    def audit_external_plugin(self, plugin_code: str, plugin_name: str) -> Dict[str, Any]:
        """Ïô∏Î∂ÄÏóêÏÑú Í∞ÄÏ†∏Ïò® ÌîåÎü¨Í∑∏Ïù∏ ÏΩîÎìúÎ•º Î≥¥Ïïà Í¥ÄÏ†êÏóêÏÑú Ï†ïÎ∞Ä Í≤ÄÏàòÌï®."""
        logger.info(f"üõ°Ô∏è Auditing external plugin: {plugin_name}...")
        
        # 1. Ï†ïÏ†Å Ìå®ÌÑ¥ Ïä§Ï∫î (Í∏∞Î≥∏ ÎèÑÍµ¨ ÌôúÏö©)
        from gortex.utils.tools import scan_security_risks
        static_risks = scan_security_risks(plugin_code)
        
        # 2. LLM Í∏∞Î∞ò Ïã¨Ï∏µ Î∂ÑÏÑù
        prompt = f"""You are the Chief Security Officer. 
        Perform a deep security audit on the following external AI Agent code.
        Look for malicious intent, hidden backdoors, unauthorized data exfiltration, or system-destructive logic.
        
        [Plugin Name]: {plugin_name}
        [Code]:
        {plugin_code[:4000]}
        
        Return JSON ONLY:
        {{
            "is_safe": true/false,
            "risk_level": "Low/Medium/High/Critical",
            "findings": ["finding 1", "finding 2"],
            "recommendation": "Approve / Reject / Sandbox"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            audit_res = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            # Ï†ïÏ†Å Î∂ÑÏÑù Í≤∞Í≥º ÌÜµÌï©
            if static_risks:
                audit_res["static_findings"] = static_risks
                if any(r["type"] == "Hardcoded Secret" for r in static_risks):
                    audit_res["is_safe"] = False
                    audit_res["risk_level"] = "High"
            
            return audit_res
        except Exception as e:
            logger.error(f"Security audit failed: {e}")
            return {"is_safe": False, "risk_level": "Critical", "recommendation": "Reject due to audit failure"}

    def analyze_and_optimize_persona(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """ÏóêÏù¥Ï†ÑÌä∏Ïùò ÏûëÏóÖ Ïù¥Î†•ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÌéòÎ•¥ÏÜåÎÇò ÏßÄÏπ®(System Prompt)ÏùÑ ÏµúÏ†ÅÌôîÌï®."""
        # 1. ÏµúÍ∑º ÏÑ±Í≥º Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        monitor = EfficiencyMonitor()
        summary = monitor.get_summary(days=7)
        
        # 2. ÌòÑÏû¨ ÌéòÎ•¥ÏÜåÎÇò ÏßÄÏπ® ÌöçÎìù
        from gortex.utils.prompt_loader import loader
        current_instruction = loader.get_prompt(agent_name.lower())
        
        prompt = f"""You are the Neural Architect. 
        Optimize the following System Instruction for the '{agent_name}' agent.
        Analyze its recent performance and mutate the instruction to be more effective.
        
        [Current Instruction]:
        {current_instruction}
        
        [Recent Performance Metrics]:
        {json.dumps(summary.get(agent_name, {}), indent=2)}
        
        Goals:
        1. Keep the core identity but refine the technical guidance.
        2. Strengthen points that led to success, fix points that led to failure.
        
        Return JSON ONLY:
        {{
            "new_instruction": "Full optimized instruction text",
            "changes": "Summary of what was changed and why",
            "version": "X.Y.Z (bump minor)"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Persona optimization failed for {agent_name}: {e}")
            return None

    def scan_system_infection(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú ÏΩîÎìúÎ≤†Ïù¥Ïä§Ïùò Î¨¥Í≤∞ÏÑ±ÏùÑ Í≤ÄÏÇ¨ÌïòÏó¨ ÎπÑÏ†ïÏÉÅÏ†ÅÏù∏ Ïò§Ïóº(Infection)ÏùÑ ÌÉêÏßÄÌï®."""
        from gortex.utils.integrity import guard
        modified, deleted = guard.check_integrity()
        
        if not modified and not deleted:
            return {"status": "healthy", "infections": []}
            
        infections = []
        # (Ïã§Ï†ú Íµ¨ÌòÑ Ïãú ÌòÑÏû¨ ÏßÑÌñâ Ï§ëÏù∏ 'ÏäπÏù∏Îêú ÎØ∏ÏÖò'Ïùò ÌÉÄÍ≤ü ÌååÏùº Î™©Î°ùÍ≥º ÎåÄÏ°∞ÌïòÏó¨ Ïò§ÌÉê Î∞©ÏßÄ)
        for path in modified:
            infections.append({"path": path, "type": "modified", "severity": "High"})
        for path in deleted:
            infections.append({"path": path, "type": "deleted", "severity": "Critical"})
            
        logger.warning(f"üö® [ImmuneSystem] Infection detected in {len(infections)} files!")
        return {"status": "infected", "infections": infections}

    def generate_strategic_roadmap(self) -> str:
        """ÌÖåÌÅ¨ Î†àÏù¥Îçî Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ Ï§ëÏû•Í∏∞ Í∏∞Ïà†Ï†Å ÏßÑÌôî Î°úÎìúÎßµÏùÑ ÏÉùÏÑ±Ìï®."""
        from gortex.utils.tech_radar import radar
        advice = radar.get_strategic_advice()
        
        prompt = f"""You are the Chief Technology Officer. 
        Based on the current Tech Radar data, design a STRATEGIC ROADMAP for the next 10 Gortex sessions.
        Focus on phasing out 'hold' status tech and accelerating 'assess/trial' tech.
        
        [Tech Radar Info]:
        {json.dumps(radar.technologies, indent=2)}
        
        Return a high-level roadmap in Markdown format.
        """
        try:
            roadmap = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            return f"{advice}\n\n{roadmap}"
        except Exception as e:
            logger.error(f"Strategic roadmap failed: {e}")
            return "Failed to generate roadmap."

    def validate_alignment_with_constitution(self, proposed_action: str) -> Dict[str, Any]:
        """Ï†úÏïàÎêú ÌñâÎèôÏù¥ Gortex ÌóåÏû•(CONSTITUTION.md)ÏùÑ Ï§ÄÏàòÌïòÎäîÏßÄ Í≤ÄÏ¶ùÌï®."""
        constitution = read_file("docs/CONSTITUTION.md")
        
        prompt = f"""You are the Neural Ethicist. 
        Verify if the following proposed action aligns with the Gortex Neural Constitution.
        
        [Constitution]:
        {constitution}
        
        [Proposed Action]:
        {proposed_action}
        
        Check for any violations of Integrity, Sovereignty, Responsibility, or Efficiency.
        Return JSON ONLY:
        {{
            "is_aligned": true/false,
            "violations": ["violation 1", "violation 2"],
            "severity": "Low/Medium/High/Critical",
            "corrective_action": "How to fix the plan to align with the constitution"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Alignment check failed: {e}")
            return {"is_aligned": True, "severity": "Low", "violations": []} # Fallback to true to avoid deadlock, but log error

    def detect_agent_fusion_opportunities(self) -> List[Dict[str, Any]]:
        """ÏóêÏù¥Ï†ÑÌä∏ Í∞ÑÏùò Í∞ïÌïú Í≤∞Ìï©ÎèÑÎ•º Î∂ÑÏÑùÌïòÏó¨ ÏúµÌï©(Fusion) Í∞ÄÎä•ÏÑ±ÏùÑ ÏãùÎ≥ÑÌï®."""
        from gortex.core.observer import GortexObserver
        matrix = GortexObserver().get_collaboration_matrix(limit=1000)
        
        fusions = []
        if not matrix: return []
        
        # Ìò∏Ï∂ú ÎπàÎèÑÍ∞Ä Îß§Ïö∞ ÎÜíÏùÄ Ïåç Ï∞æÍ∏∞ (Ïòà: A -> B Ìò∏Ï∂úÏù¥ Ï†ÑÏ≤¥Ïùò 40% Ïù¥ÏÉÅ)
        for caller, callees in matrix.items():
            total_calls = sum(callees.values())
            for callee, count in callees.items():
                if count / total_calls > 0.4 and count > 10:
                    fusions.append({
                        "type": "agent_fusion",
                        "pair": [caller, callee],
                        "strength": round(count / total_calls, 2),
                        "reason": f"'{caller}'ÏôÄ '{callee}'Í∞Ä Îß§Ïö∞ Í∞ïÌïòÍ≤å Í≤∞Ìï©ÎêòÏñ¥ ÏûëÏóÖ Ï§ëÏûÖÎãàÎã§. (Í≤∞Ìï©ÎèÑ: {int(count/total_calls*100)}%)",
                        "suggestion": f"Îëê ÏóêÏù¥Ï†ÑÌä∏Î•º '{caller}_{callee}_Fused'Î°ú Î≥ëÌï©ÌïòÏó¨ Ï§ëÍ∞Ñ Ìï∏ÎìúÏò§ÌîÑ ÎπÑÏö©ÏùÑ Ï†úÍ±∞ÌïòÏã≠ÏãúÏò§."
                    })
        return fusions

    def predict_runtime_errors(self, code: str, file_path: str) -> Dict[str, Any]:
        """ÏΩîÎìú Î≥ÄÍ≤ΩÎ∂ÑÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ïû†Ïû¨Ï†Å Îü∞ÌÉÄÏûÑ Ïû•Ïï† Î∞úÏÉù ÌôïÎ•†ÏùÑ ÏòàÏ∏°Ìï®."""
        # 1. Í≥ºÍ±∞ Ïû•Ïï† Ìå®ÌÑ¥ ÏÜåÌôò
        from gortex.utils.log_vectorizer import SemanticLogSearch
        past_failures = SemanticLogSearch().search_similar_cases(f"Error in {file_path}", limit=10)
        
        prompt = f"""You are the Oracle Architect. 
        Analyze the following code for potential runtime failures (e.g., unhandled exceptions, race conditions, edge cases).
        Cross-reference with the historical failures provided.
        
        [Target File]: {file_path}
        [New Code]:
        {code[:3000]}
        
        [Historical Failure Patterns]:
        {json.dumps(past_failures, ensure_ascii=False)}
        
        Return JSON ONLY:
        {{
            "risk_probability": 0.0 ~ 1.0,
            "predicted_error_type": "ZeroDivisionError/KeyError/etc",
            "reason": "Detailed justification",
            "preemptive_fix": "Specific instruction to fix before it crashes"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Error prediction failed: {e}")
            return {"risk_probability": 0.0}

    def identify_dormant_assets(self) -> Dict[str, List[str]]:
        """ÏãúÏä§ÌÖú ÎÇ¥Ïùò ÎèÑÌÉú ÎåÄÏÉÅ(Dormant/Underperforming) ÏûêÏÇ∞ÏùÑ ÏãùÎ≥ÑÌï®."""
        from gortex.core.registry import registry
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        monitor = EfficiencyMonitor()
        summary = monitor.get_summary(days=30)
        
        dormant_agents = []
        # 1. Ï†ÄÏÑ±Í≥º ÏóêÏù¥Ï†ÑÌä∏ ÏãùÎ≥Ñ
        for agent_name in registry.list_agents():
            if agent_name.lower() in ["manager", "analyst", "planner", "coder"]: continue
            
            stats = summary.get(agent_name, {})
            calls = stats.get("calls", 0)
            success_rate = stats.get("success_rate", 100)
            
            # Ï°∞Í±¥: 10Ìöå Ïù¥ÏÉÅ Ìò∏Ï∂úÎêòÏóàÏúºÎÇò ÏÑ±Í≥µÎ•†Ïù¥ 30% ÎØ∏ÎßåÏù∏ Í≤ΩÏö∞
            if calls >= 10 and success_rate < 30:
                dormant_agents.append(agent_name)
                logger.info(f"ü•Ä Agent '{agent_name}' identified for offboarding (Success Rate: {success_rate:.1f}%)")

        # 2. ÏúµÌï©(Fusion)Ïóê ÏùòÌï¥ ÎåÄÏ≤¥Îêú ÏõêÎ≥∏ ÏóêÏù¥Ï†ÑÌä∏ ÏãùÎ≥Ñ
        # (Ïã§Ï†ú Íµ¨ÌòÑ Ïãú Super RulesÏùò 'Neural Fusion established' Í∏∞Î°ù ÎåÄÏ°∞)
        
        return {"agents": dormant_agents}

    def analyze_infrastructure_scaling(self, state: GortexState) -> Dict[str, Any]:
        """Í≤ΩÏ†úÏ†Å ÏÉÅÌÉúÏôÄ Î∂ÄÌïòÎ•º Î∂ÑÏÑùÌïòÏó¨ Ïù∏ÌîÑÎùº ÌôïÏû• Ïó¨Î∂ÄÎ•º Í≤∞Ï†ïÌï®."""
        from gortex.utils.infra import infra
        load = infra.check_cluster_load()
        
        # Ï†ÑÏ≤¥ ÏòàÏÇ∞ Ìï©ÏÇ∞
        total_credits = sum(a.get("credits", 0) for a in state.get("agent_economy", {}).values())
        
        should_scale = False
        reason = ""
        
        # Ï°∞Í±¥: ÌèâÍ∑† CPUÍ∞Ä 70% Ïù¥ÏÉÅÏù¥Í≥†, Ï¥ù ÏûîÍ≥†Í∞Ä $100 Ïù¥ÏÉÅÏùº Îïå
        if load["avg_cpu"] > 70 and total_credits > 100.0:
            should_scale = True
            reason = f"High cluster load ({load['avg_cpu']:.1f}%) with healthy budget (${total_credits:.2f})"
        elif load["count"] == 0:
            should_scale = True
            reason = "No remote workers active. Establishing baseline capacity."
            
        return {
            "should_scale": should_scale,
            "reason": reason,
            "current_load": load,
            "total_credits": total_credits
        }

    def generate_agent_avatar(self, agent_name: str) -> Optional[str]:
        """ÏóêÏù¥Ï†ÑÌä∏Ïùò ÌéòÎ•¥ÏÜåÎÇòÏóê ÏµúÏ†ÅÌôîÎêú Í≥†Ïú† ÏïÑÎ∞îÌÉÄÎ•º ÏÉùÏÑ±Ìï®."""
        from gortex.utils.prompt_loader import loader
        persona = loader.personas.get(agent_name.lower(), {"description": f"A specialized AI agent named {agent_name}"})
        
        prompt = f"""Create a professional, modern avatar icon for an AI agent.
        Name: {agent_name}
        Role: {persona.get('role', 'Expert')}
        Personality: {persona.get('description')}
        Style: Cyberpunk, clean, minimalist, glowing neural pathways.
        """
        
        try:
            from openai import OpenAI
            client = OpenAI()
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size="1024x1024",
                quality="standard",
                n=1,
            )
            image_url = response.data[0].url
            # (Ïã§Ï†ú Íµ¨ÌòÑ Ïãú URLÏùò Ïù¥ÎØ∏ÏßÄÎ•º ui/assets/avatars/ Ìè¥ÎçîÏóê Îã§Ïö¥Î°úÎìúÌïòÏó¨ Ï†ÄÏû•)
            logger.info(f"üé® Avatar generated for {agent_name}: {image_url}")
            return image_url
        except Exception as e:
            logger.error(f"Avatar generation failed: {e}")
            return None

    def audit_autonomous_mission(self, mission: Dict[str, Any]) -> Dict[str, Any]:
        """ÏûêÏú®Ï†ÅÏúºÎ°ú ÏÉùÏÑ±Îêú ÎØ∏ÏÖòÏù¥ ÏãúÏä§ÌÖú ÌóåÎ≤ï Î∞è ÏïàÏ†Ñ Í∞ÄÏù¥ÎìúÎùºÏù∏Ïóê Î∂ÄÌï©ÌïòÎäîÏßÄ Ïò§ÎîîÌä∏Ìï®."""
        constitution = read_file("docs/CONSTITUTION.md")
        
        prompt = f"""You are the Constitutional Guardian. 
        Audit the following self-generated mission for ethical and safety compliance.
        
        [Constitution]:
        {constitution}
        
        [Proposed Mission]:
        {json.dumps(mission, indent=2, ensure_ascii=False)}
        
        Analyze if this mission could lead to:
        1. Unauthorized data exfiltration.
        2. Permanent damage to core system logic.
        3. Wasteful resource consumption.
        
        Return JSON ONLY:
        {{
            "is_approved": true/false,
            "risk_score": 0.0 ~ 1.0,
            "findings": ["finding 1", "finding 2"],
            "suggestion": "How to align the mission better"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Mission audit failed: {e}")
            return {"is_approved": False, "risk_score": 1.0, "findings": [str(e)]}

    def calculate_system_maturity(self, state: GortexState) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖúÏùò Ï¢ÖÌï© ÏÑ±ÏàôÎèÑ(System Maturity Index)Î•º ÏÇ∞Ï∂úÌï®."""
        from gortex.core.registry import registry
        # 1. ÏßÄÎä• ÏßÄÏàò: Îì±Î°ùÎêú ÏóêÏù¥Ï†ÑÌä∏ Ïàò Î∞è Î≤ÑÏ†Ñ Ìï©ÏÇ∞
        agent_count = len(registry.list_agents())
        intel_score = min(40, agent_count * 2)
        
        # 2. ÏûêÎ≥∏ ÏßÄÏàò: Ï¥ù ÌÅ¨Î†àÎîß ÏûîÍ≥†
        total_credits = sum(a.get("credits", 0) for a in state.get("agent_economy", {}).values())
        capital_score = min(30, total_credits / 10.0)
        
        # 3. Ïã†Î¢∞ ÏßÄÏàò: ÌèâÍ∑† ÏÑ±Í≥µÎ•†
        success_rates = [a.get("success_rate", 100) for a in state.get("agent_economy", {}).values()]
        avg_success = sum(success_rates) / len(success_rates) if success_rates else 100
        trust_score = (avg_success / 100.0) * 30
        
        total_maturity = round(intel_score + capital_score + trust_score, 1)
        
        return {
            "score": total_maturity,
            "intelligence": intel_score,
            "capital": capital_score,
            "trust": trust_score,
            "grade": "Ascending" if total_maturity > 80 else "Stable"
        }

    def create_mentoring_package(self, mentor_name: str, category: str) -> Optional[Dict[str, Any]]:
        """ÏàôÎ†®Îêú ÏóêÏù¥Ï†ÑÌä∏Ïùò ÏßÄÏãùÏùÑ Ï†ïÏ†úÌïòÏó¨ ÍµêÏú°Ïö© Ìå®ÌÇ§ÏßÄÎ°ú ÎßåÎì¶."""
        # 1. Î©òÌÜ†Ïùò Í≥†ÏÑ±Í≥º Í∑úÏπô Ï∂îÏ∂ú
        rules = [r for r in self.memory.shards.get(category, []) if r.get("success_count", 0) > 10 and r.get("is_certified")]
        
        if not rules: return None
        
        logger.info(f"üë®‚Äçüè´ [Mentoring] Extracting wisdom from Mentor '{mentor_name}' in {category}...")
        
        prompt = f"""You are the Synaptic Mentor. 
        Compress the following certified rules from a Master '{mentor_name}' into a core 'Mentoring Syllabus' for a junior agent.
        Focus on high-level principles and common pitfalls.
        
        [Rules]:
        {json.dumps(rules, ensure_ascii=False)}
        
        Return JSON ONLY:
        {{
            "syllabus_id": "SYLLABUS_ID",
            "mentor": "{mentor_name}",
            "core_lessons": ["lesson 1", "lesson 2"],
            "distilled_rules": [{{ "instruction": "...", "trigger": "..." }}]
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Mentoring package creation failed: {e}")
            return None

    def audit_dependency(self, package_name: str) -> Dict[str, Any]:
        """ÏÑ§Ïπò Ï†úÏïàÎêú Ïô∏Î∂Ä ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò Î≥¥ÏïàÏÑ± Î∞è ÌÉÄÎãπÏÑ±ÏùÑ Í≤ÄÏàòÌï®."""
        logger.info(f"üõ°Ô∏è Auditing dependency: {package_name}...")
        
        prompt = f"""You are the Supply Chain Security Officer. 
        Evaluate the following Python library for Gortex integration.
        [Package Name]: {package_name}
        
        Analyze:
        1. Popularity & Reliability (Is it well-maintained?)
        2. Security Risk (Are there known vulnerabilities or dangerous behaviors?)
        3. Necessity (Is there a built-in alternative?)
        
        Return JSON ONLY:
        {{
            "is_approved": true/false,
            "risk_level": "Low/Medium/High",
            "findings": ["finding 1", "finding 2"],
            "recommendation": "Install / Use standard lib / Block"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Dependency audit failed: {e}")
            return {"is_approved": False, "risk_level": "High", "findings": [str(e)]}

    def evaluate_artifact_value(self, directory: str = "logs") -> List[Dict[str, Any]]:
        """ÏûëÏóÖ Î∂ÄÏÇ∞Î¨ºÎì§Ïùò Í∞ÄÏπòÎ•º ÌèâÍ∞ÄÌïòÏó¨ ÏÇ≠Ï†ú ÌõÑÎ≥¥ Î™©Î°ùÏùÑ ÏÉùÏÑ±Ìï®."""
        cleanup_candidates = []
        now = datetime.now()
        
        # Ï≤≠ÏÜå ÎåÄÏÉÅ Ìè¥Îçî Ï†ïÏùò
        target_dirs = [
            os.path.join(directory, "backups"),
            os.path.join(directory, "versions"),
            "training_jobs" # Ïò§ÎûòÎêú ÌïôÏäµ Ïû° Ìè¨Ìï®
        ]
        
        for d in target_dirs:
            if not os.path.exists(d): continue
            
            for f in os.listdir(d):
                path = os.path.join(d, f)
                if os.path.isdir(path): continue
                
                mtime = datetime.fromtimestamp(os.path.getmtime(path))
                age_days = (now - mtime).days
                size_kb = os.path.getsize(path) / 1024
                
                # Í∞ÄÏπò ÌèâÍ∞Ä Î°úÏßÅ: 7Ïùº Ïù¥ÏÉÅ Îêú Î∞±ÏóÖÏùÄ ÎÇÆÏùÄ Í∞ÄÏπò
                value_score = 100
                if age_days > 7: value_score -= 50
                if age_days > 30: value_score -= 40
                
                # ÌäπÏ†ï ÌôïÏû•Ïûê(Î∞±ÏóÖ) Í∞ÄÏ§ëÏπò
                if f.endswith(".bak"): value_score -= 10
                
                if value_score < 50:
                    cleanup_candidates.append({
                        "path": path,
                        "age_days": age_days,
                        "size_kb": round(size_kb, 1),
                        "reason": "Old backup/artifact" if age_days > 7 else "Low priority"
                    })
                    
        return sorted(cleanup_candidates, key=lambda x: x["age_days"], reverse=True)

    def perform_autonomous_cleanup(self) -> Dict[str, Any]:
        """Î∂ÄÏÇ∞Î¨º Í∞ÄÏπò ÌèâÍ∞Ä Î∞è ÏûêÏú® ÏÇ≠Ï†ú ÌÜµÌï© ÏàòÌñâ"""
        candidates = self.evaluate_artifact_value()
        if not candidates:
            return {"status": "skipped", "message": "No cleanup candidates found."}
            
        target_paths = [c["path"] for c in candidates]
        total_size_kb = sum(c["size_kb"] for c in candidates)
        
        from gortex.utils.tools import safe_bulk_delete
        result = safe_bulk_delete(target_paths)
        
        freed_count = len(result["success"])
        return {
            "status": "success",
            "deleted_count": freed_count,
            "freed_kb": round(total_size_kb, 1) if freed_count > 0 else 0,
            "message": f"üßπ Autonomous cleanup finished. {freed_count} files removed, {round(total_size_kb, 1)} KB freed."
        }

    def generate_milestone_report(self, start_session: int = 1, end_session: int = 100) -> str:
        """ÏßÄÏ†ïÎêú Î≤îÏúÑÏùò ÏÑ∏ÏÖòÎì§ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÎßàÏùºÏä§ÌÜ§ Î≥¥Í≥†ÏÑúÎ•º ÏÉùÏÑ±Ìï®."""
        session_dir = "docs/sessions"
        if not os.path.exists(session_dir):
            return "Session directory not found."

        summary_parts = []
        for i in range(start_session, end_session + 1):
            path = os.path.join(session_dir, f"session_{i:04d}.md")
            if os.path.exists(path):
                from gortex.utils.tools import read_file
                content = read_file(path)
                # Í∞Å ÏÑ∏ÏÖòÏùò Î™©ÌëúÏôÄ Í≤∞Í≥ºÎßå Ï∂îÏ∂ú (Îã®ÏàúÌôî)
                goal_match = re.search(r"## üéØ Goal(.*?)(?=\n##|$)", content, re.DOTALL)
                outcome_match = re.search(r"## üìà Outcomes(.*?)(?=\n##|$)", content, re.DOTALL)
                
                if goal_match:
                    summary_parts.append(f"S{i:03d}: {goal_match.group(1).strip()}")

        combined_summary = "\n".join(summary_parts)
        
        prompt = f"""Îã§ÏùåÏùÄ Gortex ÏãúÏä§ÌÖúÏùò {start_session}ÌöåÎ∂ÄÌÑ∞ {end_session}ÌöåÍπåÏßÄÏùò Í∞úÎ∞ú Í∏∞Î°ùÏù¥Îã§.
        Ïù¥ Í∏∞Î°ùÏùÑ Î∞îÌÉïÏúºÎ°ú GortexÍ∞Ä Ïñ¥ÎñªÍ≤å ÏßÑÌôîÌï¥ÏôîÎäîÏßÄ 5Í∞ÄÏßÄ ÌïµÏã¨ ÌÖåÎßàÎ°ú ÏöîÏïΩÌïòÍ≥†, 
        ÎØ∏ÎûòÎ•º ÏúÑÌïú Ï†úÏñ∏ÏùÑ Ìè¨Ìï®Ìïú '100ÏÑ∏ÏÖò Í∏∞ÎÖê ÎßàÏùºÏä§ÌÜ§ Î≥¥Í≥†ÏÑú'Î•º ÏûëÏÑ±ÌïòÎùº.
        
        [Session Logs]:
        {combined_summary}
        
        ÎãµÎ≥ÄÏùÄ Markdown ÌòïÏãùÏúºÎ°ú ÏûëÏÑ±ÌïòÎùº.
        """
        
        try:
            report = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            output_path = "docs/MILESTONE_100.md"
            from gortex.utils.tools import write_file
            write_file(output_path, f"# üèÜ Gortex 100-Session Milestone Report\n\n> {datetime.now()}\n\n{report}")
            return f"‚úÖ Milestone report generated: {output_path}"
        except Exception as e:
            logger.error(f"Milestone report generation failed: {e}")
            return f"‚ùå Failed: {e}"

    def archive_system_logs(self) -> Dict[str, Any]:
        """ÎàÑÏ†ÅÎêú Î°úÍ∑∏ ÌååÏùºÏùÑ ÏïÑÏπ¥Ïù¥ÎπôÌïòÍ≥† ÏßÄÏãù ÌååÏùºÏùÑ Î∞±ÏóÖÌï®."""
        from gortex.utils.tools import compress_directory, backup_file_with_rotation
        
        # 1. ÌïµÏã¨ ÏßÄÏãù ÌååÏùº Î∞±ÏóÖ
        bk_res = backup_file_with_rotation("experience.json", max_versions=10)
        
        # 2. Ïò§ÎûòÎêú Î°úÍ∑∏ ÏïÑÏπ¥Ïù¥Îπô
        log_dir = "logs"
        archive_dir = "logs/archives"
        os.makedirs(archive_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d")
        zip_path = os.path.join(archive_dir, f"logs_backup_{timestamp}.zip")
        
        # logs/ ÎÇ¥Î∂ÄÏùò Í∞úÎ≥Ñ .jsonl ÌååÏùºÎì§ÏùÑ Ï∞æÏïÑÏÑú ÏïïÏ∂ï (Ïù¥ÎØ∏ ÏïïÏ∂ïÎêú archives Ï†úÏô∏)
        files_to_archive = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.endswith(".jsonl")]
        
        if not files_to_archive:
            return {"status": "skipped", "backup": bk_res, "reason": "No logs to archive."}
            
        # ÏûÑÏãú Ìè¥ÎçîÎ°ú Î≥µÏÇ¨ ÌõÑ ÏïïÏ∂ï (ÏõêÎ≥∏ Î≥¥Ìò∏)
        temp_archive_root = "logs/temp_archive"
        os.makedirs(temp_archive_root, exist_ok=True)
        for f in files_to_archive:
            shutil.copy2(f, temp_archive_root)
            
        comp_res = compress_directory(temp_archive_root, zip_path)
        shutil.rmtree(temp_archive_root)
        
        # ÏïÑÏπ¥Ïù¥Îπô ÏÑ±Í≥µ Ïãú ÏõêÎ≥∏ Î°úÍ∑∏ ÏÇ≠Ï†ú (Ï†ïÏ±ÖÏóê Îî∞Îùº ÏÑ†ÌÉùÏ†Å)
        # Ïó¨Í∏∞ÏÑúÎäî ÏïàÏ†ÑÏùÑ ÏúÑÌï¥ ÏÇ≠Ï†ú ÎåÄÏã† .old ÌôïÏû•ÏûêÎ•º Î∂ôÏù¥Í±∞ÎÇò Í∑∏ÎåÄÎ°ú Îë†.
        # ÏùºÎã® ÏïÑÏπ¥Ïù¥Îπô ÏÑ±Í≥µ Î©îÏãúÏßÄÎßå Î∞òÌôò
        
        return {
            "status": "success",
            "backup": bk_res,
            "archive": zip_path,
            "message": f"System maintenance complete. 10 knowledge versions kept. Logs archived to {zip_path}"
        }

    def propose_proactive_refactoring(self) -> List[Dict[str, Any]]:
        """Î≥µÏû°ÎèÑÍ∞Ä ÎÜíÏùÄ ÌååÏùºÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏÑ†Ï†úÏ†Å Î¶¨Ìå©ÌÜ†ÎßÅ Í≥ÑÌöçÏùÑ Ï†úÏïàÌï®."""
        # 1. Í≥†Î≥µÏû°ÎèÑ ÌååÏùº ÏãùÎ≥Ñ
        complex_files = self.scan_project_complexity()
        if not complex_files:
            return []
            
        proposals = []
        for item in complex_files[:2]: # Í≥ºÎ∂ÄÌïò Î∞©ÏßÄÎ•º ÏúÑÌï¥ ÏÉÅÏúÑ 2Í∞úÎßå Ï≤òÎ¶¨
            file_path = item["file"]
            content = read_file(file_path)
            
            prompt = f"""You are the Guardian Architect. 
            Analyze the following complex code and propose a PROACTIVE refactoring to improve maintainability and prevent future bugs.
            
            [File]: {file_path}
            [Complexity Score]: {item['score']}
            [Issue]: {item['issue']}
            [Code]:
            {content[:3000]}
            
            Return JSON ONLY:
            {{
                "target_file": "{file_path}",
                "reason": "Specific technical justification",
                "action_plan": ["Step 1: ...", "Step 2: ..."],
                "risk_level": "Low/Medium/High",
                "expected_gain": "e.g., Reduced cyclomatic complexity"
            }}
            """
            try:
                response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
                
                proposals.append(res_data)
                logger.info(f"üõ°Ô∏è Proactive refactoring proposed for: {file_path}")
            except Exception as e:
                logger.error(f"Failed to generate proactive refactoring for {file_path}: {e}")
                
        return proposals

    def scan_project_complexity(self, directory: str = ".") -> List[Dict[str, Any]]:
        debt_list = []
        ignore_dirs = {'.git', 'venv', '__pycache__', 'logs', 'site-packages'}
        for root, dirs, files in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            for f in files:
                if f.endswith(".py"):
                    path = os.path.join(root, f)
                    try:
                        with open(path, 'r', encoding='utf-8') as file:
                            content = file.read()
                            lines = content.splitlines()
                            score = len(re.findall(r"\b(if|elif|for|while|except|def|class|with|async)\b", content))
                            score += len(lines) // 20
                            if score > 10:
                                debt_list.append({
                                    "file": path, "score": score, 
                                    "reason": "High logical density" if score > 30 else "Moderate complexity",
                                    "issue": "ÌååÏùºÏùò ÎÖºÎ¶¨Ï†Å Î∞ÄÎèÑÍ∞Ä ÎÑàÎ¨¥ ÎÜíÏïÑ Í∞ÄÎèÖÏÑ±Ïù¥ Ï†ÄÌïòÎê®",
                                    "refactor_strategy": "Í∏¥ Î©îÏÑúÎìúÎ•º Î∂ÑÎ¶¨ÌïòÍ≥† Í¥ÄÏã¨ÏÇ¨Î•º Î™®ÎìàÎ°ú Í≤©Î¶¨ÌïòÎùº"
                                })
                    except: pass
        return sorted(debt_list, key=lambda x: x["score"], reverse=True)

    def analyze_data(self, file_path: str) -> Dict[str, Any]:
        try:
            import pandas as pd
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
                return {"status": "success", "summary": df.describe().to_dict(), "file": file_path}
        except: pass
        return {"status": "failed", "reason": "Data analysis failed"}

    def identify_missing_tests(self) -> List[Dict[str, Any]]:
        try:
            import subprocess
            subprocess.run(["python3", "-m", "coverage", "json", "-o", "logs/coverage.json"], capture_output=True)
            if os.path.exists("logs/coverage.json"):
                with open("logs/coverage.json", "r") as f:
                    data = json.load(f)
                results = []
                for file_path, info in data.get("files", {}).items():
                    p = info.get("summary", {}).get("percent_covered", 100)
                    if p < 80:
                        results.append({"file": file_path, "coverage": round(p, 1), "missing_lines": info.get("missing_lines", [])})
                return sorted(results, key=lambda x: x["coverage"])
        except: pass
        return []

    def audit_architecture(self) -> List[Dict[str, Any]]:
        from gortex.utils.indexer import SynapticIndexer
        deps = SynapticIndexer().generate_dependency_graph()
        violations = []
        layers = {"utils": 0, "core": 1, "ui": 2, "agents": 3, "tests": 4}
        for dep in deps:
            s, t = dep["source"], dep["target"]
            sl = next((l for l in layers if f"gortex.{l}" in s or s.startswith(l)), None)
            tl = next((l for l in layers if f"gortex.{l}" in t or t.startswith(l)), None)
            if sl and tl and layers[sl] < layers[tl]:
                violations.append({"type": "Layer Violation", "source": s, "target": t, "reason": f"ÌïòÏúÑ Î†àÏù¥Ïñ¥ '{sl}'Í∞Ä ÏÉÅÏúÑ Î†àÏù¥Ïñ¥ '{tl}'Î•º Ï∞∏Ï°∞Ìï®"})
        return violations

    def generate_dependency_graph_with_weights(self) -> Dict[str, Any]:
        """
        ÌîÑÎ°úÏ†ùÌä∏ ÎÇ¥ Î™®Îìà ÏùòÏ°¥ÏÑ± Í∑∏ÎûòÌîÑÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
        Í∞ÄÏ§ëÏπò(Ïó∞Í≤∞ Ïàò)ÏôÄ ÎÖ∏Îìú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ìè¨Ìï®ÌïòÏó¨ ÏãúÍ∞ÅÌôîÏóê Ï†ÅÌï©Ìïú ÌòïÌÉúÎ•º Î∞òÌôòÌï©ÎãàÎã§.
        """
        from gortex.utils.indexer import SynapticIndexer
        raw_deps = SynapticIndexer().generate_dependency_graph()
        
        nodes = {}
        edges = []
        
        # 1. ÎÖ∏Îìú Î∞è Ïó£ÏßÄ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
        for dep in raw_deps:
            s, t = dep["source"], dep["target"]
            
            # ÎÖ∏Îìú Îì±Î°ù (ÏóÜÏúºÎ©¥ Ï¥àÍ∏∞Ìôî)
            if s not in nodes: nodes[s] = {"id": s, "value": 0, "connections": 0}
            if t not in nodes: nodes[t] = {"id": t, "value": 0, "connections": 0}
            
            # Ïó∞Í≤∞ Ïàò Ï¶ùÍ∞Ä (Ï§ëÏöîÎèÑ)
            nodes[s]["value"] += 1
            nodes[t]["connections"] += 1
            
            # Ïó£ÏßÄ Ï∂îÍ∞Ä
            edges.append({"from": s, "to": t, "weight": 1})
            
        return {"nodes": list(nodes.values()), "edges": edges}

    def synthesize_global_rules(self, model_id: str = "gemini-1.5-pro") -> str:
        rules = self.memory.memory
        if not rules: return "Ï†ïÎ¶¨Ìï† Í∑úÏπôÏù¥ ÏóÜÏäµÎãàÎã§."
        ctx = "".join([f"- [{r['severity']}] {r['learned_instruction']}\n" for r in rules])
        try:
            summary = self.backend.generate(model_id, [{"role": "user", "content": f"Îã§Ïùå Í∑úÏπôÏùÑ 5Í∞ÄÏßÄ ÏõêÏπôÏúºÎ°ú ÏöîÏïΩÌïòÎùº:\n{ctx}"}])
            rules_md_path = "docs/RULES.md"
            original = ""
            if os.path.exists(rules_md_path):
                with open(rules_md_path, 'r', encoding='utf-8') as f: original = f.read()
            section = "## ü§ñ Auto-Evolved Coding Standards"
            new_c = f"{original.split(section)[0]}{section}\n\n> Í∞±Ïã†: {datetime.now()}\n\n{summary}" if section in original else f"{original}\n\n{section}\n\n{summary}"
            with open(rules_md_path, 'w', encoding='utf-8') as f: f.write(new_c)
            return "‚úÖ Ï†ÑÏó≠ Í∑úÏπô Ï¢ÖÌï© ÏôÑÎ£å."
        except: return "‚ùå Ïã§Ìå®"

    def predict_architectural_bottleneck(self) -> Dict[str, Any]:
        """Í≥ºÍ±∞ Í±¥Í∞ïÎèÑ Ï†êÏàò Ïù¥Î†•ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÎØ∏Îûò Î≥ëÎ™© ÏßÄÏ†ê ÏòàÏ∏°"""
        # (Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî logs/trace.jsonl ÎòêÎäî Î≥ÑÎèÑ ÌÜµÍ≥Ñ ÌååÏùº Ï∞∏Ï°∞)
        # ÌòÑÏû¨Îäî Îã®Ïàú ÏÑ†Ìòï ÌöåÍ∑Ä Ï∂îÏ†ï Î∞©ÏãùÏùò Î°úÏßÅ Íµ¨Ï°∞ ÎßàÎ†®
        from gortex.utils.indexer import SynapticIndexer
        current_health = SynapticIndexer().calculate_health_score()
        
        # Í∞ÄÏÉÅÏùò ÌûàÏä§ÌÜ†Î¶¨ Î∂ÑÏÑù (Ï∂îÌõÑ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ïó∞Îèô)
        score = current_health["score"]
        trend = "Stable"
        if score < 60: trend = "Declining"
        elif score > 80: trend = "Improving"
        
        prediction = {
            "current_score": score,
            "projected_score_3_sessions": round(score * 0.95, 1) if trend == "Declining" else score,
            "risk_level": "High" if score < 50 else "Medium" if score < 70 else "Low",
            "bottleneck_candidates": ["Dependency Bloat", "Missing Unit Tests"] if score < 70 else []
        }
        return prediction

    def reinforce_successful_personas(self):
        """Í∞ÄÏÉÅ ÌéòÎ•¥ÏÜåÎÇòÏùò ÏÑ±Í≥ºÎ•º Î∂ÑÏÑùÌïòÏó¨ Ïö∞Ïàò ÏßÄÏπ®ÏùÑ Ï†ïÏãù ÌéòÎ•¥ÏÜåÎÇòÏóê ÌÜµÌï©"""
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        perf = EfficiencyMonitor().get_persona_performance()
        
        p_path = "docs/i18n/personas.json"
        if not os.path.exists(p_path): return
        
        with open(p_path, 'r', encoding='utf-8') as f:
            personas = json.load(f)
            
        updated = False
        for p_name, stats in perf.items():
            # ÏÑ±Í≥µÎ•† 90% Ïù¥ÏÉÅÏù∏ Í≤ΩÏö∞ Í∞ïÌôî ÎåÄÏÉÅÏúºÎ°ú Í≥†Î†§
            if stats["rate"] >= 90.0 and p_name not in personas:
                logger.info(f"üåü High performing virtual persona detected: {p_name}")
                # (Îã®ÏàúÌôî: Ïã§Ï†ú Íµ¨ÌòÑ Ïãú LLMÏù¥ ÏßÄÏπ®ÏùÑ Ï†ïÏ†úÌïòÏó¨ Î≥ëÌï©)
                personas[p_name] = {
                    "name": p_name,
                    "description": "Successfully evolved from virtual persona",
                    "traits": ["proven", "reliable"],
                    "focus": ["general"]
                }
                updated = True
        
        if updated:
            with open(p_path, 'w', encoding='utf-8') as f:
                json.dump(personas, f, indent=2, ensure_ascii=False)
            logger.info("‚úÖ Official personas reinforced with successful evolution.")

    def generate_release_note(self, model_id: str = "gemini-1.5-pro") -> str:
        try:
            import subprocess
            git_log = subprocess.run(["git", "log", "-n", "10", "--pretty=format:%s"], capture_output=True, text=True).stdout
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            evo = "\n".join([f"- {h['metadata'].get('tech')} applied to {h['metadata'].get('file')}" for h in EfficiencyMonitor().get_evolution_history(limit=5)])
            prompt = f"Îã§Ïùå Î°úÍ∑∏Î°ú Î¶¥Î¶¨Ï¶à ÎÖ∏Ìä∏Î•º ÏûëÏÑ±ÌïòÎùº:\n\n[Git]\n{git_log}\n\n[Evo]\n{evo}"
            summary = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            with open("docs/release_note.md", "w", encoding="utf-8") as f:
                f.write(f"# üöÄ Gortex Release Note\n\n> Generated at: {datetime.now()}\n\n{summary}")
            return "‚úÖ release_note.md Í∞±Ïã† ÏôÑÎ£å."
        except: return "‚ùå Ïã§Ìå®"

    def bump_version(self) -> str:
        v_path = "VERSION"
        try:
            cur_v = "1.0.0"
            if os.path.exists(v_path):
                with open(v_path, "r") as f: cur_v = f.read().strip()
            parts = [int(p) for p in cur_v.split(".")] if "." in cur_v else [1, 0, 0]
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            if len(EfficiencyMonitor().get_evolution_history(limit=5)) >= 5:
                parts[1] += 1
                parts[2] = 0
            else:
                parts[2] += 1
            new_v = ".".join(map(str, parts))
            with open(v_path, "w") as f: f.write(new_v)
            return new_v
        except: return "Error"

    def evolve_personas(self, model_id: str = "gemini-1.5-pro") -> str:
        """ÏóêÏù¥Ï†ÑÌä∏Î≥Ñ ÏÑ±Í≥º Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ ÌéòÎ•¥ÏÜåÎÇò ÏßÄÏπ®(personas.json)ÏùÑ ÏûêÎèô ÌäúÎãù"""
        try:
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            summary = EfficiencyMonitor().get_summary(days=14)
            
            # ÌòÑÏû¨ ÌéòÎ•¥ÏÜåÎÇò Î°úÎìú
            p_path = "docs/i18n/personas.json"
            with open(p_path, 'r', encoding='utf-8') as f:
                personas = json.load(f)

            prompt = f"""Îã§Ïùå ÏóêÏù¥Ï†ÑÌä∏ ÏÑ±Í≥º ÏöîÏïΩÍ≥º ÌòÑÏû¨ ÌéòÎ•¥ÏÜåÎÇò Ï†ïÏùòÎ•º Î∞îÌÉïÏúºÎ°ú, 
            ÏÑ±Îä•Ïù¥ ÎÇÆÏùÄ ÏóêÏù¥Ï†ÑÌä∏Ïùò ÏÑ±Í≤©ÏùÑ Îçî Ï†ÑÎ¨∏ÌôîÌïòÍ±∞ÎÇò ÏÑ±Í≥µÏ†ÅÏù∏ Ìå®ÌÑ¥ÏùÑ Î∞òÏòÅÌïòÏó¨ ÏßÄÏπ®ÏùÑ Í∞ïÌôîÌïòÎùº.
            
            [ÏÑ±Îä• ÏöîÏïΩ]
            {json.dumps(summary, indent=2)}
            
            [ÌòÑÏû¨ ÌéòÎ•¥ÏÜåÎÇò]
            {json.dumps(personas, indent=2, ensure_ascii=False)}
            
            ÏóÖÎç∞Ïù¥Ìä∏Îêú Ï†ÑÏ≤¥ personas.json ÎÇ¥Ïö©ÏùÑ Î∞òÌôòÌïòÎùº. Ïò§ÏßÅ JSONÎßå Ï∂úÎ†•ÌïòÎùº.
            """
            
            new_json_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            # JSON Ï∂îÏ∂ú Î°úÏßÅ (Ï†ïÍ∑úÏãù ÏÉùÎûµ - LLMÏù¥ Ï†ïÍµêÌïòÍ≤å Ï§Ñ Í≤ÉÏúºÎ°ú Í∏∞ÎåÄÌïòÎÇò Ï∂îÌõÑ Î≥¥Í∞ï Í∞ÄÎä•)
            
            with open(p_path, 'w', encoding='utf-8') as f:
                f.write(new_json_text)
                
            return "‚úÖ ÌéòÎ•¥ÏÜåÎÇò ÏûêÍ∞Ä ÏßÑÌôî ÏôÑÎ£å."
        except Exception as e:
            logger.error(f"Persona evolution failed: {e}")
            return f"‚ùå Ïã§Ìå®: {e}"

    def curate_evolution_data(self, output_path: str = "logs/datasets/evolution.jsonl") -> str:
        """
        ÏÑ±Í≥µÏ†ÅÏù∏ ÏûêÍ∞Ä ÏßÑÌôî ÏÇ¨Î°Ä(Experience Rules)Î•º ÏÑ†Î≥ÑÌïòÏó¨ 
        LLM Fine-tuningÏùÑ ÏúÑÌïú JSONL Ìè¨Îß∑ÏúºÎ°ú ÌÅêÎ†àÏù¥ÏÖòÌï©ÎãàÎã§.
        """
        memories = self.memory.memory
        if not memories:
            return "No evolutionary data found."
            
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        curated_count = 0
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                for mem in memories:
                    # Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌïÑÌÑ∞ÎßÅ (Ïã¨Í∞ÅÎèÑÍ∞Ä ÎÜíÍ±∞ÎÇò Î™ÖÌôïÌïú ÍµêÏ†ï ÏßÄÏãúÍ∞Ä ÏûàÎäî Í≤ΩÏö∞)
                    if not mem.get("learned_instruction") or not mem.get("trigger_context"):
                        continue
                        
                    # Fine-tuning Format (Chat-style)
                    entry = {
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are Gortex, an evolving AI agent. Analyze the failure and provide a corrected rule."
                            },
                            {
                                "role": "user", 
                                "content": f"Context/Failure:\n{mem['trigger_context']}\n\nFailed Attempt:\n{mem.get('failed_solution', 'N/A')}"
                            },
                            {
                                "role": "assistant",
                                "content": f"Evolutionary Rule:\n{mem['learned_instruction']}"
                            }
                        ]
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    curated_count += 1
                    
            return f"‚úÖ Curated {curated_count} items to {output_path}"
        except Exception as e:
            logger.error(f"Failed to curate evolution data: {e}")
            return f"‚ùå Failed: {e}"

    def optimize_knowledge_base(self, model_id: str = "gemini-2.0-flash") -> Dict[str, Any]:
        """
        ÏßÄÏãù Î≤†Ïù¥Ïä§(Experience Rules)Ïùò ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÍ≥† ÏµúÏ†ÅÌôîÌï®.
        ÏÑ±Í≥µÎ•†Ïù¥ ÎÇÆÏùÄ Í∑úÏπôÏùÑ Ï†úÍ±∞ÌïòÍ≥†, Ïú†ÏÇ¨Ìïú Í≥†ÏÑ±Í≥º Í∑úÏπôÏùÑ Î≥ëÌï©Ìï®.
        """
        rules = self.memory.memory
        if len(rules) < 5:
            return {"status": "skipped", "reason": "ÏßÄÏãù Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±ÌïòÏó¨ ÏµúÏ†ÅÌôîÎ•º ÏàòÌñâÌïòÏßÄ ÏïäÏùå."}

        original_count = len(rules)
        optimized_rules = []
        removed_count = 0
        
        # 1. ÏàòÏπò Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Heuristic Pruning)
        active_pool = []
        for r in rules:
            usage = r.get("usage_count", 0)
            success = r.get("success_count", 0)
            # ÏÉùÏÑ±Îêú ÏßÄ Ïò§ÎûòÎêòÏóàÎäîÎç∞(Ïòà: ÏÇ¨Ïö© 5Ìöå Ïù¥ÏÉÅ) ÏÑ±Í≥µÎ•†Ïù¥ 30% ÎØ∏ÎßåÏù∏ Í≤ΩÏö∞ Ìá¥Ï∂ú
            if usage >= 5 and (success / usage) < 0.3:
                removed_count += 1
                logger.info(f"üóëÔ∏è Rule {r['id']} removed due to low performance.")
                continue
            active_pool.append(r)

        # 2. LLM Í∏∞Î∞ò ÏãúÎß®Ìã± Î≥ëÌï© (Semantic Merging)
        rules_text = "\n".join([f"- [{r['id']}] {r['learned_instruction']} (Success: {r.get('success_count',0)}/{r.get('usage_count',0)})" for r in active_pool])
        
        prompt = f"""Îã§ÏùåÏùÄ ÏûêÍ∞Ä ÏßÑÌôî ÏãúÏä§ÌÖúÏù¥ ÏäµÎìùÌïú ÏßÄÏãù Î¶¨Ïä§Ìä∏Îã§.
        1. ÎÇ¥Ïö©Ïù¥ Ï§ëÎ≥µÎêòÍ±∞ÎÇò ÏÑúÎ°ú Î≥¥ÏôÑÏ†ÅÏù∏ Í≥†ÏÑ±Í≥º Í∑úÏπôÎì§ÏùÄ ÌïòÎÇòÏùò Îçî Í∞ïÎ†•ÌïòÍ≥† Î≤îÏö©Ï†ÅÏù∏ Í∑úÏπôÏúºÎ°ú Î≥ëÌï©ÌïòÎùº.
        2. Î≥ëÌï©Îêú Í∑úÏπôÏùÄ Í∞ÄÏû• ÌïµÏã¨Ï†ÅÏù∏ Ìä∏Î¶¨Í±∞ Ìå®ÌÑ¥ÏùÑ Ïú†ÏßÄÌï¥Ïïº ÌïúÎã§.
        3. Ïã§Ï†ú ÏÑ±Í≥µ ÏÇ¨Î°ÄÍ∞Ä ÎßéÏùÄ ÏßÄÏãùÏùÑ Ïö∞ÏÑ†ÌïòÎùº.
        
        [Knowledge List]
        {rules_text}
        
        Í≤∞Í≥ºÎäî Î∞òÎìúÏãú Î≥ëÌï© Î∞è Ï†ïÏ†úÎêú ÏµúÏ¢Ö JSON Î¶¨Ïä§Ìä∏Îßå Î∞òÌôòÌïòÎùº:
        [{{ "instruction": "...", "trigger_patterns": ["...", "..."], "severity": 1~5 }}]
        """
        
        try:
            response_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
            new_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            if isinstance(new_data, list):
                # ÏµúÏ¢Ö Î©îÎ™®Î¶¨ ÍµêÏ≤¥ (ÏïÑÏπ¥Ïù¥Îπô Ïù¥Î†•ÏùÑ ÎÇ®Í∏∞Í±∞ÎÇò Î∞±ÏóÖ Í∂åÏû•)
                updated_memory = []
                for idx, item in enumerate(new_data):
                    updated_memory.append({
                        "id": f"RULE_EVOLVED_{datetime.now().strftime('%Y%m%d')}_{idx}",
                        "learned_instruction": item["instruction"],
                        "trigger_patterns": item["trigger_patterns"],
                        "severity": item.get("severity", 3),
                        "created_at": datetime.now().isoformat(),
                        "usage_count": 0,
                        "success_count": 0,
                        "failure_count": 0,
                        "parent_rules": [r["id"] for r in active_pool], # Î™®Îì† Î∂ÄÎ™® ÌõÑÎ≥¥ Í∏∞Î°ù (Í≥ÑÎ≥¥ Ïó∞Í≤∞)
                        "is_super_rule": True # Î≥ëÌï©Îêú ÏßÄÎä•ÏûÑÏùÑ ÌëúÏãú
                    })
                
                # ÏÉ§Îî© ÏïÑÌÇ§ÌÖçÏ≤ò ÎåÄÏùë: Ï†ÑÏ≤¥Î•º 'general' ÏÉ§ÎìúÎ°ú Ï∑®Í∏âÌïòÍ±∞ÎÇò Í∞úÎ≥Ñ Î∂ÑÎ•ò ÌïÑÏöî
                # Ïó¨Í∏∞ÏÑúÎäî Î≥ëÌï©Îêú Ï†ÑÏó≠ ÏßÄÏãùÏù¥ÎØÄÎ°ú 'general' ÏÉ§ÎìúÎ°ú ÏóÖÎç∞Ïù¥Ìä∏
                self.memory.shards["general"] = updated_memory
                self.memory._persist_shard("general")
                
                return {
                    "status": "success",
                    "original": original_count,
                    "final": len(updated_memory),
                    "removed": removed_count,
                    "merged": original_count - removed_count - len(updated_memory)
                }
        except Exception as e:
            logger.error(f"Knowledge optimization failed: {e}")
            return {"status": "error", "reason": str(e)}

    def rank_context_relevance(self, messages: List[Dict[str, str]], current_plan: List[str]) -> List[float]:
        """Î©îÏãúÏßÄ Î≠âÏπòÏôÄ ÌòÑÏû¨ Í≥ÑÌöç Í∞ÑÏùò ÏãúÎß®Ìã± Í¥ÄÎ†®ÏÑ± Ï†êÏàòÎ•º ÏÇ∞Ï∂úÌï®."""
        if not messages or not current_plan: return [0.5] * len(messages)
        
        prompt = f"""You are the Context Librarian. 
        Rank each message's relevance to the current execution plan (0.0 to 1.0).
        High score if the message contains essential technical details or tool outputs needed for the plan.
        Low score if it's general chat or unrelated noise.
        
        [Plan]:
        {json.dumps(current_plan, ensure_ascii=False)}
        
        [Messages]:
        {json.dumps(messages, ensure_ascii=False)}
        
        Return JSON ONLY:
        {{ "scores": [0.9, 0.2, 0.5, ...] }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            return res_data.get("scores", [0.5] * len(messages))
        except Exception as e:
            logger.error(f"Context ranking failed: {e}")
            return [0.5] * len(messages)

    def garbage_collect_knowledge(self, model_id: str = "gemini-2.0-flash") -> Dict[str, Any]:
        """ÏßÄÏãù Î≤†Ïù¥Ïä§Ïùò Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò Î∞è ÏãúÎß®Ìã± ÏµúÏ†ÅÌôî ÏàòÌñâ."""
        report = {"removed": 0, "merged": 0, "optimized_shards": []}
        
        for cat in list(self.memory.shards.keys()):
            shard = self.memory.shards[cat]
            if not shard: continue
            
            # 1. Í∞ÄÏπò Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Heuristic Pruning)
            original_count = len(shard)
            active_rules = []
            for r in shard:
                val = self.memory.calculate_rule_value(r)
                if val <= 30.0:
                    report["removed"] += 1
                    logger.info(f"üóëÔ∏è Pruned low-value rule: {r['id']} (Value: {val})")
                    continue
                active_rules.append(r)
            
            # 2. Í≥†Í∞ÄÏπò Í∑úÏπô ÏãúÎß®Ìã± Î≥ëÌï© (Semantic Merging)
            if len(active_rules) >= 5:
                logger.info(f"‚ú® Optimizing shard '{cat}' semantically...")
                # Í∏∞Ï°¥ prune_memory Î°úÏßÅ ÌôúÏö© ÎòêÎäî Í≥†ÎèÑÌôî
                self.memory.shards[cat] = active_rules
                self.memory.prune_memory(model_id=model_id)
                optimized_count = len(self.memory.shards[cat])
                report["merged"] += (len(active_rules) - optimized_count)
                report["optimized_shards"].append(cat)
            else:
                self.memory.shards[cat] = active_rules
                
            self.memory._persist_shard(cat)
            
        return report

    def identify_test_hotspots(self) -> List[Dict[str, Any]]:
        """ÏàòÏ†ï ÏòÅÌñ•Î†•Ïù¥ ÌÅ¨ÏßÄÎßå ÌÖåÏä§Ìä∏Í∞Ä ÎàÑÎùΩÎêú 'Ìï´Ïä§Ìåü'ÏùÑ ÏãùÎ≥ÑÌï®."""
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        
        # 1. ÌÖåÏä§Ìä∏ ÎàÑÎùΩ ÌååÏùº Î™©Î°ù ÌöçÎìù
        missing_tests = self.identify_missing_tests()
        if not missing_tests: return []
        
        hotspots = []
        for item in missing_tests:
            file_path = item["file"]
            # 2. Ìï¥Îãπ ÌååÏùºÏùò ÏòÅÌñ•Î†• Î∞òÍ≤Ω(Impact Radius) Î∂ÑÏÑù
            impact = indexer.get_impact_radius(file_path)
            
            # ÏúÑÌóò Ï†êÏàò ÏÇ∞Ï∂ú: (ÎàÑÎùΩ Ïª§Î≤ÑÎ¶¨ÏßÄ Í∞ÄÏ§ëÏπò) * (ÏòÅÌñ• Î∞õÎäî Î™®Îìà Ïàò + 1)
            coverage_gap = 100 - item["coverage"]
            impact_score = (len(impact["direct"]) + len(impact["indirect"]) * 0.5 + 1)
            risk_score = round(coverage_gap * impact_score, 1)
            
            hotspots.append({
                "file": file_path,
                "coverage": item["coverage"],
                "impact_count": len(impact["direct"]) + len(impact["indirect"]),
                "risk_score": risk_score,
                "reason": f"High impact ({len(impact['direct'])} direct deps) with low coverage ({item['coverage']}%)"
            })
            
        return sorted(hotspots, key=lambda x: x["risk_score"], reverse=True)

    def generate_evolution_roadmap(self) -> List[Dict[str, Any]]:
        """ÏßÄÎä• ÏßÄÏàòÍ∞Ä ÎÇÆÏùÄ Î™®ÎìàÏùÑ ÏãùÎ≥ÑÌïòÏó¨ ÏßÑÌôî Ïö∞ÏÑ†ÏàúÏúÑ Î°úÎìúÎßµ ÏÉùÏÑ±"""
        from gortex.utils.indexer import SynapticIndexer
        intel_map = SynapticIndexer().calculate_intelligence_index()
        
        # ÏßÄÎä• ÏßÄÏàòÍ∞Ä ÎÇÆÏùÄ ÏàúÏúºÎ°ú Ï†ïÎ†¨ (Î≥¥ÏôÑÏù¥ ÌïÑÏöîÌïú Î™®Îìà)
        weak_modules = sorted(intel_map.items(), key=lambda x: x[1])
        
        # Tech Radar ÌõÑÎ≥¥Íµ∞ ÌöçÎìù
        adoption_candidates = []
        if os.path.exists("tech_radar.json"):
            try:
                with open("tech_radar.json", "r") as f:
                    radar_data = json.load(f)
                    adoption_candidates = radar_data.get("adoption_candidates", [])
            except: pass

        roadmap = []
        for file_path, score in weak_modules[:5]: # Í∞ÄÏû• Ï∑®ÏïΩÌïú 5Í∞ú Î™®Îìà ÎåÄÏÉÅ
            # Ìï¥Îãπ ÌååÏùºÏóê Ï†ÅÏö© Í∞ÄÎä•Ìïú Ïã†Í∏∞Ïà† Ï†úÏïà Îß§Ïπ≠
            suggested_tech = next((c["tech"] for c in adoption_candidates if c["target_file"] == file_path), "Refactoring Required")
            
            roadmap.append({
                "target": file_path,
                "current_maturity": score,
                "suggested_tech": suggested_tech,
                "priority": "High" if score < 10 else "Medium"
            })
            
        return roadmap
