import logging
import json
import os
import re
import math
from typing import Dict, Any, List, Optional
from datetime import datetime
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.vector_store import LongTermMemory

logger = logging.getLogger("GortexAnalystBase")

class AnalystAgent:
    """Gortex ì‹œìŠ¤í…œì˜ ë¶„ì„ ë° ì§„í™” ë‹´ë‹¹ ì—ì´ì „íŠ¸ (Base Class)"""
    def __init__(self):
        self.backend = LLMFactory.get_default_backend()
        self.memory = EvolutionaryMemory()
        self.ltm = LongTermMemory()

    def calculate_efficiency_score(self, success: bool, tokens: int, latency_ms: int, energy_cost: int) -> float:
        if not success: return 0.0
        cost = (tokens * 0.01) + (latency_ms * 0.005) + (energy_cost * 2.0)
        score = 100.0 / (1.0 + math.log1p(cost / 5.0))
        return round(min(100.0, score), 1)

    def scan_project_complexity(self, directory: str = ".") -> List[Dict[str, Any]]:
        debt_list = []
        ignore_dirs = {'.git', 'venv', '__pycache__', 'logs', 'site-packages'}
        for root, dirs, files in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            for f in files:
                if f.endswith(".py"):
                    path = os.path.join(root, f)
                    try:
                        with open(path, 'r', encoding='utf-8') as file:
                            content = file.read()
                            lines = content.splitlines()
                            score = len(re.findall(r"\b(if|elif|for|while|except|def|class|with|async)\b", content))
                            score += len(lines) // 20
                            if score > 10:
                                debt_list.append({
                                    "file": path, "score": score, 
                                    "reason": "High logical density" if score > 30 else "Moderate complexity",
                                    "issue": "íŒŒì¼ì˜ ë…¼ë¦¬ì  ë°€ë„ê°€ ë„ˆë¬´ ë†’ì•„ ê°€ë…ì„±ì´ ì €í•˜ë¨",
                                    "refactor_strategy": "ê¸´ ë©”ì„œë“œë¥¼ ë¶„ë¦¬í•˜ê³  ê´€ì‹¬ì‚¬ë¥¼ ëª¨ë“ˆë¡œ ê²©ë¦¬í•˜ë¼"
                                })
                    except: pass
        return sorted(debt_list, key=lambda x: x["score"], reverse=True)

    def analyze_data(self, file_path: str) -> Dict[str, Any]:
        try:
            import pandas as pd
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
                return {"status": "success", "summary": df.describe().to_dict(), "file": file_path}
        except: pass
        return {"status": "failed", "reason": "Data analysis failed"}

    def identify_missing_tests(self) -> List[Dict[str, Any]]:
        try:
            import subprocess
            subprocess.run(["python3", "-m", "coverage", "json", "-o", "logs/coverage.json"], capture_output=True)
            if os.path.exists("logs/coverage.json"):
                with open("logs/coverage.json", "r") as f:
                    data = json.load(f)
                results = []
                for file_path, info in data.get("files", {}).items():
                    p = info.get("summary", {}).get("percent_covered", 100)
                    if p < 80:
                        results.append({"file": file_path, "coverage": round(p, 1), "missing_lines": info.get("missing_lines", [])})
                return sorted(results, key=lambda x: x["coverage"])
        except: pass
        return []

    def audit_architecture(self) -> List[Dict[str, Any]]:
        from gortex.utils.indexer import SynapticIndexer
        deps = SynapticIndexer().generate_dependency_graph()
        violations = []
        layers = {"utils": 0, "core": 1, "ui": 2, "agents": 3, "tests": 4}
        for dep in deps:
            s, t = dep["source"], dep["target"]
            sl = next((l for l in layers if f"gortex.{l}" in s or s.startswith(l)), None)
            tl = next((l for l in layers if f"gortex.{l}" in t or t.startswith(l)), None)
            if sl and tl and layers[sl] < layers[tl]:
                violations.append({"type": "Layer Violation", "source": s, "target": t, "reason": f"í•˜ìœ„ ë ˆì´ì–´ '{sl}'ê°€ ìƒìœ„ ë ˆì´ì–´ '{tl}'ë¥¼ ì°¸ì¡°í•¨"})
        return violations

    def generate_dependency_graph_with_weights(self) -> Dict[str, Any]:
        """
        í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“ˆ ì˜ì¡´ì„± ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°€ì¤‘ì¹˜(ì—°ê²° ìˆ˜)ì™€ ë…¸ë“œ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ì‹œê°í™”ì— ì í•©í•œ í˜•íƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        from gortex.utils.indexer import SynapticIndexer
        raw_deps = SynapticIndexer().generate_dependency_graph()
        
        nodes = {}
        edges = []
        
        # 1. ë…¸ë“œ ë° ì—£ì§€ ê°€ì¤‘ì¹˜ ê³„ì‚°
        for dep in raw_deps:
            s, t = dep["source"], dep["target"]
            
            # ë…¸ë“œ ë“±ë¡ (ì—†ìœ¼ë©´ ì´ˆê¸°í™”)
            if s not in nodes: nodes[s] = {"id": s, "value": 0, "connections": 0}
            if t not in nodes: nodes[t] = {"id": t, "value": 0, "connections": 0}
            
            # ì—°ê²° ìˆ˜ ì¦ê°€ (ì¤‘ìš”ë„)
            nodes[s]["value"] += 1
            nodes[t]["connections"] += 1
            
            # ì—£ì§€ ì¶”ê°€
            edges.append({"from": s, "to": t, "weight": 1})
            
        return {"nodes": list(nodes.values()), "edges": edges}

    def synthesize_global_rules(self, model_id: str = "gemini-1.5-pro") -> str:
        rules = self.memory.memory
        if not rules: return "ì •ë¦¬í•  ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤."
        ctx = "".join([f"- [{r['severity']}] {r['learned_instruction']}\n" for r in rules])
        try:
            summary = self.backend.generate(model_id, [{"role": "user", "content": f"ë‹¤ìŒ ê·œì¹™ì„ 5ê°€ì§€ ì›ì¹™ìœ¼ë¡œ ìš”ì•½í•˜ë¼:\n{ctx}"}])
            rules_md_path = "docs/RULES.md"
            original = ""
            if os.path.exists(rules_md_path):
                with open(rules_md_path, 'r', encoding='utf-8') as f: original = f.read()
            section = "## ğŸ¤– Auto-Evolved Coding Standards"
            new_c = f"{original.split(section)[0]}{section}\n\n> ê°±ì‹ : {datetime.now()}\n\n{summary}" if section in original else f"{original}\n\n{section}\n\n{summary}"
            with open(rules_md_path, 'w', encoding='utf-8') as f: f.write(new_c)
            return "âœ… ì „ì—­ ê·œì¹™ ì¢…í•© ì™„ë£Œ."
        except: return "âŒ ì‹¤íŒ¨"

    def predict_architectural_bottleneck(self) -> Dict[str, Any]:
        """ê³¼ê±° ê±´ê°•ë„ ì ìˆ˜ ì´ë ¥ì„ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ ë³‘ëª© ì§€ì  ì˜ˆì¸¡"""
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” logs/trace.jsonl ë˜ëŠ” ë³„ë„ í†µê³„ íŒŒì¼ ì°¸ì¡°)
        # í˜„ì¬ëŠ” ë‹¨ìˆœ ì„ í˜• íšŒê·€ ì¶”ì • ë°©ì‹ì˜ ë¡œì§ êµ¬ì¡° ë§ˆë ¨
        from gortex.utils.indexer import SynapticIndexer
        current_health = SynapticIndexer().calculate_health_score()
        
        # ê°€ìƒì˜ íˆìŠ¤í† ë¦¬ ë¶„ì„ (ì¶”í›„ ì‹¤ì œ ë°ì´í„° ì—°ë™)
        score = current_health["score"]
        trend = "Stable"
        if score < 60: trend = "Declining"
        elif score > 80: trend = "Improving"
        
        prediction = {
            "current_score": score,
            "projected_score_3_sessions": round(score * 0.95, 1) if trend == "Declining" else score,
            "risk_level": "High" if score < 50 else "Medium" if score < 70 else "Low",
            "bottleneck_candidates": ["Dependency Bloat", "Missing Unit Tests"] if score < 70 else []
        }
        return prediction

    def reinforce_successful_personas(self):
        """ê°€ìƒ í˜ë¥´ì†Œë‚˜ì˜ ì„±ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìš°ìˆ˜ ì§€ì¹¨ì„ ì •ì‹ í˜ë¥´ì†Œë‚˜ì— í†µí•©"""
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        perf = EfficiencyMonitor().get_persona_performance()
        
        p_path = "docs/i18n/personas.json"
        if not os.path.exists(p_path): return
        
        with open(p_path, 'r', encoding='utf-8') as f:
            personas = json.load(f)
            
        updated = False
        for p_name, stats in perf.items():
            # ì„±ê³µë¥  90% ì´ìƒì¸ ê²½ìš° ê°•í™” ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤
            if stats["rate"] >= 90.0 and p_name not in personas:
                logger.info(f"ğŸŒŸ High performing virtual persona detected: {p_name}")
                # (ë‹¨ìˆœí™”: ì‹¤ì œ êµ¬í˜„ ì‹œ LLMì´ ì§€ì¹¨ì„ ì •ì œí•˜ì—¬ ë³‘í•©)
                personas[p_name] = {
                    "name": p_name,
                    "description": "Successfully evolved from virtual persona",
                    "traits": ["proven", "reliable"],
                    "focus": ["general"]
                }
                updated = True
        
        if updated:
            with open(p_path, 'w', encoding='utf-8') as f:
                json.dump(personas, f, indent=2, ensure_ascii=False)
            logger.info("âœ… Official personas reinforced with successful evolution.")

    def generate_release_note(self, model_id: str = "gemini-1.5-pro") -> str:
        try:
            import subprocess
            git_log = subprocess.run(["git", "log", "-n", "10", "--pretty=format:%s"], capture_output=True, text=True).stdout
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            evo = "\n".join([f"- {h['metadata'].get('tech')} applied to {h['metadata'].get('file')}" for h in EfficiencyMonitor().get_evolution_history(limit=5)])
            prompt = f"ë‹¤ìŒ ë¡œê·¸ë¡œ ë¦´ë¦¬ì¦ˆ ë…¸íŠ¸ë¥¼ ì‘ì„±í•˜ë¼:\n\n[Git]\n{git_log}\n\n[Evo]\n{evo}"
            summary = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            with open("docs/release_note.md", "w", encoding="utf-8") as f:
                f.write(f"# ğŸš€ Gortex Release Note\n\n> Generated at: {datetime.now()}\n\n{summary}")
            return "âœ… release_note.md ê°±ì‹  ì™„ë£Œ."
        except: return "âŒ ì‹¤íŒ¨"

    def bump_version(self) -> str:
        v_path = "VERSION"
        try:
            cur_v = "1.0.0"
            if os.path.exists(v_path):
                with open(v_path, "r") as f: cur_v = f.read().strip()
            parts = [int(p) for p in cur_v.split(".")] if "." in cur_v else [1, 0, 0]
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            if len(EfficiencyMonitor().get_evolution_history(limit=5)) >= 5:
                parts[1] += 1
                parts[2] = 0
            else:
                parts[2] += 1
            new_v = ".".join(map(str, parts))
            with open(v_path, "w") as f: f.write(new_v)
            return new_v
        except: return "Error"

    def evolve_personas(self, model_id: str = "gemini-1.5-pro") -> str:
        """ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨(personas.json)ì„ ìë™ íŠœë‹"""
        try:
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            summary = EfficiencyMonitor().get_summary(days=14)
            
            # í˜„ì¬ í˜ë¥´ì†Œë‚˜ ë¡œë“œ
            p_path = "docs/i18n/personas.json"
            with open(p_path, 'r', encoding='utf-8') as f:
                personas = json.load(f)

            prompt = f"""ë‹¤ìŒ ì—ì´ì „íŠ¸ ì„±ê³¼ ìš”ì•½ê³¼ í˜„ì¬ í˜ë¥´ì†Œë‚˜ ì •ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 
            ì„±ëŠ¥ì´ ë‚®ì€ ì—ì´ì „íŠ¸ì˜ ì„±ê²©ì„ ë” ì „ë¬¸í™”í•˜ê±°ë‚˜ ì„±ê³µì ì¸ íŒ¨í„´ì„ ë°˜ì˜í•˜ì—¬ ì§€ì¹¨ì„ ê°•í™”í•˜ë¼.
            
            [ì„±ëŠ¥ ìš”ì•½]
            {json.dumps(summary, indent=2)}
            
            [í˜„ì¬ í˜ë¥´ì†Œë‚˜]
            {json.dumps(personas, indent=2, ensure_ascii=False)}
            
            ì—…ë°ì´íŠ¸ëœ ì „ì²´ personas.json ë‚´ìš©ì„ ë°˜í™˜í•˜ë¼. ì˜¤ì§ JSONë§Œ ì¶œë ¥í•˜ë¼.
            """
            
            new_json_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            # JSON ì¶”ì¶œ ë¡œì§ (ì •ê·œì‹ ìƒëµ - LLMì´ ì •êµí•˜ê²Œ ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ë‚˜ ì¶”í›„ ë³´ê°• ê°€ëŠ¥)
            
            with open(p_path, 'w', encoding='utf-8') as f:
                f.write(new_json_text)
                
            return "âœ… í˜ë¥´ì†Œë‚˜ ìê°€ ì§„í™” ì™„ë£Œ."
        except Exception as e:
            logger.error(f"Persona evolution failed: {e}")
            return f"âŒ ì‹¤íŒ¨: {e}"

    def curate_evolution_data(self, output_path: str = "logs/datasets/evolution.jsonl") -> str:
        """
        ì„±ê³µì ì¸ ìê°€ ì§„í™” ì‚¬ë¡€(Experience Rules)ë¥¼ ì„ ë³„í•˜ì—¬ 
        LLM Fine-tuningì„ ìœ„í•œ JSONL í¬ë§·ìœ¼ë¡œ íë ˆì´ì…˜í•©ë‹ˆë‹¤.
        """
        memories = self.memory.memory
        if not memories:
            return "No evolutionary data found."
            
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        curated_count = 0
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                for mem in memories:
                    # ë°ì´í„° í’ˆì§ˆ í•„í„°ë§ (ì‹¬ê°ë„ê°€ ë†’ê±°ë‚˜ ëª…í™•í•œ êµì • ì§€ì‹œê°€ ìˆëŠ” ê²½ìš°)
                    if not mem.get("learned_instruction") or not mem.get("trigger_context"):
                        continue
                        
                    # Fine-tuning Format (Chat-style)
                    entry = {
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are Gortex, an evolving AI agent. Analyze the failure and provide a corrected rule."
                            },
                            {
                                "role": "user", 
                                "content": f"Context/Failure:\n{mem['trigger_context']}\n\nFailed Attempt:\n{mem.get('failed_solution', 'N/A')}"
                            },
                            {
                                "role": "assistant",
                                "content": f"Evolutionary Rule:\n{mem['learned_instruction']}"
                            }
                        ]
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    curated_count += 1
                    
            return f"âœ… Curated {curated_count} items to {output_path}"
        except Exception as e:
            logger.error(f"Failed to curate evolution data: {e}")
            return f"âŒ Failed: {e}"

    def optimize_knowledge_base(self, model_id: str = "gemini-2.0-flash") -> Dict[str, Any]:
        """
        ì§€ì‹ ë² ì´ìŠ¤(Experience Rules)ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ìµœì í™”í•¨.
        ì„±ê³µë¥ ì´ ë‚®ì€ ê·œì¹™ì„ ì œê±°í•˜ê³ , ìœ ì‚¬í•œ ê³ ì„±ê³¼ ê·œì¹™ì„ ë³‘í•©í•¨.
        """
        rules = self.memory.memory
        if len(rules) < 5:
            return {"status": "skipped", "reason": "ì§€ì‹ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ."}

        original_count = len(rules)
        optimized_rules = []
        removed_count = 0
        
        # 1. ìˆ˜ì¹˜ ê¸°ë°˜ í•„í„°ë§ (Heuristic Pruning)
        active_pool = []
        for r in rules:
            usage = r.get("usage_count", 0)
            success = r.get("success_count", 0)
            # ìƒì„±ëœ ì§€ ì˜¤ë˜ë˜ì—ˆëŠ”ë°(ì˜ˆ: ì‚¬ìš© 5íšŒ ì´ìƒ) ì„±ê³µë¥ ì´ 30% ë¯¸ë§Œì¸ ê²½ìš° í‡´ì¶œ
            if usage >= 5 and (success / usage) < 0.3:
                removed_count += 1
                logger.info(f"ğŸ—‘ï¸ Rule {r['id']} removed due to low performance.")
                continue
            active_pool.append(r)

        # 2. LLM ê¸°ë°˜ ì‹œë§¨í‹± ë³‘í•© (Semantic Merging)
        rules_text = "\n".join([f"- [{r['id']}] {r['learned_instruction']} (Success: {r.get('success_count',0)}/{r.get('usage_count',0)})" for r in active_pool])
        
        prompt = f"""ë‹¤ìŒì€ ìê°€ ì§„í™” ì‹œìŠ¤í…œì´ ìŠµë“í•œ ì§€ì‹ ë¦¬ìŠ¤íŠ¸ë‹¤.
        1. ë‚´ìš©ì´ ì¤‘ë³µë˜ê±°ë‚˜ ì„œë¡œ ë³´ì™„ì ì¸ ê³ ì„±ê³¼ ê·œì¹™ë“¤ì€ í•˜ë‚˜ì˜ ë” ê°•ë ¥í•˜ê³  ë²”ìš©ì ì¸ ê·œì¹™ìœ¼ë¡œ ë³‘í•©í•˜ë¼.
        2. ë³‘í•©ëœ ê·œì¹™ì€ ê°€ì¥ í•µì‹¬ì ì¸ íŠ¸ë¦¬ê±° íŒ¨í„´ì„ ìœ ì§€í•´ì•¼ í•œë‹¤.
        3. ì‹¤ì œ ì„±ê³µ ì‚¬ë¡€ê°€ ë§ì€ ì§€ì‹ì„ ìš°ì„ í•˜ë¼.
        
        [Knowledge List]
        {rules_text}
        
        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë³‘í•© ë° ì •ì œëœ ìµœì¢… JSON ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë¼:
        [{{ "instruction": "...", "trigger_patterns": ["...", "..."], "severity": 1~5 }}]
        """
        
        try:
            response_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
            new_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            if isinstance(new_data, list):
                # ìµœì¢… ë©”ëª¨ë¦¬ êµì²´ (ì•„ì¹´ì´ë¹™ ì´ë ¥ì„ ë‚¨ê¸°ê±°ë‚˜ ë°±ì—… ê¶Œì¥)
                updated_memory = []
                for idx, item in enumerate(new_data):
                    updated_memory.append({
                        "id": f"RULE_EVOLVED_{datetime.now().strftime('%Y%m%d')}_{idx}",
                        "learned_instruction": item["instruction"],
                        "trigger_patterns": item["trigger_patterns"],
                        "severity": item.get("severity", 3),
                        "created_at": datetime.now().isoformat(),
                        "usage_count": 0,
                        "success_count": 0,
                        "failure_count": 0,
                        "is_super_rule": True # ë³‘í•©ëœ ì§€ëŠ¥ì„ì„ í‘œì‹œ
                    })
                self.memory.memory = updated_memory
                self.memory._persist()
                
                return {
                    "status": "success",
                    "original": original_count,
                    "final": len(updated_memory),
                    "removed": removed_count,
                    "merged": original_count - removed_count - len(updated_memory)
                }
        except Exception as e:
            logger.error(f"Knowledge optimization failed: {e}")
            return {"status": "error", "reason": str(e)}

    def generate_evolution_roadmap(self) -> List[Dict[str, Any]]:
        """ì§€ëŠ¥ ì§€ìˆ˜ê°€ ë‚®ì€ ëª¨ë“ˆì„ ì‹ë³„í•˜ì—¬ ì§„í™” ìš°ì„ ìˆœìœ„ ë¡œë“œë§µ ìƒì„±"""
        from gortex.utils.indexer import SynapticIndexer
        intel_map = SynapticIndexer().calculate_intelligence_index()
        
        # ì§€ëŠ¥ ì§€ìˆ˜ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ë³´ì™„ì´ í•„ìš”í•œ ëª¨ë“ˆ)
        weak_modules = sorted(intel_map.items(), key=lambda x: x[1])
        
        # Tech Radar í›„ë³´êµ° íšë“
        adoption_candidates = []
        if os.path.exists("tech_radar.json"):
            try:
                with open("tech_radar.json", "r") as f:
                    radar_data = json.load(f)
                    adoption_candidates = radar_data.get("adoption_candidates", [])
            except: pass

        roadmap = []
        for file_path, score in weak_modules[:5]: # ê°€ì¥ ì·¨ì•½í•œ 5ê°œ ëª¨ë“ˆ ëŒ€ìƒ
            # í•´ë‹¹ íŒŒì¼ì— ì ìš© ê°€ëŠ¥í•œ ì‹ ê¸°ìˆ  ì œì•ˆ ë§¤ì¹­
            suggested_tech = next((c["tech"] for c in adoption_candidates if c["target_file"] == file_path), "Refactoring Required")
            
            roadmap.append({
                "target": file_path,
                "current_maturity": score,
                "suggested_tech": suggested_tech,
                "priority": "High" if score < 10 else "Medium"
            })
            
        return roadmap
