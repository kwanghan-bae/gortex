import logging
import json
import os
import re
import math
from typing import Dict, Any, List, Optional
from datetime import datetime
from gortex.core.state import GortexState
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.vector_store import LongTermMemory

from gortex.agents.base import BaseAgent
from gortex.core.registry import AgentMetadata

logger = logging.getLogger("GortexAnalystBase")

class AnalystAgent(BaseAgent):
    """Gortex ì‹œìŠ¤í…œì˜ ë¶„ì„ ë° ì§„í™” ë‹´ë‹¹ ì—ì´ì „íŠ¸ (Base Class)"""
    def __init__(self):
        super().__init__()
        self.memory = EvolutionaryMemory()
        self.ltm = LongTermMemory()

    @property
    def metadata(self) -> AgentMetadata:
        return AgentMetadata(
            name="Analyst",
            role="Analyst",
            description="Analyzes work quality, audits architecture, and curates knowledge base.",
            tools=["scan_complexity", "audit_architecture", "optimize_knowledge"],
            version="3.0.0"
        )

    def run(self, state: GortexState) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ë£¨í‹´: í’ˆì§ˆ í‰ê°€ ë˜ëŠ” ë¦¬ì„œì¹˜ ê²°ê³¼ ìš”ì•½"""
        # [INTEGRATION] Update Skill Points on Success
        from gortex.utils.economy import get_economy_manager
        eco_manager = get_economy_manager()
        
        eco_manager.update_skill_points(
            state, 
            self.metadata.name, 
            category="Analysis", 
            quality_score=1.0, 
            difficulty=1.0
        )
        
        # (ê¸°ë³¸ êµ¬í˜„: managerë¡œ ë³µê·€í•˜ë©° ì„±ê³¼ ë¦¬í¬íŠ¸)
        return {
            "next_node": "manager", 
            "thought": "Analysis routine complete.",
            "agent_economy": state.get("agent_economy")
        }

    def calculate_efficiency_score(self, success: bool, tokens: int, latency_ms: int, energy_cost: int) -> float:
        if not success: return 0.0
        cost = (tokens * 0.01) + (latency_ms * 0.005) + (energy_cost * 2.0)
        score = 100.0 / (1.0 + math.log1p(cost / 5.0))
        return round(min(100.0, score), 1)

    def identify_tool_gap(self, failure_context: str) -> Optional[Dict[str, Any]]:
        """ì‘ì—… ì‹¤íŒ¨ ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì‹ ê·œ ë„êµ¬(Tool)ë¥¼ ì„¤ê³„í•¨."""
        prompt = f"""You are the Master ToolSmith. 
        Analyze the following failure and design a NEW Python tool (function) that would prevent this in the future.
        
        [Failure Context]:
        {failure_context}
        
        [Current Tools]: {tool_registry.list_tools()}
        
        Design a reusable tool. Return JSON ONLY:
        {{
            "tool_name": "reusable_tool_name",
            "description": "What it does",
            "parameters": {{"param1": "type", "param2": "type"}},
            "logic_blueprint": "Step-by-step logic for the function",
            "target_agent": "Which agent should receive this tool"
        }}
        """
        try:
            from gortex.core.tools.registry import tool_registry
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Tool gap analysis failed: {e}")
            return None

    def resolve_knowledge_conflict(self, conflict: Dict[str, Any], model_id: str = "gemini-2.0-flash") -> Optional[Dict[str, Any]]:
        """ë‘ ìƒ¤ë“œ ê°„ì˜ ìƒì¶©ë˜ëŠ” ì§€ì‹ì„ í•˜ë‚˜ë¡œ í†µí•©í•˜ê±°ë‚˜ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•¨."""
        rule_a = conflict["rule_a"]
        rule_b = conflict["rule_b"]
        
        logger.info(f"âš–ï¸ Resolving conflict between {rule_a['id']} and {rule_b['id']}...")
        
        # 1. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìë™ í•´ê²° ì‹œë„
        score_a = (rule_a.get("success_count", 0) + 1) * rule_a.get("reinforcement_count", 1)
        score_b = (rule_b.get("success_count", 0) + 1) * rule_b.get("reinforcement_count", 1)
        
        # ì ìˆ˜ ì°¨ì´ê°€ í¬ë©´ (ì˜ˆ: 3ë°° ì´ìƒ) ìš°ì„¸í•œ ìª½ì„ ì„ íƒ
        if score_a > score_b * 3:
            logger.info(f"âœ… Auto-resolved: {rule_a['id']} wins by performance score.")
            return rule_a
        elif score_b > score_a * 3:
            logger.info(f"âœ… Auto-resolved: {rule_b['id']} wins by performance score.")
            return rule_b

        # 2. ì ìˆ˜ê°€ ë¹„ìŠ·í•˜ë©´ LLMì„ í†µí•´ í†µí•©(Synthesis) ì‹œë„
        prompt = f"""ë‹¹ì‹ ì€ ì‹œìŠ¤í…œì˜ ì¼ê´€ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì§€ì‹ ì¡°ì •ìì…ë‹ˆë‹¤. ë‹¤ìŒ ë‘ ìƒì¶©ë˜ëŠ” ê·œì¹™ì„ ë¶„ì„í•˜ì—¬ í•˜ë‚˜ì˜ ìµœì í™”ëœ ê·œì¹™ìœ¼ë¡œ í†µí•©í•˜ì‹­ì‹œì˜¤.
        
        [Rule A (Category: {rule_a['category']})]: {rule_a['learned_instruction']}
        [Rule B (Category: {rule_b['category']})]: {rule_b['learned_instruction']}
        
        í†µí•© ì›ì¹™:
        1. ëª¨ìˆœë˜ëŠ” ë¶€ë¶„ì€ ë” í˜„ëŒ€ì ì´ê³  ì•ˆì „í•œ ê¸°ìˆ ì  ê´€ì ì„ ë”°ë¥´ì‹­ì‹œì˜¤.
        2. ë‘ ë¶„ì•¼ì˜ ë§¥ë½ì„ ëª¨ë‘ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” ë²”ìš©ì ì¸ ì§€ì¹¨ì„ ë§Œë“œì‹­ì‹œì˜¤.
        
        ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì‹­ì‹œì˜¤:
        {{ "instruction": "í†µí•©ëœ ì§€ì¹¨ ë‚´ìš©", "trigger_patterns": ["íŒ¨í„´1", "íŒ¨í„´2"], "severity": 1~5, "target_category": "ì–´ëŠ ìƒ¤ë“œë¡œ ë³´ë‚¼ì§€" }}
        """
        
        try:
            response_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            return {
                "learned_instruction": res_data["instruction"],
                "trigger_patterns": res_data["trigger_patterns"],
                "severity": res_data.get("severity", 3),
                "category": res_data.get("target_category", rule_a["category"])
            }
        except Exception as e:
            logger.error(f"Semantic conflict resolution failed: {e}")
            return rule_a if score_a >= score_b else rule_b # ìµœì•…ì˜ ê²½ìš° ì„±ê³¼ ì¢‹ì€ ìª½ ìœ ì§€

    def identify_capability_gap(self, error_log: str = "", unresolved_task: str = "") -> Optional[Dict[str, Any]]:
        """
        ì‹œìŠ¤í…œì´ ì²˜ë¦¬í•˜ì§€ ëª»í•œ ê³¼ì œë‚˜ ì—ëŸ¬ë¥¼ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ìƒˆë¡œìš´ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ ëª…ì„¸ë¥¼ ì œì•ˆí•¨.
        """
        prompt = f"""You are the Intelligence Growth Strategist. 
        Analyze the following failure/unresolved task and design a NEW specialized agent to handle it.
        
        [Failure/Task]: {error_log or unresolved_task}
        
        Design an agent that inherits from 'BaseAgent'.
        Return JSON ONLY:
        {{
            "agent_name": "UniqueNameAgent",
            "role": "Specific role title",
            "description": "What this agent does better than others",
            "required_tools": ["tool1", "tool2"],
            "version": "1.0.0",
            "logic_strategy": "How its 'run' method should behave"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Capability gap analysis failed: {e}")
            return None

    def synthesize_debug_consensus(self, error_log: str, debate_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ë””ë²„ê¹… ê°€ì„¤ê³¼ í† ë¡  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ìˆ˜ë¦¬ ê³„íšì„ í™•ì •í•¨.
        """
        prompt = f"""You are the Chief Surgeon. Synthesize the following debugging debate into one final, authoritative fix plan.
        
        [Original Error]:
        {error_log}
        
        [Debate History]:
        {json.dumps(debate_history, indent=2, ensure_ascii=False)}
        
        Analyze the pros and cons of each hypothesis and output the best combined solution.
        Return JSON ONLY:
        {{
            "diagnosis": "Final root cause identification",
            "fix_strategy": "Authoritative fix strategy",
            "action_plan": ["Step 1", "Step 2"],
            "verification_method": "How to verify the fix"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Debug consensus synthesis failed: {e}")
            return {"diagnosis": "Failed to synthesize", "fix_strategy": str(e), "action_plan": []}

    def summarize_system_trace(self, log_path: str = "logs/trace.jsonl") -> str:
        """ê±°ëŒ€í•œ ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ íƒ€ì„ë¼ì¸ê³¼ í†µì°°ì„ ìš”ì•½í•¨."""
        if not os.path.exists(log_path):
            return "No trace logs available for summarization."

        try:
            with open(log_path, "r", encoding="utf-8") as f:
                logs = [json.loads(line) for line in f][-300:] # ìµœê·¼ 300ê°œ ì´ë²¤íŠ¸ ëŒ€ìƒ
            
            # ì¤‘ìš” ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ (ì—ëŸ¬, ë…¸ë“œ ì™„ë£Œ, ë„êµ¬ ê²°ê³¼ ë“±)
            significant_events = []
            for l in logs:
                if l.get("event") in ["error", "node_complete", "tool_call"] or "âŒ" in str(l.get("payload")):
                    significant_events.append({
                        "agent": l.get("agent"),
                        "event": l.get("event"),
                        "time": l.get("timestamp"),
                        "info": str(l.get("payload"))[:200]
                    })

            prompt = f"""ë‹¤ìŒì€ Gortex ì‹œìŠ¤í…œì˜ ìµœê·¼ ì‹¤í–‰ ë¡œê·¸ ë°ì´í„°ë‹¤.
            ì´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œìŠ¤í…œì˜ 'ìµœê·¼ ì—­ì‚¬'ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ë¼.
            
            [ë¶„ì„ í•­ëª©]
            1. ì£¼ìš” ë§ˆì¼ìŠ¤í†¤: ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ í° ì‘ì—…ë“¤
            2. ìœ„ê¸° ë° í•´ê²°: ë°œìƒí–ˆë˜ ì¹˜ëª…ì  ì—ëŸ¬ì™€ ììœ¨ ìˆ˜ë¦¬ ê²°ê³¼
            3. í˜‘ì—… íŒ¨í„´: ê°€ì¥ í™œë°œí–ˆë˜ ì—ì´ì „íŠ¸ ê°„ì˜ ê´€ê³„
            4. ê°œì„  ê¶Œê³ : ë¡œê·¸ë¥¼ í†µí•´ ë³¸ ì•„í‚¤í…ì²˜ì  ì•½ì 
            
            [Raw Data]
            {json.dumps(significant_events, ensure_ascii=False)}
            """
            
            summary = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            summary_path = "logs/trace_summary.md"
            from gortex.utils.tools import write_file
            write_file(summary_path, f"# ğŸ“œ Gortex Historical Trace Summary\n\n> Generated: {datetime.now()}\n\n{summary}")
            return summary
            
        except Exception as e:
            logger.error(f"Trace summarization failed: {e}")
            return f"Error: {e}"

    def apply_consensus_result(self, debate_result: Dict[str, Any], conflicting_rules: List[Dict[str, Any]]):
        """Swarmì˜ í•©ì˜ ê²°ê³¼ë¥¼ ì§€ì‹ ë² ì´ìŠ¤ì— ì˜êµ¬ ë°˜ì˜í•¨."""
        unified = debate_result.get("unified_rule")
        if not unified:
            logger.warning("No unified rule found in consensus result. Skipping integration.")
            return

        # 1. ìƒˆë¡œìš´ ì „ì—­ ê·œì¹™ ìƒì„± (ê³„ë³´ ì—°ê²°)
        parent_ids = [r["id"] for r in conflicting_rules]
        new_rule_id = self.memory.save_rule(
            instruction=unified["instruction"],
            trigger_patterns=unified["trigger_patterns"],
            category=unified.get("category", "general"),
            severity=unified.get("severity", 3),
            context=f"Consensus achieved via Swarm Intelligence. Rationale: {debate_result.get('rationale')}"
        )
        
        # 2. ê³„ë³´(Lineage) ì •ë³´ ì¶”ê°€ ì—…ë°ì´íŠ¸ (save_rule ì´í›„ ë©”íƒ€ë°ì´í„° ë³´ê°•)
        # shardë¥¼ ì§ì ‘ ì°¾ì•„ parent_rules ì£¼ì…
        cat = unified.get("category", "general")
        for rule in self.memory.shards.get(cat, []):
            if rule["id"] == new_rule_id:
                rule["parent_rules"] = parent_ids
                rule["is_super_rule"] = True
                break
        self.memory._persist_shard(cat)

        # 3. ê¸°ì¡´ ê°ˆë“± ê·œì¹™ë“¤ ì •ë¦¬ (Soft-delete ë˜ëŠ” Flag ì²˜ë¦¬)
        # í˜„ì¬ëŠ” ë‹¨ìˆœí•˜ê²Œ ìƒˆ ê·œì¹™ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìš´ì˜ (ì¤‘ë³µ ì œê±° ë£¨í‹´ì—ì„œ ì¶”í›„ ì™„ì „ ì†Œê±°)
        logger.info(f"âœ¨ Unified rule {new_rule_id} created from parents: {parent_ids}")

    def generate_impact_map(self, symbol_name: str) -> str:
        """íŠ¹ì • ì‹¬ë³¼ ë³€ê²½ ì‹œì˜ ì˜í–¥ë ¥ ì§€ë„ë¥¼ Mermaid í˜•ì‹ìœ¼ë¡œ ìƒì„±í•¨."""
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        # ìµœì‹  ì½”ë“œ ìƒíƒœ ë°˜ì˜ì„ ìœ„í•œ ìŠ¤ìº” ê°•ì œ ì‹¤í–‰
        indexer.scan_project()
        deps = indexer.find_reverse_dependencies(symbol_name)
        
        if not deps:
            return f"graph TD\n  {symbol_name} -->|No Direct Dependents| Safe"
            
        diagram = f"graph RL\n  %% Impact map for {symbol_name}\n"
        diagram += f"  Target(({symbol_name})):::target\n"
        
        for idx, d in enumerate(deps):
            caller_label = f"{d['file']}\\n({d['caller']})"
            diagram += f"  Dep{idx}[{caller_label}] --> Target\n"
            
        diagram += "\n  classDef target fill:#f96,stroke:#333,stroke-width:4px;"
        return diagram

    def analyze_workflow_bottlenecks(self) -> List[Dict[str, Any]]:
        """ì—ì´ì „íŠ¸ ê°„ í˜‘ì—… ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ë¹„íš¨ìœ¨ì ì¸ ì›Œí¬í”Œë¡œìš° íŒ¨í„´ì„ ì‹ë³„í•¨."""
        from gortex.core.observer import GortexObserver
        observer = GortexObserver()
        matrix = observer.get_collaboration_matrix(limit=1000)
        
        bottlenecks = []
        if not matrix: return []
        
        # 1. í•‘í í˜„ìƒ ê°ì§€ (A -> B -> A ë°˜ë³µ)
        for caller, callees in matrix.items():
            for callee, count in callees.items():
                if count > 5: # ì„ê³„ì¹˜: 5íšŒ ì´ìƒ í˜¸ì¶œ
                    # ì—­ë°©í–¥ í˜¸ì¶œ í™•ì¸
                    back_count = matrix.get(callee, {}).get(caller, 0)
                    if back_count > 5:
                        bottlenecks.append({
                            "type": "ping_pong",
                            "agents": [caller, callee],
                            "severity": "High" if min(count, back_count) > 10 else "Medium",
                            "reason": f"{caller}ì™€ {callee} ê°„ì˜ ì¦ì€ í•‘í({count}:{back_count})ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                            "suggestion": f"{caller}ì˜ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨ì„ ê°•í™”í•˜ì—¬ ë‹¨ë²ˆì— í•´ê²°í•˜ë„ë¡ ê°œì„ í•˜ê±°ë‚˜, ì¤‘ê°„ ê²€ì¦ ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì‹­ì‹œì˜¤."
                        })
        
        # 2. ê³ ë¶€í•˜ ë…¸ë“œ ê°ì§€ (In-degreeê°€ ë„ˆë¬´ ë†’ì€ ê²½ìš°)
        node_load = {}
        for caller, callees in matrix.items():
            for callee, count in callees.items():
                node_load[callee] = node_load.get(callee, 0) + count
                
        for node, load in node_load.items():
            if load > 50: # ê³¼ë¶€í•˜ ì„ê³„ì¹˜
                bottlenecks.append({
                    "type": "hotspot",
                    "agent": node,
                    "severity": "Medium",
                    "reason": f"'{node}' ë…¸ë“œê°€ ì‹œìŠ¤í…œ ë¶€í•˜ì˜ ì¤‘ì‹¬({load} calls)ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                    "suggestion": f"'{node}'ì˜ ì—­í• ì„ ì—¬ëŸ¬ ì „ë¬¸ê°€ë¡œ ë¶„ë¦¬(Role Splitting)í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ ë„í•˜ì‹­ì‹œì˜¤."
                })
                
        return bottlenecks

    def audit_external_plugin(self, plugin_code: str, plugin_name: str) -> Dict[str, Any]:
        """ì™¸ë¶€ì—ì„œ ê°€ì ¸ì˜¨ í”ŒëŸ¬ê·¸ì¸ ì½”ë“œë¥¼ ë³´ì•ˆ ê´€ì ì—ì„œ ì •ë°€ ê²€ìˆ˜í•¨."""
        logger.info(f"ğŸ›¡ï¸ Auditing external plugin: {plugin_name}...")
        
        # 1. ì •ì  íŒ¨í„´ ìŠ¤ìº” (ê¸°ë³¸ ë„êµ¬ í™œìš©)
        from gortex.utils.tools import scan_security_risks
        static_risks = scan_security_risks(plugin_code)
        
        # 2. LLM ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„
        prompt = f"""You are the Chief Security Officer. 
        Perform a deep security audit on the following external AI Agent code.
        Look for malicious intent, hidden backdoors, unauthorized data exfiltration, or system-destructive logic.
        
        [Plugin Name]: {plugin_name}
        [Code]:
        {plugin_code[:4000]}
        
        Return JSON ONLY:
        {{
            "is_safe": true/false,
            "risk_level": "Low/Medium/High/Critical",
            "findings": ["finding 1", "finding 2"],
            "recommendation": "Approve / Reject / Sandbox"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            audit_res = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            # ì •ì  ë¶„ì„ ê²°ê³¼ í†µí•©
            if static_risks:
                audit_res["static_findings"] = static_risks
                if any(r["type"] == "Hardcoded Secret" for r in static_risks):
                    audit_res["is_safe"] = False
                    audit_res["risk_level"] = "High"
            
            return audit_res
        except Exception as e:
            logger.error(f"Security audit failed: {e}")
            return {"is_safe": False, "risk_level": "Critical", "recommendation": "Reject due to audit failure"}

    def analyze_and_optimize_persona(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """ì—ì´ì „íŠ¸ì˜ ì‘ì—… ì´ë ¥ì„ ë¶„ì„í•˜ì—¬ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨(System Prompt)ì„ ìµœì í™”í•¨."""
        # 1. ìµœê·¼ ì„±ê³¼ ë°ì´í„° ìˆ˜ì§‘
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        monitor = EfficiencyMonitor()
        summary = monitor.get_summary(days=7)
        
        # 2. í˜„ì¬ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨ íšë“
        from gortex.utils.prompt_loader import loader
        current_instruction = loader.get_prompt(agent_name.lower())
        
        prompt = f"""You are the Neural Architect. 
        Optimize the following System Instruction for the '{agent_name}' agent.
        Analyze its recent performance and mutate the instruction to be more effective.
        
        [Current Instruction]:
        {current_instruction}
        
        [Recent Performance Metrics]:
        {json.dumps(summary.get(agent_name, {}), indent=2)}
        
        Goals:
        1. Keep the core identity but refine the technical guidance.
        2. Strengthen points that led to success, fix points that led to failure.
        
        Return JSON ONLY:
        {{
            "new_instruction": "Full optimized instruction text",
            "changes": "Summary of what was changed and why",
            "version": "X.Y.Z (bump minor)"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Persona optimization failed for {agent_name}: {e}")
            return None

    def scan_system_infection(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì½”ë“œë² ì´ìŠ¤ì˜ ë¬´ê²°ì„±ì„ ê²€ì‚¬í•˜ì—¬ ë¹„ì •ìƒì ì¸ ì˜¤ì—¼(Infection)ì„ íƒì§€í•¨."""
        from gortex.utils.integrity import guard
        modified, deleted = guard.check_integrity()
        
        if not modified and not deleted:
            return {"status": "healthy", "infections": []}
            
        infections = []
        # (ì‹¤ì œ êµ¬í˜„ ì‹œ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ 'ìŠ¹ì¸ëœ ë¯¸ì…˜'ì˜ íƒ€ê²Ÿ íŒŒì¼ ëª©ë¡ê³¼ ëŒ€ì¡°í•˜ì—¬ ì˜¤íƒ ë°©ì§€)
        for path in modified:
            infections.append({"path": path, "type": "modified", "severity": "High"})
        for path in deleted:
            infections.append({"path": path, "type": "deleted", "severity": "Critical"})
            
        logger.warning(f"ğŸš¨ [ImmuneSystem] Infection detected in {len(infections)} files!")
        return {"status": "infected", "infections": infections}

    def generate_strategic_roadmap(self) -> str:
        """í…Œí¬ ë ˆì´ë” ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¤‘ì¥ê¸° ê¸°ìˆ ì  ì§„í™” ë¡œë“œë§µì„ ìƒì„±í•¨."""
        from gortex.utils.tech_radar import radar
        advice = radar.get_strategic_advice()
        
        prompt = f"""You are the Chief Technology Officer. 
        Based on the current Tech Radar data, design a STRATEGIC ROADMAP for the next 10 Gortex sessions.
        Focus on phasing out 'hold' status tech and accelerating 'assess/trial' tech.
        
        [Tech Radar Info]:
        {json.dumps(radar.technologies, indent=2)}
        
        Return a high-level roadmap in Markdown format.
        """
        try:
            roadmap = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            return f"{advice}\n\n{roadmap}"
        except Exception as e:
            logger.error(f"Strategic roadmap failed: {e}")
            return "Failed to generate roadmap."

    def validate_alignment_with_constitution(self, proposed_action: str) -> Dict[str, Any]:
        """ì œì•ˆëœ í–‰ë™ì´ Gortex í—Œì¥(CONSTITUTION.md)ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ ê²€ì¦í•¨."""
        constitution = read_file("docs/CONSTITUTION.md")
        
        prompt = f"""You are the Neural Ethicist. 
        Verify if the following proposed action aligns with the Gortex Neural Constitution.
        
        [Constitution]:
        {constitution}
        
        [Proposed Action]:
        {proposed_action}
        
        Check for any violations of Integrity, Sovereignty, Responsibility, or Efficiency.
        Return JSON ONLY:
        {{
            "is_aligned": true/false,
            "violations": ["violation 1", "violation 2"],
            "severity": "Low/Medium/High/Critical",
            "corrective_action": "How to fix the plan to align with the constitution"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Alignment check failed: {e}")
            return {"is_aligned": True, "severity": "Low", "violations": []} # Fallback to true to avoid deadlock, but log error

    def detect_agent_fusion_opportunities(self) -> List[Dict[str, Any]]:
        """ì—ì´ì „íŠ¸ ê°„ì˜ ê°•í•œ ê²°í•©ë„ë¥¼ ë¶„ì„í•˜ì—¬ ìœµí•©(Fusion) ê°€ëŠ¥ì„±ì„ ì‹ë³„í•¨."""
        from gortex.core.observer import GortexObserver
        matrix = GortexObserver().get_collaboration_matrix(limit=1000)
        
        fusions = []
        if not matrix: return []
        
        # í˜¸ì¶œ ë¹ˆë„ê°€ ë§¤ìš° ë†’ì€ ìŒ ì°¾ê¸° (ì˜ˆ: A -> B í˜¸ì¶œì´ ì „ì²´ì˜ 40% ì´ìƒ)
        for caller, callees in matrix.items():
            total_calls = sum(callees.values())
            for callee, count in callees.items():
                if count / total_calls > 0.4 and count > 10:
                    fusions.append({
                        "type": "agent_fusion",
                        "pair": [caller, callee],
                        "strength": round(count / total_calls, 2),
                        "reason": f"'{caller}'ì™€ '{callee}'ê°€ ë§¤ìš° ê°•í•˜ê²Œ ê²°í•©ë˜ì–´ ì‘ì—… ì¤‘ì…ë‹ˆë‹¤. (ê²°í•©ë„: {int(count/total_calls*100)}%)",
                        "suggestion": f"ë‘ ì—ì´ì „íŠ¸ë¥¼ '{caller}_{callee}_Fused'ë¡œ ë³‘í•©í•˜ì—¬ ì¤‘ê°„ í•¸ë“œì˜¤í”„ ë¹„ìš©ì„ ì œê±°í•˜ì‹­ì‹œì˜¤."
                    })
        return fusions

    def predict_runtime_errors(self, code: str, file_path: str) -> Dict[str, Any]:
        """ì½”ë“œ ë³€ê²½ë¶„ì„ ë¶„ì„í•˜ì—¬ ì ì¬ì  ëŸ°íƒ€ì„ ì¥ì•  ë°œìƒ í™•ë¥ ì„ ì˜ˆì¸¡í•¨."""
        # 1. ê³¼ê±° ì¥ì•  íŒ¨í„´ ì†Œí™˜
        from gortex.utils.log_vectorizer import SemanticLogSearch
        past_failures = SemanticLogSearch().search_similar_cases(f"Error in {file_path}", limit=10)
        
        prompt = f"""You are the Oracle Architect. 
        Analyze the following code for potential runtime failures (e.g., unhandled exceptions, race conditions, edge cases).
        Cross-reference with the historical failures provided.
        
        [Target File]: {file_path}
        [New Code]:
        {code[:3000]}
        
        [Historical Failure Patterns]:
        {json.dumps(past_failures, ensure_ascii=False)}
        
        Return JSON ONLY:
        {{
            "risk_probability": 0.0 ~ 1.0,
            "predicted_error_type": "ZeroDivisionError/KeyError/etc",
            "reason": "Detailed justification",
            "preemptive_fix": "Specific instruction to fix before it crashes"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Error prediction failed: {e}")
            return {"risk_probability": 0.0}

    def identify_dormant_assets(self) -> Dict[str, List[str]]:
        """ì‹œìŠ¤í…œ ë‚´ì˜ ë„íƒœ ëŒ€ìƒ(Dormant/Underperforming) ìì‚°ì„ ì‹ë³„í•¨."""
        from gortex.core.registry import registry
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        monitor = EfficiencyMonitor()
        summary = monitor.get_summary(days=30)
        
        dormant_agents = []
        # 1. ì €ì„±ê³¼ ì—ì´ì „íŠ¸ ì‹ë³„
        for agent_name in registry.list_agents():
            if agent_name.lower() in ["manager", "analyst", "planner", "coder"]: continue
            
            stats = summary.get(agent_name, {})
            calls = stats.get("calls", 0)
            success_rate = stats.get("success_rate", 100)
            
            # ì¡°ê±´: 10íšŒ ì´ìƒ í˜¸ì¶œë˜ì—ˆìœ¼ë‚˜ ì„±ê³µë¥ ì´ 30% ë¯¸ë§Œì¸ ê²½ìš°
            if calls >= 10 and success_rate < 30:
                dormant_agents.append(agent_name)
                logger.info(f"ğŸ¥€ Agent '{agent_name}' identified for offboarding (Success Rate: {success_rate:.1f}%)")

        # 2. ìœµí•©(Fusion)ì— ì˜í•´ ëŒ€ì²´ëœ ì›ë³¸ ì—ì´ì „íŠ¸ ì‹ë³„
        # (ì‹¤ì œ êµ¬í˜„ ì‹œ Super Rulesì˜ 'Neural Fusion established' ê¸°ë¡ ëŒ€ì¡°)
        
        return {"agents": dormant_agents}

    def analyze_infrastructure_scaling(self, state: GortexState) -> Dict[str, Any]:
        """ê²½ì œì  ìƒíƒœì™€ ë¶€í•˜ë¥¼ ë¶„ì„í•˜ì—¬ ì¸í”„ë¼ í™•ì¥ ì—¬ë¶€ë¥¼ ê²°ì •í•¨."""
        from gortex.utils.infra import infra
        load = infra.check_cluster_load()
        
        # ì „ì²´ ì˜ˆì‚° í•©ì‚°
        total_credits = sum(a.get("credits", 0) for a in state.get("agent_economy", {}).values())
        
        should_scale = False
        reason = ""
        
        # ì¡°ê±´: í‰ê·  CPUê°€ 70% ì´ìƒì´ê³ , ì´ ì”ê³ ê°€ $100 ì´ìƒì¼ ë•Œ
        if load["avg_cpu"] > 70 and total_credits > 100.0:
            should_scale = True
            reason = f"High cluster load ({load['avg_cpu']:.1f}%) with healthy budget (${total_credits:.2f})"
        elif load["count"] == 0:
            should_scale = True
            reason = "No remote workers active. Establishing baseline capacity."
            
        return {
            "should_scale": should_scale,
            "reason": reason,
            "current_load": load,
            "total_credits": total_credits
        }

    def evaluate_artifact_value(self, directory: str = "logs") -> List[Dict[str, Any]]:
        """ì‘ì—… ë¶€ì‚°ë¬¼ë“¤ì˜ ê°€ì¹˜ë¥¼ í‰ê°€í•˜ì—¬ ì‚­ì œ í›„ë³´ ëª©ë¡ì„ ìƒì„±í•¨."""
        cleanup_candidates = []
        now = datetime.now()
        
        # ì²­ì†Œ ëŒ€ìƒ í´ë” ì •ì˜
        target_dirs = [
            os.path.join(directory, "backups"),
            os.path.join(directory, "versions"),
            "training_jobs" # ì˜¤ë˜ëœ í•™ìŠµ ì¡ í¬í•¨
        ]
        
        for d in target_dirs:
            if not os.path.exists(d): continue
            
            for f in os.listdir(d):
                path = os.path.join(d, f)
                if os.path.isdir(path): continue
                
                mtime = datetime.fromtimestamp(os.path.getmtime(path))
                age_days = (now - mtime).days
                size_kb = os.path.getsize(path) / 1024
                
                # ê°€ì¹˜ í‰ê°€ ë¡œì§: 7ì¼ ì´ìƒ ëœ ë°±ì—…ì€ ë‚®ì€ ê°€ì¹˜
                value_score = 100
                if age_days > 7: value_score -= 50
                if age_days > 30: value_score -= 40
                
                # íŠ¹ì • í™•ì¥ì(ë°±ì—…) ê°€ì¤‘ì¹˜
                if f.endswith(".bak"): value_score -= 10
                
                if value_score < 50:
                    cleanup_candidates.append({
                        "path": path,
                        "age_days": age_days,
                        "size_kb": round(size_kb, 1),
                        "reason": "Old backup/artifact" if age_days > 7 else "Low priority"
                    })
                    
        return sorted(cleanup_candidates, key=lambda x: x["age_days"], reverse=True)

    def perform_autonomous_cleanup(self) -> Dict[str, Any]:
        """ë¶€ì‚°ë¬¼ ê°€ì¹˜ í‰ê°€ ë° ììœ¨ ì‚­ì œ í†µí•© ìˆ˜í–‰"""
        candidates = self.evaluate_artifact_value()
        if not candidates:
            return {"status": "skipped", "message": "No cleanup candidates found."}
            
        target_paths = [c["path"] for c in candidates]
        total_size_kb = sum(c["size_kb"] for c in candidates)
        
        from gortex.utils.tools import safe_bulk_delete
        result = safe_bulk_delete(target_paths)
        
        freed_count = len(result["success"])
        return {
            "status": "success",
            "deleted_count": freed_count,
            "freed_kb": round(total_size_kb, 1) if freed_count > 0 else 0,
            "message": f"ğŸ§¹ Autonomous cleanup finished. {freed_count} files removed, {round(total_size_kb, 1)} KB freed."
        }

    def generate_milestone_report(self, start_session: int = 1, end_session: int = 100) -> str:
        """ì§€ì •ëœ ë²”ìœ„ì˜ ì„¸ì…˜ë“¤ì„ ë¶„ì„í•˜ì—¬ ë§ˆì¼ìŠ¤í†¤ ë³´ê³ ì„œë¥¼ ìƒì„±í•¨."""
        session_dir = "docs/sessions"
        if not os.path.exists(session_dir):
            return "Session directory not found."

        summary_parts = []
        for i in range(start_session, end_session + 1):
            path = os.path.join(session_dir, f"session_{i:04d}.md")
            if os.path.exists(path):
                from gortex.utils.tools import read_file
                content = read_file(path)
                # ê° ì„¸ì…˜ì˜ ëª©í‘œì™€ ê²°ê³¼ë§Œ ì¶”ì¶œ (ë‹¨ìˆœí™”)
                goal_match = re.search(r"## ğŸ¯ Goal(.*?)(?=\n##|$)", content, re.DOTALL)
                outcome_match = re.search(r"## ğŸ“ˆ Outcomes(.*?)(?=\n##|$)", content, re.DOTALL)
                
                if goal_match:
                    summary_parts.append(f"S{i:03d}: {goal_match.group(1).strip()}")

        combined_summary = "\n".join(summary_parts)
        
        prompt = f"""ë‹¤ìŒì€ Gortex ì‹œìŠ¤í…œì˜ {start_session}íšŒë¶€í„° {end_session}íšŒê¹Œì§€ì˜ ê°œë°œ ê¸°ë¡ì´ë‹¤.
        ì´ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ Gortexê°€ ì–´ë–»ê²Œ ì§„í™”í•´ì™”ëŠ”ì§€ 5ê°€ì§€ í•µì‹¬ í…Œë§ˆë¡œ ìš”ì•½í•˜ê³ , 
        ë¯¸ë˜ë¥¼ ìœ„í•œ ì œì–¸ì„ í¬í•¨í•œ '100ì„¸ì…˜ ê¸°ë… ë§ˆì¼ìŠ¤í†¤ ë³´ê³ ì„œ'ë¥¼ ì‘ì„±í•˜ë¼.
        
        [Session Logs]:
        {combined_summary}
        
        ë‹µë³€ì€ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë¼.
        """
        
        try:
            report = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}])
            output_path = "docs/MILESTONE_100.md"
            from gortex.utils.tools import write_file
            write_file(output_path, f"# ğŸ† Gortex 100-Session Milestone Report\n\n> {datetime.now()}\n\n{report}")
            return f"âœ… Milestone report generated: {output_path}"
        except Exception as e:
            logger.error(f"Milestone report generation failed: {e}")
            return f"âŒ Failed: {e}"

    def archive_system_logs(self) -> Dict[str, Any]:
        """ëˆ„ì ëœ ë¡œê·¸ íŒŒì¼ì„ ì•„ì¹´ì´ë¹™í•˜ê³  ì§€ì‹ íŒŒì¼ì„ ë°±ì—…í•¨."""
        from gortex.utils.tools import compress_directory, backup_file_with_rotation
        
        # 1. í•µì‹¬ ì§€ì‹ íŒŒì¼ ë°±ì—…
        bk_res = backup_file_with_rotation("experience.json", max_versions=10)
        
        # 2. ì˜¤ë˜ëœ ë¡œê·¸ ì•„ì¹´ì´ë¹™
        log_dir = "logs"
        archive_dir = "logs/archives"
        os.makedirs(archive_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d")
        zip_path = os.path.join(archive_dir, f"logs_backup_{timestamp}.zip")
        
        # logs/ ë‚´ë¶€ì˜ ê°œë³„ .jsonl íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ì••ì¶• (ì´ë¯¸ ì••ì¶•ëœ archives ì œì™¸)
        files_to_archive = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.endswith(".jsonl")]
        
        if not files_to_archive:
            return {"status": "skipped", "backup": bk_res, "reason": "No logs to archive."}
            
        # ì„ì‹œ í´ë”ë¡œ ë³µì‚¬ í›„ ì••ì¶• (ì›ë³¸ ë³´í˜¸)
        temp_archive_root = "logs/temp_archive"
        os.makedirs(temp_archive_root, exist_ok=True)
        for f in files_to_archive:
            shutil.copy2(f, temp_archive_root)
            
        comp_res = compress_directory(temp_archive_root, zip_path)
        shutil.rmtree(temp_archive_root)
        
        # ì•„ì¹´ì´ë¹™ ì„±ê³µ ì‹œ ì›ë³¸ ë¡œê·¸ ì‚­ì œ (ì •ì±…ì— ë”°ë¼ ì„ íƒì )
        # ì—¬ê¸°ì„œëŠ” ì•ˆì „ì„ ìœ„í•´ ì‚­ì œ ëŒ€ì‹  .old í™•ì¥ìë¥¼ ë¶™ì´ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ .
        # ì¼ë‹¨ ì•„ì¹´ì´ë¹™ ì„±ê³µ ë©”ì‹œì§€ë§Œ ë°˜í™˜
        
        return {
            "status": "success",
            "backup": bk_res,
            "archive": zip_path,
            "message": f"System maintenance complete. 10 knowledge versions kept. Logs archived to {zip_path}"
        }

    def propose_proactive_refactoring(self) -> List[Dict[str, Any]]:
        """ë³µì¡ë„ê°€ ë†’ì€ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì„ ì œì  ë¦¬íŒ©í† ë§ ê³„íšì„ ì œì•ˆí•¨."""
        # 1. ê³ ë³µì¡ë„ íŒŒì¼ ì‹ë³„
        complex_files = self.scan_project_complexity()
        if not complex_files:
            return []
            
        proposals = []
        for item in complex_files[:2]: # ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ìƒìœ„ 2ê°œë§Œ ì²˜ë¦¬
            file_path = item["file"]
            content = read_file(file_path)
            
            prompt = f"""You are the Guardian Architect. 
            Analyze the following complex code and propose a PROACTIVE refactoring to improve maintainability and prevent future bugs.
            
            [File]: {file_path}
            [Complexity Score]: {item['score']}
            [Issue]: {item['issue']}
            [Code]:
            {content[:3000]}
            
            Return JSON ONLY:
            {{
                "target_file": "{file_path}",
                "reason": "Specific technical justification",
                "action_plan": ["Step 1: ...", "Step 2: ..."],
                "risk_level": "Low/Medium/High",
                "expected_gain": "e.g., Reduced cyclomatic complexity"
            }}
            """
            try:
                response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
                
                proposals.append(res_data)
                logger.info(f"ğŸ›¡ï¸ Proactive refactoring proposed for: {file_path}")
            except Exception as e:
                logger.error(f"Failed to generate proactive refactoring for {file_path}: {e}")
                
        return proposals

    def scan_project_complexity(self, directory: str = ".") -> List[Dict[str, Any]]:
        debt_list = []
        ignore_dirs = {'.git', 'venv', '__pycache__', 'logs', 'site-packages'}
        for root, dirs, files in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            for f in files:
                if f.endswith(".py"):
                    path = os.path.join(root, f)
                    try:
                        with open(path, 'r', encoding='utf-8') as file:
                            content = file.read()
                            lines = content.splitlines()
                            score = len(re.findall(r"\b(if|elif|for|while|except|def|class|with|async)\b", content))
                            score += len(lines) // 20
                            if score > 10:
                                debt_list.append({
                                    "file": path, "score": score, 
                                    "reason": "High logical density" if score > 30 else "Moderate complexity",
                                    "issue": "íŒŒì¼ì˜ ë…¼ë¦¬ì  ë°€ë„ê°€ ë„ˆë¬´ ë†’ì•„ ê°€ë…ì„±ì´ ì €í•˜ë¨",
                                    "refactor_strategy": "ê¸´ ë©”ì„œë“œë¥¼ ë¶„ë¦¬í•˜ê³  ê´€ì‹¬ì‚¬ë¥¼ ëª¨ë“ˆë¡œ ê²©ë¦¬í•˜ë¼"
                                })
                    except: pass
        return sorted(debt_list, key=lambda x: x["score"], reverse=True)

    def analyze_data(self, file_path: str) -> Dict[str, Any]:
        try:
            import pandas as pd
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
                return {"status": "success", "summary": df.describe().to_dict(), "file": file_path}
        except: pass
        return {"status": "failed", "reason": "Data analysis failed"}

    def identify_missing_tests(self) -> List[Dict[str, Any]]:
        try:
            import subprocess
            subprocess.run(["python3", "-m", "coverage", "json", "-o", "logs/coverage.json"], capture_output=True)
            if os.path.exists("logs/coverage.json"):
                with open("logs/coverage.json", "r") as f:
                    data = json.load(f)
                results = []
                for file_path, info in data.get("files", {}).items():
                    p = info.get("summary", {}).get("percent_covered", 100)
                    if p < 80:
                        results.append({"file": file_path, "coverage": round(p, 1), "missing_lines": info.get("missing_lines", [])})
                return sorted(results, key=lambda x: x["coverage"])
        except: pass
        return []

    def audit_architecture(self) -> List[Dict[str, Any]]:
        from gortex.utils.indexer import SynapticIndexer
        deps = SynapticIndexer().generate_dependency_graph()
        violations = []
        layers = {"utils": 0, "core": 1, "ui": 2, "agents": 3, "tests": 4}
        for dep in deps:
            s, t = dep["source"], dep["target"]
            sl = next((l for l in layers if f"gortex.{l}" in s or s.startswith(l)), None)
            tl = next((l for l in layers if f"gortex.{l}" in t or t.startswith(l)), None)
            if sl and tl and layers[sl] < layers[tl]:
                violations.append({"type": "Layer Violation", "source": s, "target": t, "reason": f"í•˜ìœ„ ë ˆì´ì–´ '{sl}'ê°€ ìƒìœ„ ë ˆì´ì–´ '{tl}'ë¥¼ ì°¸ì¡°í•¨"})
        return violations

    def generate_dependency_graph_with_weights(self) -> Dict[str, Any]:
        """
        í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“ˆ ì˜ì¡´ì„± ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°€ì¤‘ì¹˜(ì—°ê²° ìˆ˜)ì™€ ë…¸ë“œ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ì‹œê°í™”ì— ì í•©í•œ í˜•íƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        from gortex.utils.indexer import SynapticIndexer
        raw_deps = SynapticIndexer().generate_dependency_graph()
        
        nodes = {}
        edges = []
        
        # 1. ë…¸ë“œ ë° ì—£ì§€ ê°€ì¤‘ì¹˜ ê³„ì‚°
        for dep in raw_deps:
            s, t = dep["source"], dep["target"]
            
            # ë…¸ë“œ ë“±ë¡ (ì—†ìœ¼ë©´ ì´ˆê¸°í™”)
            if s not in nodes: nodes[s] = {"id": s, "value": 0, "connections": 0}
            if t not in nodes: nodes[t] = {"id": t, "value": 0, "connections": 0}
            
            # ì—°ê²° ìˆ˜ ì¦ê°€ (ì¤‘ìš”ë„)
            nodes[s]["value"] += 1
            nodes[t]["connections"] += 1
            
            # ì—£ì§€ ì¶”ê°€
            edges.append({"from": s, "to": t, "weight": 1})
            
        return {"nodes": list(nodes.values()), "edges": edges}

    def synthesize_global_rules(self, model_id: str = "gemini-1.5-pro") -> str:
        rules = self.memory.memory
        if not rules: return "ì •ë¦¬í•  ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤."
        ctx = "".join([f"- [{r['severity']}] {r['learned_instruction']}\n" for r in rules])
        try:
            summary = self.backend.generate(model_id, [{"role": "user", "content": f"ë‹¤ìŒ ê·œì¹™ì„ 5ê°€ì§€ ì›ì¹™ìœ¼ë¡œ ìš”ì•½í•˜ë¼:\n{ctx}"}])
            rules_md_path = "docs/RULES.md"
            original = ""
            if os.path.exists(rules_md_path):
                with open(rules_md_path, 'r', encoding='utf-8') as f: original = f.read()
            section = "## ğŸ¤– Auto-Evolved Coding Standards"
            new_c = f"{original.split(section)[0]}{section}\n\n> ê°±ì‹ : {datetime.now()}\n\n{summary}" if section in original else f"{original}\n\n{section}\n\n{summary}"
            with open(rules_md_path, 'w', encoding='utf-8') as f: f.write(new_c)
            return "âœ… ì „ì—­ ê·œì¹™ ì¢…í•© ì™„ë£Œ."
        except: return "âŒ ì‹¤íŒ¨"

    def predict_architectural_bottleneck(self) -> Dict[str, Any]:
        """ê³¼ê±° ê±´ê°•ë„ ì ìˆ˜ ì´ë ¥ì„ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ ë³‘ëª© ì§€ì  ì˜ˆì¸¡"""
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” logs/trace.jsonl ë˜ëŠ” ë³„ë„ í†µê³„ íŒŒì¼ ì°¸ì¡°)
        # í˜„ì¬ëŠ” ë‹¨ìˆœ ì„ í˜• íšŒê·€ ì¶”ì • ë°©ì‹ì˜ ë¡œì§ êµ¬ì¡° ë§ˆë ¨
        from gortex.utils.indexer import SynapticIndexer
        current_health = SynapticIndexer().calculate_health_score()
        
        # ê°€ìƒì˜ íˆìŠ¤í† ë¦¬ ë¶„ì„ (ì¶”í›„ ì‹¤ì œ ë°ì´í„° ì—°ë™)
        score = current_health["score"]
        trend = "Stable"
        if score < 60: trend = "Declining"
        elif score > 80: trend = "Improving"
        
        prediction = {
            "current_score": score,
            "projected_score_3_sessions": round(score * 0.95, 1) if trend == "Declining" else score,
            "risk_level": "High" if score < 50 else "Medium" if score < 70 else "Low",
            "bottleneck_candidates": ["Dependency Bloat", "Missing Unit Tests"] if score < 70 else []
        }
        return prediction

    def reinforce_successful_personas(self):
        """ê°€ìƒ í˜ë¥´ì†Œë‚˜ì˜ ì„±ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìš°ìˆ˜ ì§€ì¹¨ì„ ì •ì‹ í˜ë¥´ì†Œë‚˜ì— í†µí•©"""
        from gortex.utils.efficiency_monitor import EfficiencyMonitor
        perf = EfficiencyMonitor().get_persona_performance()
        
        p_path = "docs/i18n/personas.json"
        if not os.path.exists(p_path): return
        
        with open(p_path, 'r', encoding='utf-8') as f:
            personas = json.load(f)
            
        updated = False
        for p_name, stats in perf.items():
            # ì„±ê³µë¥  90% ì´ìƒì¸ ê²½ìš° ê°•í™” ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤
            if stats["rate"] >= 90.0 and p_name not in personas:
                logger.info(f"ğŸŒŸ High performing virtual persona detected: {p_name}")
                # (ë‹¨ìˆœí™”: ì‹¤ì œ êµ¬í˜„ ì‹œ LLMì´ ì§€ì¹¨ì„ ì •ì œí•˜ì—¬ ë³‘í•©)
                personas[p_name] = {
                    "name": p_name,
                    "description": "Successfully evolved from virtual persona",
                    "traits": ["proven", "reliable"],
                    "focus": ["general"]
                }
                updated = True
        
        if updated:
            with open(p_path, 'w', encoding='utf-8') as f:
                json.dump(personas, f, indent=2, ensure_ascii=False)
            logger.info("âœ… Official personas reinforced with successful evolution.")

    def generate_release_note(self, model_id: str = "gemini-1.5-pro") -> str:
        try:
            import subprocess
            git_log = subprocess.run(["git", "log", "-n", "10", "--pretty=format:%s"], capture_output=True, text=True).stdout
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            evo = "\n".join([f"- {h['metadata'].get('tech')} applied to {h['metadata'].get('file')}" for h in EfficiencyMonitor().get_evolution_history(limit=5)])
            prompt = f"ë‹¤ìŒ ë¡œê·¸ë¡œ ë¦´ë¦¬ì¦ˆ ë…¸íŠ¸ë¥¼ ì‘ì„±í•˜ë¼:\n\n[Git]\n{git_log}\n\n[Evo]\n{evo}"
            summary = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            with open("docs/release_note.md", "w", encoding="utf-8") as f:
                f.write(f"# ğŸš€ Gortex Release Note\n\n> Generated at: {datetime.now()}\n\n{summary}")
            return "âœ… release_note.md ê°±ì‹  ì™„ë£Œ."
        except: return "âŒ ì‹¤íŒ¨"

    def bump_version(self) -> str:
        v_path = "VERSION"
        try:
            cur_v = "1.0.0"
            if os.path.exists(v_path):
                with open(v_path, "r") as f: cur_v = f.read().strip()
            parts = [int(p) for p in cur_v.split(".")] if "." in cur_v else [1, 0, 0]
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            if len(EfficiencyMonitor().get_evolution_history(limit=5)) >= 5:
                parts[1] += 1
                parts[2] = 0
            else:
                parts[2] += 1
            new_v = ".".join(map(str, parts))
            with open(v_path, "w") as f: f.write(new_v)
            return new_v
        except: return "Error"

    def evolve_personas(self, model_id: str = "gemini-1.5-pro") -> str:
        """ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨(personas.json)ì„ ìë™ íŠœë‹"""
        try:
            from gortex.utils.efficiency_monitor import EfficiencyMonitor
            summary = EfficiencyMonitor().get_summary(days=14)
            
            # í˜„ì¬ í˜ë¥´ì†Œë‚˜ ë¡œë“œ
            p_path = "docs/i18n/personas.json"
            with open(p_path, 'r', encoding='utf-8') as f:
                personas = json.load(f)

            prompt = f"""ë‹¤ìŒ ì—ì´ì „íŠ¸ ì„±ê³¼ ìš”ì•½ê³¼ í˜„ì¬ í˜ë¥´ì†Œë‚˜ ì •ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 
            ì„±ëŠ¥ì´ ë‚®ì€ ì—ì´ì „íŠ¸ì˜ ì„±ê²©ì„ ë” ì „ë¬¸í™”í•˜ê±°ë‚˜ ì„±ê³µì ì¸ íŒ¨í„´ì„ ë°˜ì˜í•˜ì—¬ ì§€ì¹¨ì„ ê°•í™”í•˜ë¼.
            
            [ì„±ëŠ¥ ìš”ì•½]
            {json.dumps(summary, indent=2)}
            
            [í˜„ì¬ í˜ë¥´ì†Œë‚˜]
            {json.dumps(personas, indent=2, ensure_ascii=False)}
            
            ì—…ë°ì´íŠ¸ëœ ì „ì²´ personas.json ë‚´ìš©ì„ ë°˜í™˜í•˜ë¼. ì˜¤ì§ JSONë§Œ ì¶œë ¥í•˜ë¼.
            """
            
            new_json_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            # JSON ì¶”ì¶œ ë¡œì§ (ì •ê·œì‹ ìƒëµ - LLMì´ ì •êµí•˜ê²Œ ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ë‚˜ ì¶”í›„ ë³´ê°• ê°€ëŠ¥)
            
            with open(p_path, 'w', encoding='utf-8') as f:
                f.write(new_json_text)
                
            return "âœ… í˜ë¥´ì†Œë‚˜ ìê°€ ì§„í™” ì™„ë£Œ."
        except Exception as e:
            logger.error(f"Persona evolution failed: {e}")
            return f"âŒ ì‹¤íŒ¨: {e}"

    def curate_evolution_data(self, output_path: str = "logs/datasets/evolution.jsonl") -> str:
        """
        ì„±ê³µì ì¸ ìê°€ ì§„í™” ì‚¬ë¡€(Experience Rules)ë¥¼ ì„ ë³„í•˜ì—¬ 
        LLM Fine-tuningì„ ìœ„í•œ JSONL í¬ë§·ìœ¼ë¡œ íë ˆì´ì…˜í•©ë‹ˆë‹¤.
        """
        memories = self.memory.memory
        if not memories:
            return "No evolutionary data found."
            
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        curated_count = 0
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                for mem in memories:
                    # ë°ì´í„° í’ˆì§ˆ í•„í„°ë§ (ì‹¬ê°ë„ê°€ ë†’ê±°ë‚˜ ëª…í™•í•œ êµì • ì§€ì‹œê°€ ìˆëŠ” ê²½ìš°)
                    if not mem.get("learned_instruction") or not mem.get("trigger_context"):
                        continue
                        
                    # Fine-tuning Format (Chat-style)
                    entry = {
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are Gortex, an evolving AI agent. Analyze the failure and provide a corrected rule."
                            },
                            {
                                "role": "user", 
                                "content": f"Context/Failure:\n{mem['trigger_context']}\n\nFailed Attempt:\n{mem.get('failed_solution', 'N/A')}"
                            },
                            {
                                "role": "assistant",
                                "content": f"Evolutionary Rule:\n{mem['learned_instruction']}"
                            }
                        ]
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    curated_count += 1
                    
            return f"âœ… Curated {curated_count} items to {output_path}"
        except Exception as e:
            logger.error(f"Failed to curate evolution data: {e}")
            return f"âŒ Failed: {e}"

    def optimize_knowledge_base(self, model_id: str = "gemini-2.0-flash") -> Dict[str, Any]:
        """
        ì§€ì‹ ë² ì´ìŠ¤(Experience Rules)ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ìµœì í™”í•¨.
        ì„±ê³µë¥ ì´ ë‚®ì€ ê·œì¹™ì„ ì œê±°í•˜ê³ , ìœ ì‚¬í•œ ê³ ì„±ê³¼ ê·œì¹™ì„ ë³‘í•©í•¨.
        """
        rules = self.memory.memory
        if len(rules) < 5:
            return {"status": "skipped", "reason": "ì§€ì‹ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ."}

        original_count = len(rules)
        optimized_rules = []
        removed_count = 0
        
        # 1. ìˆ˜ì¹˜ ê¸°ë°˜ í•„í„°ë§ (Heuristic Pruning)
        active_pool = []
        for r in rules:
            usage = r.get("usage_count", 0)
            success = r.get("success_count", 0)
            # ìƒì„±ëœ ì§€ ì˜¤ë˜ë˜ì—ˆëŠ”ë°(ì˜ˆ: ì‚¬ìš© 5íšŒ ì´ìƒ) ì„±ê³µë¥ ì´ 30% ë¯¸ë§Œì¸ ê²½ìš° í‡´ì¶œ
            if usage >= 5 and (success / usage) < 0.3:
                removed_count += 1
                logger.info(f"ğŸ—‘ï¸ Rule {r['id']} removed due to low performance.")
                continue
            active_pool.append(r)

        # 2. LLM ê¸°ë°˜ ì‹œë§¨í‹± ë³‘í•© (Semantic Merging)
        rules_text = "\n".join([f"- [{r['id']}] {r['learned_instruction']} (Success: {r.get('success_count',0)}/{r.get('usage_count',0)})" for r in active_pool])
        
        prompt = f"""ë‹¤ìŒì€ ìê°€ ì§„í™” ì‹œìŠ¤í…œì´ ìŠµë“í•œ ì§€ì‹ ë¦¬ìŠ¤íŠ¸ë‹¤.
        1. ë‚´ìš©ì´ ì¤‘ë³µë˜ê±°ë‚˜ ì„œë¡œ ë³´ì™„ì ì¸ ê³ ì„±ê³¼ ê·œì¹™ë“¤ì€ í•˜ë‚˜ì˜ ë” ê°•ë ¥í•˜ê³  ë²”ìš©ì ì¸ ê·œì¹™ìœ¼ë¡œ ë³‘í•©í•˜ë¼.
        2. ë³‘í•©ëœ ê·œì¹™ì€ ê°€ì¥ í•µì‹¬ì ì¸ íŠ¸ë¦¬ê±° íŒ¨í„´ì„ ìœ ì§€í•´ì•¼ í•œë‹¤.
        3. ì‹¤ì œ ì„±ê³µ ì‚¬ë¡€ê°€ ë§ì€ ì§€ì‹ì„ ìš°ì„ í•˜ë¼.
        
        [Knowledge List]
        {rules_text}
        
        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë³‘í•© ë° ì •ì œëœ ìµœì¢… JSON ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë¼:
        [{{ "instruction": "...", "trigger_patterns": ["...", "..."], "severity": 1~5 }}]
        """
        
        try:
            response_text = self.backend.generate(model_id, [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
            new_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            if isinstance(new_data, list):
                # ìµœì¢… ë©”ëª¨ë¦¬ êµì²´ (ì•„ì¹´ì´ë¹™ ì´ë ¥ì„ ë‚¨ê¸°ê±°ë‚˜ ë°±ì—… ê¶Œì¥)
                updated_memory = []
                for idx, item in enumerate(new_data):
                    updated_memory.append({
                        "id": f"RULE_EVOLVED_{datetime.now().strftime('%Y%m%d')}_{idx}",
                        "learned_instruction": item["instruction"],
                        "trigger_patterns": item["trigger_patterns"],
                        "severity": item.get("severity", 3),
                        "created_at": datetime.now().isoformat(),
                        "usage_count": 0,
                        "success_count": 0,
                        "failure_count": 0,
                        "parent_rules": [r["id"] for r in active_pool], # ëª¨ë“  ë¶€ëª¨ í›„ë³´ ê¸°ë¡ (ê³„ë³´ ì—°ê²°)
                        "is_super_rule": True # ë³‘í•©ëœ ì§€ëŠ¥ì„ì„ í‘œì‹œ
                    })
                
                # ìƒ¤ë”© ì•„í‚¤í…ì²˜ ëŒ€ì‘: ì „ì²´ë¥¼ 'general' ìƒ¤ë“œë¡œ ì·¨ê¸‰í•˜ê±°ë‚˜ ê°œë³„ ë¶„ë¥˜ í•„ìš”
                # ì—¬ê¸°ì„œëŠ” ë³‘í•©ëœ ì „ì—­ ì§€ì‹ì´ë¯€ë¡œ 'general' ìƒ¤ë“œë¡œ ì—…ë°ì´íŠ¸
                self.memory.shards["general"] = updated_memory
                self.memory._persist_shard("general")
                
                return {
                    "status": "success",
                    "original": original_count,
                    "final": len(updated_memory),
                    "removed": removed_count,
                    "merged": original_count - removed_count - len(updated_memory)
                }
        except Exception as e:
            logger.error(f"Knowledge optimization failed: {e}")
            return {"status": "error", "reason": str(e)}

    def rank_context_relevance(self, messages: List[Dict[str, str]], current_plan: List[str]) -> List[float]:
        """ë©”ì‹œì§€ ë­‰ì¹˜ì™€ í˜„ì¬ ê³„íš ê°„ì˜ ì‹œë§¨í‹± ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ì‚°ì¶œí•¨."""
        if not messages or not current_plan: return [0.5] * len(messages)
        
        prompt = f"""You are the Context Librarian. 
        Rank each message's relevance to the current execution plan (0.0 to 1.0).
        High score if the message contains essential technical details or tool outputs needed for the plan.
        Low score if it's general chat or unrelated noise.
        
        [Plan]:
        {json.dumps(current_plan, ensure_ascii=False)}
        
        [Messages]:
        {json.dumps(messages, ensure_ascii=False)}
        
        Return JSON ONLY:
        {{ "scores": [0.9, 0.2, 0.5, ...] }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            return res_data.get("scores", [0.5] * len(messages))
        except Exception as e:
            logger.error(f"Context ranking failed: {e}")
            return [0.5] * len(messages)

    def garbage_collect_knowledge(self, model_id: str = "gemini-2.0-flash") -> Dict[str, Any]:
        """ì§€ì‹ ë² ì´ìŠ¤ì˜ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° ì‹œë§¨í‹± ìµœì í™” ìˆ˜í–‰."""
        report = {"removed": 0, "merged": 0, "optimized_shards": []}
        
        for cat in list(self.memory.shards.keys()):
            shard = self.memory.shards[cat]
            if not shard: continue
            
            # 1. ê°€ì¹˜ ê¸°ë°˜ í•„í„°ë§ (Heuristic Pruning)
            original_count = len(shard)
            active_rules = []
            for r in shard:
                val = self.memory.calculate_rule_value(r)
                if val <= 30.0:
                    report["removed"] += 1
                    logger.info(f"ğŸ—‘ï¸ Pruned low-value rule: {r['id']} (Value: {val})")
                    continue
                active_rules.append(r)
            
            # 2. ê³ ê°€ì¹˜ ê·œì¹™ ì‹œë§¨í‹± ë³‘í•© (Semantic Merging)
            if len(active_rules) >= 5:
                logger.info(f"âœ¨ Optimizing shard '{cat}' semantically...")
                # ê¸°ì¡´ prune_memory ë¡œì§ í™œìš© ë˜ëŠ” ê³ ë„í™”
                self.memory.shards[cat] = active_rules
                self.memory.prune_memory(model_id=model_id)
                optimized_count = len(self.memory.shards[cat])
                report["merged"] += (len(active_rules) - optimized_count)
                report["optimized_shards"].append(cat)
            else:
                self.memory.shards[cat] = active_rules
                
            self.memory._persist_shard(cat)
            
        return report

    def identify_test_hotspots(self) -> List[Dict[str, Any]]:
        """ìˆ˜ì • ì˜í–¥ë ¥ì´ í¬ì§€ë§Œ í…ŒìŠ¤íŠ¸ê°€ ëˆ„ë½ëœ 'í•«ìŠ¤íŒŸ'ì„ ì‹ë³„í•¨."""
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        
        # 1. í…ŒìŠ¤íŠ¸ ëˆ„ë½ íŒŒì¼ ëª©ë¡ íšë“
        missing_tests = self.identify_missing_tests()
        if not missing_tests: return []
        
        hotspots = []
        for item in missing_tests:
            file_path = item["file"]
            # 2. í•´ë‹¹ íŒŒì¼ì˜ ì˜í–¥ë ¥ ë°˜ê²½(Impact Radius) ë¶„ì„
            impact = indexer.get_impact_radius(file_path)
            
            # ìœ„í—˜ ì ìˆ˜ ì‚°ì¶œ: (ëˆ„ë½ ì»¤ë²„ë¦¬ì§€ ê°€ì¤‘ì¹˜) * (ì˜í–¥ ë°›ëŠ” ëª¨ë“ˆ ìˆ˜ + 1)
            coverage_gap = 100 - item["coverage"]
            impact_score = (len(impact["direct"]) + len(impact["indirect"]) * 0.5 + 1)
            risk_score = round(coverage_gap * impact_score, 1)
            
            hotspots.append({
                "file": file_path,
                "coverage": item["coverage"],
                "impact_count": len(impact["direct"]) + len(impact["indirect"]),
                "risk_score": risk_score,
                "reason": f"High impact ({len(impact['direct'])} direct deps) with low coverage ({item['coverage']}%)"
            })
            
        return sorted(hotspots, key=lambda x: x["risk_score"], reverse=True)

    def generate_evolution_roadmap(self) -> List[Dict[str, Any]]:
        """ì§€ëŠ¥ ì§€ìˆ˜ê°€ ë‚®ì€ ëª¨ë“ˆì„ ì‹ë³„í•˜ì—¬ ì§„í™” ìš°ì„ ìˆœìœ„ ë¡œë“œë§µ ìƒì„±"""
        from gortex.utils.indexer import SynapticIndexer
        intel_map = SynapticIndexer().calculate_intelligence_index()
        
        # ì§€ëŠ¥ ì§€ìˆ˜ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ë³´ì™„ì´ í•„ìš”í•œ ëª¨ë“ˆ)
        weak_modules = sorted(intel_map.items(), key=lambda x: x[1])
        
        # Tech Radar í›„ë³´êµ° íšë“
        adoption_candidates = []
        if os.path.exists("tech_radar.json"):
            try:
                with open("tech_radar.json", "r") as f:
                    radar_data = json.load(f)
                    adoption_candidates = radar_data.get("adoption_candidates", [])
            except: pass

        roadmap = []
        for file_path, score in weak_modules[:5]: # ê°€ì¥ ì·¨ì•½í•œ 5ê°œ ëª¨ë“ˆ ëŒ€ìƒ
            # í•´ë‹¹ íŒŒì¼ì— ì ìš© ê°€ëŠ¥í•œ ì‹ ê¸°ìˆ  ì œì•ˆ ë§¤ì¹­
            suggested_tech = next((c["tech"] for c in adoption_candidates if c["target_file"] == file_path), "Refactoring Required")
            
            roadmap.append({
                "target": file_path,
                "current_maturity": score,
                "suggested_tech": suggested_tech,
                "priority": "High" if score < 10 else "Medium"
            })
            
        return roadmap
