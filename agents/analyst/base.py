import logging
import json
import os
import re
import math
from typing import Dict, Any, List, Optional
from datetime import datetime
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.vector_store import LongTermMemory

logger = logging.getLogger("GortexAnalystBase")

class AnalystAgent:
    """Gortex ì‹œìŠ¤í…œì˜ ë¶„ì„ ë° ì§„í™” ë‹´ë‹¹ ì—ì´ì „íŠ¸ (Base Class)"""
    def __init__(self):
        self.backend = LLMFactory.get_default_backend()
        self.memory = EvolutionaryMemory()
        self.ltm = LongTermMemory() # LTM ì§ì ‘ ì†Œìœ  (ë³µêµ¬)

    def calculate_efficiency_score(self, success: bool, tokens: int, latency_ms: int, energy_cost: int) -> float:
        """ì‘ì—…ì˜ íš¨ìœ¨ì„±ì„ ìˆ˜ì¹˜í™” (0~100) - ì›ë³¸ ì •êµí•œ ê³µì‹ ë³µêµ¬"""
        if not success: return 0.0
        
        # ë¹„ìš© í•¨ìˆ˜: í† í° 1ê°œ = 0.01, ë ˆì´í„´ì‹œ 1ms = 0.01, ì—ë„ˆì§€ 1 = 1.0 (ê°€ì¤‘ì¹˜ ì ìš©)
        cost = (tokens * 0.01) + (latency_ms * 0.005) + (energy_cost * 2.0)
        # íš¨ìœ¨ì„± = ê¸°ë³¸ ë³´ìƒ(100) / (ë¹„ìš© + 1)
        score = 100.0 / (1.0 + math.log1p(cost / 5.0))
        return round(min(100.0, score), 1)

    def scan_project_complexity(self, directory: str = ".") -> List[Dict[str, Any]]:
        """ì½”ë“œì˜ ë³µì¡ë„ì™€ ê¸°ìˆ  ë¶€ì±„ ì •ë°€ ìŠ¤ìº” (ì›ë³¸ ë¡œì§ ë³µêµ¬)"""
        debt_list = []
        ignore_dirs = {'.git', 'venv', '__pycache__', 'logs', 'site-packages'}
        
        for root, dirs, files in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            for f in files:
                if f.endswith(".py"):
                    path = os.path.join(root, f)
                    try:
                        with open(path, 'r', encoding='utf-8') as file:
                            content = file.read()
                            lines = content.splitlines()
                            
                            # ì •ë°€ ë³µì¡ë„ ì¶”ì •
                            score = len(re.findall(r"\b(if|elif|for|while|except|def|class|with|async)\b", content))
                            score += len(lines) // 20
                            
                            if score > 10:
                                debt_list.append({
                                    "file": path, 
                                    "score": score, 
                                    "reason": "High logical density" if score > 30 else "Moderate complexity",
                                    "issue": "íŒŒì¼ì˜ ë…¼ë¦¬ì  ë°€ë„ê°€ ë„ˆë¬´ ë†’ì•„ ê°€ë…ì„±ì´ ì €í•˜ë¨",
                                    "refactor_strategy": "ê¸´ ë©”ì„œë“œë¥¼ ë¶„ë¦¬í•˜ê³  ê´€ì‹¬ì‚¬ë¥¼ ëª¨ë“ˆë¡œ ê²©ë¦¬í•˜ë¼"
                                })
                    except: pass
        return sorted(debt_list, key=lambda x: x["score"], reverse=True)

    def analyze_data(self, file_path: str) -> Dict[str, Any]:
        """ë°ì´í„° íŒŒì¼(CSV, JSON ë“±) ì •ë°€ ë¶„ì„ ìˆ˜í–‰ (ì›ë³¸ ë¡œì§ ë³µêµ¬)"""
        try:
            import pandas as pd
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
                summary = {
                    "rows": len(df), "columns": list(df.columns),
                    "stats": df.describe().to_dict()
                }
                return {"status": "success", "summary": summary, "file": file_path}
        except: pass
        return {"status": "failed", "reason": "Data analysis failed"}

    def identify_missing_tests(self) -> List[Dict[str, Any]]:
        """ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í…ŒìŠ¤íŠ¸ê°€ ì‹œê¸‰í•œ íŒŒì¼ì„ ì‹ë³„í•©ë‹ˆë‹¤."""
        try:
            # ì‹¤ì‹œê°„ ì»¤ë²„ë¦¬ì§€ ë°ì´í„° íšë“ ì‹œë„ (ëª…ë ¹ì–´ ì‹¤í–‰)
            import subprocess
            subprocess.run(["python3", "-m", "coverage", "json", "-o", "logs/coverage.json"], capture_output=True)
            
            if os.path.exists("logs/coverage.json"):
                with open("logs/coverage.json", "r") as f:
                    data = json.load(f)
                
                results = []
                for file_path, info in data.get("files", {}).items():
                    summary = info.get("summary", {})
                    percent = summary.get("percent_covered", 100)
                    if percent < 80: # 80% ë¯¸ë§Œì¸ íŒŒì¼ ëŒ€ìƒ
                        results.append({
                            "file": file_path,
                            "coverage": round(percent, 1),
                            "missing_lines": info.get("missing_lines", []),
                            "priority": "High" if percent < 50 else "Medium"
                        })
                return sorted(results, key=lambda x: x["coverage"])
        except Exception as e:
            logger.error(f"Failed to identify missing tests: {e}")
        return []

    def audit_architecture(self) -> List[Dict[str, Any]]:
        """í”„ë¡œì íŠ¸ì˜ ì˜ì¡´ì„± êµ¬ì¡°ê°€ ì•„í‚¤í…ì²˜ ì›ì¹™ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ ê°ì‚¬"""
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        deps = indexer.generate_dependency_graph()
        
        violations = []
        layers = {"utils": 0, "core": 1, "ui": 2, "agents": 3, "tests": 4}
        
        for dep in deps:
            source, target = dep["source"], dep["target"]
            src_layer = next((l for l in layers if f"gortex.{l}" in source or source.startswith(l)), None)
            tgt_layer = next((l for l in layers if f"gortex.{l}" in target or target.startswith(l)), None)
            
            if src_layer and tgt_layer and layers[src_layer] < layers[tgt_layer]:
                violations.append({
                    "type": "Layer Violation", "source": source, "target": target,
                    "reason": f"í•˜ìœ„ ë ˆì´ì–´ '{src_layer}'ê°€ ìƒìœ„ ë ˆì´ì–´ '{tgt_layer}'ë¥¼ ì°¸ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤."
                })
        return violations

    def synthesize_global_rules(self, model_id: str = "gemini-1.5-pro") -> str:
        """ë¶„ì‚°ëœ í•™ìŠµ ê·œì¹™ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‹œìŠ¤í…œ ëŒ€ì›ì¹™(docs/RULES.md) ìë™ ê°±ì‹ """
        rules = self.memory.memory
        if not rules: return "ì •ë¦¬í•  ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤."
        
        rules_context = "".join([f"- [{r['severity']}] {r['learned_instruction']}\n" for r in rules])
        prompt = f"ë‹¤ìŒ í•™ìŠµ ê·œì¹™ë“¤ì„ 5ê°€ì§€ ì´ë‚´ì˜ í•µì‹¬ ì›ì¹™ìœ¼ë¡œ ìš”ì•½í•˜ë¼.\n\n[ê·œì¹™ ë¦¬ìŠ¤íŠ¸]\n{rules_context}\n\nê²°ê³¼ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ë”°ë¥´ë¼."
        
        try:
            summary = self.backend.generate(model_id, [{"role": "user", "content": prompt}])
            rules_md_path = "docs/RULES.md"
            original_rules = ""
            if os.path.exists(rules_md_path):
                with open(rules_md_path, 'r', encoding='utf-8') as f: original_rules = f.read()
            
            section_start = "## ğŸ¤– Auto-Evolved Coding Standards"
            if section_start in original_rules:
                header = original_rules.split(section_start)[0]
                new_content = f"{header}{section_start}\n\n> ë§ˆì§€ë§‰ ê°±ì‹ : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n{summary}"
            else:
                new_content = f"{original_rules}\n\n{section_start}\n\n{summary}"
                
            with open(rules_md_path, 'w', encoding='utf-8') as f: f.write(new_content)
            return "âœ… ì „ì—­ ê·œì¹™ ì¢…í•© ì™„ë£Œ."
        except Exception as e:
            logger.error(f"Global rule synthesis failed: {e}")
            return f"âŒ ì‹¤íŒ¨: {e}"