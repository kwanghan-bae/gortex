import logging
import asyncio
import re
from typing import Dict, Any, List
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from gortex.core.llm.factory import LLMFactory
from gortex.core.state import GortexState
from gortex.utils.cache import GortexCache
from gortex.utils.vector_store import LongTermMemory

logger = logging.getLogger("GortexResearcher")

class ResearcherAgent:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ìš”ì•½í•˜ëŠ” ì—ì´ì „íŠ¸.
    ì„±ëŠ¥ì„ ìœ„í•´ ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤(ì´ë¯¸ì§€ ë“±)ë¥¼ ì°¨ë‹¨í•˜ê³  ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        self.cache = GortexCache()
        self.ltm = LongTermMemory()
        self.timeout = 8000  # 8 seconds (SPEC)

    async def scrape_url(self, url: str) -> str:
        """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ URLì˜ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œ (ì´ë¯¸ì§€ ì œì™¸)"""
        logger.info(f"ğŸŒ Scraping: {url}")
        
        # ìºì‹œ í™•ì¸
        cached = self.cache.get(url)
        if cached:
            logger.info("â™»ï¸  Using cached research data.")
            return cached

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                # ì„±ëŠ¥ì„ ìœ„í•´ ì´ë¯¸ì§€ ë° CSS ì°¨ë‹¨ ì‹œë„
                context = await browser.new_context(user_agent="Mozilla/5.0")
                page = await context.new_page()
                
                # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ ë¡œì§
                async def block_aggressively(route):
                    if route.request.resource_type in ["image", "media", "font"]:
                        await route.abort()
                    else:
                        await route.continue_()
                await page.route("**/*", block_aggressively)

                await page.goto(url, timeout=self.timeout, wait_until="domcontentloaded")
                content = await page.content()
                await browser.close()

                # HTML ì •ì œ (BeautifulSoup)
                soup = BeautifulSoup(content, 'html.parser')
                # ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸ ë“± ì œê±°
                for s in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    s.decompose()
                
                text = soup.get_text(separator=' ', strip=True)
                # ë‹¤ì¤‘ ê³µë°± ì œê±°
                text = re.sub(r'\s+', ' ', text)
                
                # ê²°ê³¼ ìºì‹±
                self.cache.set(url, text[:10000]) # ìƒìœ„ 1ë§Œìë§Œ ì €ì¥
                return text[:5000] # ë¶„ì„ìš©ìœ¼ë¡œ 5ì²œì ë°˜í™˜
        except Exception as e:
            logger.error(f"Scraping failed for {url}: {e}")
            return f"Error: {e}"

    async def fetch_api_docs(self, library_name: str) -> str:
        """ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ìµœì‹  API ë¬¸ì„œë¥¼ ì •ë°€í•˜ê²Œ ê²€ìƒ‰ ë° ì¶”ì¶œ"""
        query = f"official documentation {library_name} python api reference example"
        logger.info(f"ğŸ” Fetching API documentation for: {library_name}")
        
        # 1. ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await self.search_and_summarize(query)
        
        # 2. LLMì„ í†µí•œ ì •ë°€ í•„í„°ë§ ë° ì‹œê·¸ë‹ˆì²˜ ì¶”ì¶œ (LLMFactory ì ìš©)
        from gortex.core.llm.factory import LLMFactory
        backend = LLMFactory.get_default_backend()
        prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ '{library_name}'ì˜ 
        í•µì‹¬ í´ë˜ìŠ¤, í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜, ê·¸ë¦¬ê³  ê°„ë‹¨í•œ ì˜ˆì œ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ë¼. 
        ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ë°°ì œí•˜ê³  ê°œë°œìê°€ ì¦‰ì‹œ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ  ì •ë³´ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ë¼.
        
        [Search Results]
        {search_results}
        """
        response_text = backend.generate("gemini-1.5-flash", [{"role": "user", "content": prompt}])
        
        # 3. [Knowledge Integration] ì‹¤ì‹œê°„ ë¬¸ì„œë¥¼ ì¥ê¸° ê¸°ì–µì— ì„ì‹œ ì €ì¥
        self.ltm.memorize(
            f"Live API Docs ({library_name}): {response_text[:1000]}...",
            {"source": "LiveDocs", "library": library_name, "type": "api_reference"}
        )
        
        return response_text

    async def search_and_summarize(self, query: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›¹ ì¡°ì‚¬ ìˆ˜í–‰"""
        # DuckDuckGo HTML ê²€ìƒ‰ í™œìš©
        search_url = f"https://duckduckgo.com/html/?q={query.replace(' ', '+')}"
        
        # ë¹„ë™ê¸° ì œì–´ê¶Œ ì–‘ë³´
        await asyncio.sleep(0)
        return await self.scrape_url(search_url)


def researcher_node(state: GortexState) -> Dict[str, Any]:
    """Researcher ë…¸ë“œ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
    agent = ResearcherAgent()
    from gortex.core.llm.factory import LLMFactory
    backend = LLMFactory.get_default_backend()
    from gortex.utils.prompt_loader import loader
    
    # 1. ì˜ë„ ë° ì¿¼ë¦¬ ì¶”ì¶œ (ì™¸ë¶€ í…œí”Œë¦¿ ì‚¬ìš©)
    last_msg_obj = state["messages"][-1]
    last_msg = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
    
    # ì§€ì¹¨ ë¡œë“œ
    base_instruction = loader.get_prompt("researcher")
    intent_prompt = f"{base_instruction}\n\nì‚¬ìš©ì ìš”ì²­: {last_msg}\n\nìœ„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ì™€ ì¿¼ë¦¬ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ë¼."
    
    assigned_model = state.get("assigned_model", "gemini-1.5-flash")
    config = {"temperature": 0.0}
    if backend.supports_structured_output():
        from google.genai import types
        config = types.GenerateContentConfig(response_mime_type="application/json")

    try:
        response_text = backend.generate(assigned_model, [{"role": "user", "content": intent_prompt}], config)
        import json
        import re
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        req_info = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        query = req_info.get("query", last_msg)
    except:
        req_info = {"is_docs_needed": False, "query": last_msg}
        query = last_msg

    # 2. ë¹„ë™ê¸° ì‹¤í–‰ (Playwright)
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    if loop.is_running():
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            if req_info.get("is_docs_needed"):
                future = executor.submit(lambda: asyncio.run(agent.fetch_api_docs(query)))
            else:
                future = executor.submit(lambda: asyncio.run(agent.search_and_summarize(query)))
            research_result = future.result()
    else:
        if req_info.get("is_docs_needed"):
            research_result = loop.run_until_complete(agent.fetch_api_docs(query))
        else:
            research_result = loop.run_until_complete(agent.search_and_summarize(query))

    # 3. ê²°ê³¼ ìš”ì•½
    summary_instruction = loader.get_prompt("researcher_summary")
    summary_prompt = f"{summary_instruction}\n\nì‚¬ìš©ì ìš”ì²­: {last_msg}\nê²€ìƒ‰ ê²°ê³¼: {research_result}"
    
    summary_text = backend.generate(assigned_model, [{"role": "user", "content": summary_prompt}])

    return {
        "messages": [("ai", summary_text)],
        "next_node": "manager"
    }