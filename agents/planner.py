import logging
import json
from typing import Dict, Any, List
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.utils.tools import list_files
from gortex.utils.indexer import SynapticIndexer

logger = logging.getLogger("GortexPlanner")

def planner_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ì„¤ê³„ì(Planner) ë…¸ë“œ.
    ì‚¬ìš©ìì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì›ìì  ë‹¨ìœ„(Atomic Unit)ì˜ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    """
    auth = GortexAuth()
    indexer = SynapticIndexer()
    
    # 1. ì¸ë±ìŠ¤ ê¸°ë°˜ ë§¥ë½ ì •ë³´ ì¶”ì¶œ
    last_msg_obj = state["messages"][-1]
    last_msg = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
    search_results = indexer.search(last_msg) if last_msg else []
    
    context_info = ""
    if search_results:
        context_info = "\n[Synaptic Index Search Results]\n"
        for res in search_results[:5]: # ìƒìœ„ 5ê°œë§Œ ì£¼ì…
            context_info += f"- {res['type'].upper()} '{res['name']}' in {res['file']} (Line {res['line']})\n"
            if res.get('docstring'):
                context_info += f"  Doc: {res['docstring'].split('\\n')[0]}\n"

    # 2. í˜„ì¬ í™˜ê²½ íŒŒì•…
    current_files = list_files(state.get("working_dir", "."))
    file_cache = state.get("file_cache", {})
    
    # 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    base_instruction = f"""ë„ˆëŠ” Gortex v1.0 ì‹œìŠ¤í…œì˜ ìˆ˜ì„ ì•„í‚¤í…íŠ¸(Planner)ë‹¤.
í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³ , ì‚¬ìš©ìì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ë‹¨ê³„(Step)ë“¤ì„ ê³„íší•˜ë¼.

[Current File Structure]
{current_files}
{context_info}

[File Cache Status]
í˜„ì¬ ë„ˆëŠ” {len(file_cache)}ê°œ íŒŒì¼ì˜ ìµœì‹  ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ìˆë‹¤. 
ì´ë¯¸ ì½ì€ íŒŒì¼(Cacheì— ì¡´ì¬)ì€ ë‹¤ì‹œ 'read_file'ì„ í•  í•„ìš”ê°€ ì—†ìœ¼ë‚˜, 
ì¤‘ìš”í•œ ë³€ê²½ì´ ì˜ˆìƒë˜ê±°ë‚˜ í™•ì‹¤í•˜ì§€ ì•Šë‹¤ë©´ ë‹¤ì‹œ ì½ëŠ” ê³„íšì„ ì„¸ì›Œë¼.

[Planning Rules]
1. ì‘ì—…ì„ 'ì›ìì  ë‹¨ìœ„(Atomic Unit)'ë¡œ ìª¼ê°œë¼.
2. ê° ë‹¨ê³„ëŠ” `coder` ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì´ì–´ì•¼ í•œë‹¤.
3. ì‹¬ë³¼ëª…(í´ë˜ìŠ¤/í•¨ìˆ˜ëª…)ì„ ì–¸ê¸‰í–ˆìœ¼ë‚˜ íŒŒì¼ ê²½ë¡œë¥¼ ëª¨ë¥¼ ê²½ìš°, ì œê³µëœ [Synaptic Index Search Results]ë¥¼ ì°¸ì¡°í•˜ì—¬ ì •í™•í•œ ê²½ë¡œë¥¼ targetìœ¼ë¡œ ì„¤ì •í•˜ë¼.
4. ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ(System Optimization Request)ì´ í¬í•¨ëœ ê²½ìš°, ê·¸ íƒ€ë‹¹ì„±ì„ ë°˜ë“œì‹œ ê²€í† í•˜ë¼.
5. ì½”ë“œë¥¼ ì‘ì„±(`write_file`, `apply_patch`)í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ëœë‹¤ë©´, ë°˜ë“œì‹œ ê·¸ ì§í›„ì— í•´ë‹¹ ê¸°ëŠ¥ì— ëŒ€í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸(`tests/test_X.py`)ë¥¼ ì‘ì„±í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•˜ëŠ” ë‹¨ê³„ë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤.
6. ì‚¬ìš©ìê°€ íŠ¹ì • ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: requests, pandas ë“±)ì˜ ì‚¬ìš©ë²•ì´ë‚˜ ìµœì‹  ê¸°ëŠ¥ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°, ì½”ë“œë¥¼ ì‘ì„±í•˜ê¸° ì „ ë°˜ë“œì‹œ `researcher` ì—ì´ì „íŠ¸ë¥¼ í†µí•´ ìµœì‹  API ë¬¸ì„œë¥¼ í•™ìŠµí•˜ëŠ” ë‹¨ê³„ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ë°°ì¹˜í•˜ë¼.

[Output Schema (Strict JSON)]
ë°˜ë“œì‹œ ë‹¤ìŒ JSON êµ¬ì¡°ë¥¼ ì¤€ìˆ˜í•˜ë¼:
{{
  "thought_process": "í˜„ì¬ ìƒí™© ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½ ì´ìœ ",
  "goal": "ìµœì¢… ë‹¬ì„± ëª©í‘œ ìš”ì•½",
  "steps": [
    {{"id": 1, "action": "tool_name", "target": "file_path_or_command", "reason": "êµ¬ì²´ì ì¸ ì´ìœ "}}
  ]
}}
"""

    # ì§„í™”ëœ ì œì•½ ì¡°ê±´ ì£¼ì…
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES]\n{constraints_str}"

    config = types.GenerateContentConfig(
        system_instruction=base_instruction + "\n\n[Thought Tree Rules]\nì‚¬ìš©ìì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì„¤ê³„ ê³¼ì •ì„ ë…¼ë¦¬ì ì¸ íŠ¸ë¦¬ êµ¬ì¡°(ë¶„ì„ -> ì„¤ê³„ -> ê²€ì¦ ê³„íš)ë¡œ êµ¬ì„±í•˜ë¼.\n\n[Architecture Sketcher]\në³µì¡í•œ ë¡œì§ì´ë‚˜ ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•œ ê²½ìš°, ë°˜ë“œì‹œ 'diagram_code' í•„ë“œì— Mermaid í˜•ì‹ì˜ ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\n[Self-Consistency Rules]\nê³„íšì„ í™•ì •í•˜ê¸° ì „, ë°˜ë“œì‹œ 'internal_critique' ë‹¨ê³„ì—ì„œ ì„¤ê³„ì˜ ëˆ„ë½ ì‚¬í•­ì´ë‚˜ ëª¨ìˆœì„ ì¬ê²€í† í•˜ë¼.\n\n[Predictive Pre-fetching]\në‹¤ìŒ ë‹¨ê³„ì—ì„œ í•„ìš”í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ë¦¬ì†ŒìŠ¤(íŒŒì¼ ì½ê¸° ë“±)ê°€ ìˆë‹¤ë©´ 'pre_fetch' ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ì‹œì¼œ ì‹œìŠ¤í…œ ì§€ì—° ì‹œê°„ì„ ìµœì í™”í•˜ë¼.",
        temperature=0.0,
        response_mime_type="application/json",
        response_schema={
            "type": "OBJECT",
            "properties": {
                "thought_process": {"type": "STRING", "description": "ì „ì²´ ì„¤ê³„ ìš”ì•½"},
                "internal_critique": {"type": "STRING", "description": "ì„¤ê³„ ê³„íšì— ëŒ€í•œ ë¹„íŒì  ì¬ê²€í† "},
                "thought_tree": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "STRING"},
                            "parent_id": {"type": "STRING", "nullable": True},
                            "text": {"type": "STRING"},
                            "type": {"type": "STRING", "enum": ["analysis", "design", "verification"]},
                            "priority": {"type": "INTEGER"},
                            "certainty": {"type": "NUMBER"}
                        },
                        "required": ["id", "text", "type", "priority", "certainty"]
                    }
                },
                "pre_fetch": {
                    "type": "ARRAY",
                    "items": {"type": "STRING"},
                    "description": "ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìœ„í•´ ë¯¸ë¦¬ ë¡œë“œí•´ë‘˜ íŒŒì¼ ê²½ë¡œ ëª©ë¡"
                },
                "diagram_code": {"type": "STRING", "description": "Mermaid í˜•ì‹ì˜ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ (ì„ íƒì‚¬í•­)"},
                "goal": {"type": "STRING"},
                "steps": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "INTEGER"},
                            "action": {
                                "type": "STRING", 
                                "enum": ["read_file", "write_file", "execute_shell", "list_files", "apply_patch"]
                            },
                            "target": {"type": "STRING"},
                            "reason": {"type": "STRING"}
                        },
                        "required": ["id", "action", "target", "reason"]
                    }
                }
            },
            "required": ["thought_process", "internal_critique", "thought_tree", "goal", "steps"]
        }
    )

    # 3. Gemini í˜¸ì¶œ
    assigned_model = state.get("assigned_model", "gemini-3-flash-preview")
    response = auth.generate(
        model_id=assigned_model,
        contents=state["messages"],
        config=config
    )

    try:
        # JSON íŒŒì‹±
        plan_data = response.parsed if hasattr(response, 'parsed') else json.loads(response.text)
        
        logger.info(f"Planner Thought: {plan_data.get('thought_process')}")
        logger.info(f"Critique: {plan_data.get('internal_critique')}")
        
        # Planì„ ìƒíƒœì— ì €ì¥í•˜ê³  Coderì—ê²Œ ë„˜ê¹€
        plan_steps = [json.dumps(step, ensure_ascii=False) for step in plan_data["steps"]]
        
        updates = {
            "thought_process": plan_data.get("thought_process"),
            "internal_critique": plan_data.get("internal_critique"),
            "thought_tree": plan_data.get("thought_tree"),
            "plan": plan_steps,
            "current_step": 0,
            "next_node": "coder",
            "messages": [("ai", f"ê³„íšì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤: {plan_data.get('goal')} ({len(plan_steps)} steps)")]
        }
        
        if plan_data.get("pre_fetch"):
            updates["pre_fetch"] = plan_data["pre_fetch"]
            logger.info(f"ğŸš€ Pre-fetching suggested for {len(plan_data['pre_fetch'])} files.")
        
        if plan_data.get("diagram_code"):
            updates["diagram_code"] = plan_data["diagram_code"]
            updates["messages"].append(("system", "ğŸ“Š ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì›¹ ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤."))
            
        return updates


    except Exception as e:
        logger.error(f"Error parsing planner response: {e}")
        return {
            "next_node": "__end__", 
            "messages": [("ai", f"ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")]
        }
