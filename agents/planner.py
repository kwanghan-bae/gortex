import logging
import json
from typing import Dict, Any, List
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.utils.tools import list_files
from gortex.utils.indexer import SynapticIndexer

logger = logging.getLogger("GortexPlanner")

def planner_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ì„¤ê³„ì(Planner) ë…¸ë“œ.
    ì‚¬ìš©ìì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì›ìì  ë‹¨ìœ„(Atomic Unit)ì˜ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    """
    auth = GortexAuth()
    indexer = SynapticIndexer()
    
    # 1. ì¸ë±ìŠ¤ ê¸°ë°˜ ë§¥ë½ ì •ë³´ ì¶”ì¶œ
    last_msg_obj = state["messages"][-1]
    last_msg = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
    search_results = indexer.search(last_msg) if last_msg else []
    
    context_info = ""
    if search_results:
        context_info = "\n[Synaptic Index Search Results]\n"
        for res in search_results[:5]: # ìƒìœ„ 5ê°œë§Œ ì£¼ì…
            context_info += f"- {res['type'].upper()} '{res['name']}' in {res['file']} (Line {res['line']})\n"
            if res.get('docstring'):
                context_info += f"  Doc: {res['docstring'].split('\\n')[0]}\n"

    # 2. í˜„ì¬ í™˜ê²½ íŒŒì•…
    current_files = list_files(state.get("working_dir", "."))
    file_cache = state.get("file_cache", {})
    
    # 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì™¸ë¶€ í…œí”Œë¦¿ ë¡œë“œ)
    from gortex.utils.prompt_loader import loader
    base_instruction = loader.get_prompt(
        "planner", 
        persona_id=state.get("assigned_persona", "standard"),
        current_files=current_files, 
        context_info=context_info
    )

    # ì§„í™”ëœ ì œì•½ ì¡°ê±´ ì£¼ì…
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES]\n{constraints_str}"

    config = types.GenerateContentConfig(
        system_instruction=base_instruction + "\n\n[Thought Tree Rules]\nì‚¬ìš©ìì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì„¤ê³„ ê³¼ì •ì„ ë…¼ë¦¬ì ì¸ íŠ¸ë¦¬ êµ¬ì¡°(ë¶„ì„ -> ì„¤ê³„ -> ê²€ì¦ ê³„íš)ë¡œ êµ¬ì„±í•˜ë¼.\n\n[Architecture Sketcher]\në³µì¡í•œ ë¡œì§ì´ë‚˜ ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•œ ê²½ìš°, ë°˜ë“œì‹œ 'diagram_code' í•„ë“œì— Mermaid í˜•ì‹ì˜ ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼.\n\n[Self-Consistency Rules]\nê³„íšì„ í™•ì •í•˜ê¸° ì „, ë°˜ë“œì‹œ 'internal_critique' ë‹¨ê³„ì—ì„œ ì„¤ê³„ì˜ ëˆ„ë½ ì‚¬í•­ì´ë‚˜ ëª¨ìˆœì„ ì¬ê²€í† í•˜ë¼.\n\n[Predictive Pre-fetching]\në‹¤ìŒ ë‹¨ê³„ì—ì„œ í•„ìš”í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ë¦¬ì†ŒìŠ¤(íŒŒì¼ ì½ê¸° ë“±)ê°€ ìˆë‹¤ë©´ 'pre_fetch' ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ì‹œì¼œ ì‹œìŠ¤í…œ ì§€ì—° ì‹œê°„ì„ ìµœì í™”í•˜ë¼.",
        temperature=0.0,
        response_mime_type="application/json",
        response_schema={
            "type": "OBJECT",
            "properties": {
                "thought_process": {"type": "STRING", "description": "ì „ì²´ ì„¤ê³„ ìš”ì•½"},
                "impact_analysis": {
                    "type": "OBJECT",
                    "properties": {
                        "target": {"type": "STRING", "description": "ìˆ˜ì • ëŒ€ìƒ ë©”ì¸ íŒŒì¼"},
                        "direct": {"type": "ARRAY", "items": {"type": "STRING"}, "description": "ì§ì ‘ ì˜í–¥ ë°›ëŠ” íŒŒì¼ ëª©ë¡"},
                        "indirect": {"type": "ARRAY", "items": {"type": "STRING"}, "description": "ê°„ì ‘ ì˜í–¥ ë°›ëŠ” íŒŒì¼ ëª©ë¡"},
                        "risk_level": {"type": "STRING", "enum": ["Critical", "High", "Medium", "Low"]}
                    },
                    "required": ["target", "direct", "indirect", "risk_level"]
                },
                "internal_critique": {"type": "STRING", "description": "ì„¤ê³„ ê³„íšì— ëŒ€í•œ ë¹„íŒì  ì¬ê²€í† "},
                "thought_tree": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "STRING"},
                            "parent_id": {"type": "STRING", "nullable": True},
                            "text": {"type": "STRING"},
                            "type": {"type": "STRING", "enum": ["analysis", "design", "verification"]},
                            "priority": {"type": "INTEGER"},
                            "certainty": {"type": "NUMBER"},
                            "visual_payload": {"type": "STRING", "nullable": True, "description": "ë…¸ë“œì™€ ê´€ë ¨ëœ ì‹œê°ì  ë°ì´í„° (ì˜ˆ: Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ)"}
                        },
                        "required": ["id", "text", "type", "priority", "certainty"]
                    }
                },
                "pre_fetch": {
                    "type": "ARRAY",
                    "items": {"type": "STRING"},
                    "description": "ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìœ„í•´ ë¯¸ë¦¬ ë¡œë“œí•´ë‘˜ íŒŒì¼ ê²½ë¡œ ëª©ë¡"
                },
                "diagram_code": {"type": "STRING", "description": "Mermaid í˜•ì‹ì˜ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ (ì„ íƒì‚¬í•­)"},
                "goal": {"type": "STRING"},
                "steps": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "INTEGER"},
                            "action": {
                                "type": "STRING", 
                                "enum": ["read_file", "write_file", "execute_shell", "list_files", "apply_patch"]
                            },
                            "target": {"type": "STRING"},
                            "reason": {"type": "STRING"}
                        },
                        "required": ["id", "action", "target", "reason"]
                    }
                }
            },
            "required": ["thought_process", "internal_critique", "thought_tree", "goal", "steps"]
        }
    )

    # 3. Gemini í˜¸ì¶œ
    assigned_model = state.get("assigned_model", "gemini-3-flash-preview")
    response = auth.generate(
        model_id=assigned_model,
        contents=state["messages"],
        config=config
    )

    try:
        # JSON íŒŒì‹±
        plan_data = response.parsed if hasattr(response, 'parsed') else json.loads(response.text)
        
        logger.info(f"Planner Thought: {plan_data.get('thought_process')}")
        logger.info(f"Critique: {plan_data.get('internal_critique')}")
        
        # Planì„ ìƒíƒœì— ì €ì¥í•˜ê³  Coderì—ê²Œ ë„˜ê¹€
        plan_steps = [json.dumps(step, ensure_ascii=False) for step in plan_data["steps"]]
        
        from gortex.utils.translator import i18n
        updates = {
            "thought_process": plan_data.get("thought_process"),
            "impact_analysis": plan_data.get("impact_analysis"),
            "internal_critique": plan_data.get("internal_critique"),
            "thought_tree": plan_data.get("thought_tree"),
            "plan": plan_steps,
            "current_step": 0,
            "next_node": "coder",
            "messages": [("ai", i18n.t("task.plan_established", goal=plan_data.get('goal'), steps=len(plan_steps)))]
        }
        
        if plan_data.get("impact_analysis"):
            impact = plan_data["impact_analysis"]
            impact_msg = f"âš ï¸ **ìˆ˜ì • ì˜í–¥ ë²”ìœ„ ë¶„ì„** (ìœ„í—˜ë„: {impact.get('risk_level', 'Unknown')})\n"
            impact_msg += f"- ëŒ€ìƒ: {impact.get('target')}\n"
            if impact.get("direct"): impact_msg += f"- ì§ì ‘ ì˜í–¥: {', '.join(impact['direct'])}\n"
            if impact.get("indirect"): impact_msg += f"- ê°„ì ‘ ì˜í–¥: {', '.join(impact['indirect'])}"
            updates["messages"].append(("system", impact_msg))
        
        if plan_data.get("pre_fetch"):
            updates["pre_fetch"] = plan_data["pre_fetch"]
            logger.info(f"ğŸš€ Pre-fetching suggested for {len(plan_data['pre_fetch'])} files.")
        
        if plan_data.get("diagram_code"):
            updates["diagram_code"] = plan_data["diagram_code"]
            updates["messages"].append(("system", "ğŸ“Š ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì›¹ ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤."))
            
        return updates


    except Exception as e:
        logger.error(f"Error parsing planner response: {e}")
        from gortex.utils.translator import i18n
        return {
            "next_node": "__end__", 
            "messages": [("ai", i18n.t("error.general", error=str(e)))]
        }
