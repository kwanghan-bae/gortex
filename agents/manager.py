import logging
import json
import os
from typing import Dict, List, Any
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.log_vectorizer import SemanticLogSearch
from gortex.utils.translator import SynapticTranslator
from gortex.utils.vector_store import LongTermMemory

logger = logging.getLogger("GortexManager")

def manager_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ì¤‘ì•™ ê´€ì œì†Œ(Manager) ë…¸ë“œ.
    ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    """
    auth = GortexAuth()
    log_search = SemanticLogSearch()
    translator = SynapticTranslator()
    ltm = LongTermMemory()
    evo_mem = EvolutionaryMemory()
    
    # 1. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ (ë‹¤êµ­ì–´ ì§€ì›)
    last_msg_obj = state["messages"][-1]
    raw_input = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
    lang_info = translator.detect_and_translate(raw_input)
    
    # ë‚´ë¶€ ì²˜ë¦¬ëŠ” í•œêµ­ì–´ ë§¥ë½ì„ í¬í•¨í•œ ì›ë¬¸ í™œìš©
    internal_input = lang_info.get("translated_text", raw_input) if not lang_info.get("is_korean") else raw_input

    # ì—ë„ˆì§€ ìƒíƒœ ì¡°ê¸° íšë“ (ì „ì—­ ì°¸ì¡°ìš©)
    energy = state.get("agent_energy", 100)

    # [Persona Lab] ìƒí™©ë³„ í˜ë¥´ì†Œë‚˜ ì„ íƒ ì „ëµ
    recommended_personas = ["Innovation", "Stability"]
    if any(k in internal_input.lower() for k in ["ë³´ì•ˆ", "security", "ì·¨ì•½ì ", "auth"]):
        recommended_personas.append("Security Expert")
    if any(k in internal_input.lower() for k in ["ui", "ux", "ë””ìì¸", "dashboard", "í™”ë©´"]):
        recommended_personas.append("UX Specialist")
    
    persona_context = f"í˜„ì¬ ìš”ì²­ì˜ ì„±ê²©ì— ë”°ë¼ ë‹¤ìŒ í˜ë¥´ì†Œë‚˜ ì¤‘ 2ê°œ ì´ìƒì„ ì„ íƒí•˜ì—¬ í† ë¡ ì„ êµ¬ì„±í•˜ë¼: {', '.join(recommended_personas)}"

    # 2. ì¥ê¸° ê¸°ì–µ ì†Œí™˜ (Recall)
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒ¤ë”©ëœ ì§€ì‹ ì†Œí™˜
    namespace = os.path.basename(state.get("working_dir", "global"))
    recalled_items = ltm.recall(internal_input, namespace=namespace)
    
    # ë§Œì•½ í”„ë¡œì íŠ¸ ì „ìš© ì§€ì‹ì´ ë¶€ì¡±í•˜ë©´ ê¸€ë¡œë²Œ ìƒ¤ë“œì—ì„œë„ ì¶”ê°€ ê²€ìƒ‰
    if len(recalled_items) < 2:
        recalled_items += ltm.recall(internal_input, namespace="global", limit=2)
    
    ltm_context = ""
    knowledge_lineage = []
    
    if recalled_items:
        texts = [item["content"] for item in recalled_items]
        ltm_context = "\n[RECALLED LONG-TERM KNOWLEDGE]\n" + "\n".join([f"- {t}" for t in texts])
        if any("ìµœì‹ " in k or "ì‹ ê·œ" in k for k in texts):
            ltm_context += "\n(ì°¸ê³ : ìœ„ ì •ë³´ì—ëŠ” ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê³„íš ìˆ˜ë¦½ì— ì ê·¹ ë°˜ì˜í•˜ì‹­ì‹œì˜¤.)"
        
        # ì§€ì‹ ê³„ë³´ ë°ì´í„° êµ¬ì„±
        for item in recalled_items:
            knowledge_lineage.append({
                "source": item["metadata"].get("source", "Unknown"),
                "score": item["score"],
                "content_preview": item["content"][:50] + "..."
            })

    # 3. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (CBR)
    past_cases = log_search.search_similar_cases(internal_input)
    
    case_context = ""
    if past_cases:
        case_context = "\n[PAST SIMILAR CASES (FOR REFERENCE)]\n"
        for i, case in enumerate(past_cases):
            case_context += f"Case {i+1}: {case.get('agent')} encountered {case.get('event')}. Payload: {json.dumps(case.get('payload'))}\n"

    # 4. í•™ìŠµëœ ë§¤í¬ë¡œ í™•ì¸
    macros = evo_mem.get_macros()
    macro_context = ""
    if macros:
        macro_context = "\n[Learned Macros (User-Defined Skills)]\n"
        for m in macros:
            macro_context += f"- Command: '{m['name']}' -> Steps: {m['steps']}\n"

    # 5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    base_instruction = f"""ë„ˆëŠ” Gortex v1.0 ì‹œìŠ¤í…œì˜ ìˆ˜ì„ ë§¤ë‹ˆì €(Manager)ë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ ê°€ì¥ ì í•©í•œ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ë°°ë¶„í•˜ë¼.
{ltm_context}
{case_context}
{macro_context}

[Interactive Decision Rules]
ë§Œì•½ ì‚¬ìš©ìì˜ ì£¼ê´€ì ì¸ ì·¨í–¥ì´ ì¤‘ìš”í•˜ê±°ë‚˜, ì—¬ëŸ¬ ê¸°ìˆ ì  ì„ íƒì§€ ì¤‘ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ëšœë ·í•œ ìƒí™©ì´ë¼ë©´ ë…ë‹¨ì ìœ¼ë¡œ ê²°ì •í•˜ì§€ ë§ˆë¼.
ì´ ê²½ìš° `requires_user_input`ì„ trueë¡œ ì„¤ì •í•˜ê³ , `question_to_user`ì— ì„ íƒì§€ì˜ ì¥ë‹¨ì ì„ í¬í•¨í•œ ì •ì¤‘í•œ ì§ˆë¬¸ì„ ì‘ì„±í•˜ë¼. ì‚¬ìš©ìì˜ ë‹µë³€ì€ ì‹œìŠ¤í…œì˜ ì¥ê¸°ì ì¸ ì„ í˜¸ë„ ê·œì¹™ìœ¼ë¡œ í•™ìŠµë  ê²ƒì´ë‹¤.

[Adaptive UI Rules]
í˜„ì¬ ìˆ˜í–‰í•  ì‘ì—…ì˜ ì„±ê²©ì— ë§ì¶° `ui_mode`ë¥¼ ì„¤ì •í•˜ë¼.
- coding: ë³µì¡í•œ ì½”ë“œ ì‘ì„± ë˜ëŠ” ë¦¬íŒ©í† ë§ ì‹œ (ì‹œë®¬ë ˆì´ì…˜ íŒ¨ë„ ê°•ì¡°)
- research: ì›¹ ê²€ìƒ‰ ë° ìµœì‹  ê¸°ìˆ  ì¡°ì‚¬ ì‹œ (ê²€ìƒ‰ ê²°ê³¼ ë° ì§€ì‹ ê·¸ë˜í”„ ê°•ì¡°)
- debugging: í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ë¶„ì„ ë° ì˜¤ë¥˜ ìˆ˜ì • ì‹œ (ë¡œê·¸ ë° ì„±ì°° ë¦¬í¬íŠ¸ ê°•ì¡°)
- analyst: ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ì‹œ (ì°¨íŠ¸ ë° ì„±ê³¼ ë¦¬í¬íŠ¸ ê°•ì¡°)
- standard: ì¼ë°˜ì ì¸ ëŒ€í™” ë° ë³µí•© ì‘ì—… ì‹œ

[User Intent Projection Rules]
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ê·¸ë“¤ì´ ë¨¸ë¦¿ì†ì— ê·¸ë¦¬ëŠ” ìµœì¢…ì ì¸ 'í° ê·¸ë¦¼(big_picture)'ê³¼ ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ 'ë‹¨ê³„ë³„ ì˜ë„(intent_nodes)'ë¥¼ ì¶”ì¶œí•˜ë¼.
- ì‚¬ìš©ìê°€ "ê²°êµ­ Xë¥¼ ë§Œë“¤ê³  ì‹¶ì–´"ë¼ê³  í•˜ë©´ Xë¥¼ `big_picture`ë¡œ ì„¤ì •í•˜ê³ , í•„ìš”í•œ êµ¬ì„± ìš”ì†Œë“¤ì„ ë…¸ë“œë¡œ ë¶„í•´í•˜ë¼.
- ê° ë…¸ë“œì˜ ìƒíƒœ(status)ë¥¼ íŒë‹¨í•˜ì—¬ í˜„ì¬ ì§„í–‰ ìƒí™©ì„ ì‹œê°í™”í•˜ë¼.

[Speculative Reasoning Rules]
ì‚¬ìš©ìì˜ ìš”ì²­ì´ ë³µì¡í•˜ê±°ë‚˜ í•´ê²° ë°©ë²•ì´ ì—¬ëŸ¬ ê°€ì§€ì¸ ê²½ìš°, 'swarm' ë…¸ë“œë¥¼ í†µí•´ ë³‘ë ¬ ê²€í† í•˜ë¼. 
ë§Œì•½ ì‘ì—…ì˜ ìœ„í—˜ë„ê°€ ë†’ê±°ë‚˜(Risk > 0.7), ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ìš”ì²­ì¸ ê²½ìš° ë°˜ë“œì‹œ **'í† ë¡  ëª¨ë“œ(Debate Mode)'**ë¥¼ í™œì„±í™”í•˜ë¼. 
ì´ ê²½ìš° ê³„íš(`parallel_tasks`)ì— "ê´€ì  í† ë¡ : [ì£¼ì œ]" í˜•ì‹ì„ í¬í•¨ì‹œí‚¤ê³ , {persona_context}ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ë“¤ì´ ìƒë°˜ëœ ì „ë¬¸ í˜ë¥´ì†Œë‚˜ë¥¼ ê°–ë„ë¡ ì§€ì‹œí•˜ë¼.

[Macro Learning Rules]
1. ì‚¬ìš©ìê°€ "ë°°ì›Œ(Learn): [ëª…ë ¹ì–´]ëŠ” [ì‘ì—…1], [ì‘ì—…2]...ë¥¼ ì˜ë¯¸í•´"ë¼ê³  í•˜ë©´, ì´ë¥¼ ìƒˆë¡œìš´ ë§¤í¬ë¡œë¡œ ì €ì¥í•˜ë„ë¡ 'analyst'ì—ê²Œ ìš”ì²­í•˜ë¼.
2. ì‚¬ìš©ìê°€ ì €ì¥ëœ ë§¤í¬ë¡œ ëª…ë ¹ì–´(ì˜ˆ: "ë°°í¬ ì‹¤í–‰í•´")ë¥¼ ì‚¬ìš©í•˜ë©´, ì •ì˜ëœ ë‹¨ê³„ë“¤ì„ ì‹¤í–‰ ê³„íšì— í¬í•¨ì‹œí‚¤ë„ë¡ 'planner'ì—ê²Œ ìƒì„¸íˆ ì§€ì‹œí•˜ë¼.

[Agent Factory Rules]
ë§Œì•½ í˜„ì¬ ê°€ìš©í•œ ì—ì´ì „íŠ¸(planner, researcher, analyst)ë¡œ ì²˜ë¦¬í•˜ê¸°ì— ì§€ë‚˜ì¹˜ê²Œ ì „ë¬¸í™”ëœ ì˜ì—­(ì˜ˆ: ì–‘ìì—­í•™ ë¶„ì„, íŠ¹ì • ê²Œì„ ì—”ì§„ íŠœë‹ ë“±)ì´ ë°˜ë³µì ìœ¼ë¡œ ìš”ì²­ëœë‹¤ë©´, ìƒˆë¡œìš´ ì „ë¬¸ ì—ì´ì „íŠ¸ì˜ ìƒì„±ì„ ê²°ì •í•˜ë¼. 
ì´ ê²½ìš° 'thought'ì— ì‚¬ìœ ë¥¼ ì ê³  'next_node'ë¥¼ 'planner'ë¡œ ì§€ì •í•˜ì—¬ ì‹ ê·œ ì—ì´ì „íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ê²Œ í•˜ë¼.

ì—ì´ì „íŠ¸ ì—­í• :
- planner: ì½”ë“œ ì‘ì„±, ë²„ê·¸ ìˆ˜ì •, ì—ì´ì „íŠ¸ ìê°€ ìƒì„±(Agent Factory) ë“± ëª¨ë“  ê°œë°œ ê´€ë ¨ ì‘ì—….
- researcher: ìµœì‹  ì •ë³´ ê²€ìƒ‰, ê¸°ìˆ  ì¡°ì‚¬.
- analyst: ë°ì´í„° ë¶„ì„, í”¼ë“œë°± ë¶„ì„, ë§¤í¬ë¡œ ì €ì¥.
- swarm: ë³‘ë ¬ ì¶”ë¡  ë° ë¶„ì‚° ì²˜ë¦¬.
"""

    # ìê°€ ì§„í™” ì—”ì§„ì—ì„œ í•™ìŠµëœ ê·œì¹™ì´ ìˆë‹¤ë©´ ì£¼ì…
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES (MUST FOLLOW)]\n{constraints_str}"

    # [Tech Radar Adoption] ì‹ ê¸°ìˆ  ë„ì… í›„ë³´ í™•ì¸
    if os.path.exists("tech_radar.json"):
        try:
            with open("tech_radar.json", "r") as f:
                radar_data = json.load(f)
                candidates = radar_data.get("adoption_candidates", [])
                if candidates:
                    candidates_str = "\n".join([f"- {c['tech']} -> {c['target_file']}: {c['reason']}" for c in candidates[:3]])
                    base_instruction += f"\n\n[OPPORTUNITY: Tech Radar Adoption]\nìƒˆë¡œìš´ ê¸°ìˆ  ë„ì… ê¸°íšŒê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì‘ì—…ì´ ë°”ì˜ì§€ ì•Šë‹¤ë©´, ì´ë¥¼ ë°˜ì˜í•œ ë¦¬íŒ©í† ë§ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.\n{candidates_str}"
        except Exception as e:
            logger.warning(f"Failed to read tech radar: {e}")

    # ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ(Improvement Task)ì´ ìˆëŠ”ì§€ í™•ì¸
    system_improvement_msg = ""
    
    # [Auto-Refactor Loop] ì—ë„ˆì§€ê°€ ì¶©ë¶„í•  ë•Œ ëŠ¥ë™ì  ê¸°ìˆ  ë¶€ì±„ í•´ì†Œ ì‹œë„
    if energy > 80 and not any("refactor" in msg.content.lower() for msg in reversed(state["messages"]) if hasattr(msg, 'content')):
        from gortex.agents.analyst import AnalystAgent
        refactor_target = AnalystAgent().suggest_refactor_target()
        if refactor_target:
            base_instruction += f"\n\n[AUTO-REFACTOR OPPORTUNITY]\ní˜„ì¬ ì‹œìŠ¤í…œ ì—ë„ˆì§€ê°€ ì¶©ë¶„í•˜ì—¬ ê¸°ìˆ  ë¶€ì±„ í•´ì†Œë¥¼ ì œì•ˆí•œë‹¤.\nëŒ€ìƒ íŒŒì¼: {refactor_target['file']}\në¬¸ì œ: {refactor_target['issue']}\nì „ëµ: {refactor_target['refactor_strategy']}\nì´ ì‘ì—…ì„ ìµœìš°ì„ ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê³„íšì„ ìˆ˜ë¦½í•˜ë¼."

    for msg in reversed(state["messages"]):
        content = msg.content if hasattr(msg, 'content') else str(msg)
        if "ìµœì í™” ì „ë¬¸ê°€ì˜ ì œì•ˆ:" in content:
            system_improvement_msg = content
            base_instruction += f"\n\n[SYSTEM OPTIMIZATION REQUEST (HIGH PRIORITY)]\n{system_improvement_msg}"
            base_instruction += "\nê²°ì •: í˜„ì¬ ì‹œìŠ¤í…œ ìµœì í™” ìš”ì²­ì´ ìˆìœ¼ë¯€ë¡œ, ë¬´ì¡°ê±´ 'next_node'ë¥¼ 'planner'ë¡œ ì§€ì •í•˜ë¼."
            break


    # ì—ë„ˆì§€ ìƒíƒœì— ë”°ë¥¸ ì§€ì¹¨ ì£¼ì…
    energy = state.get("agent_energy", 100)
    if energy < 50:
        base_instruction += f"\n\n[Energy Alert] í˜„ì¬ ë„ˆì˜ ì—ë„ˆì§€ê°€ {energy}%ë¡œ ë‚®ë‹¤. ê°€ê¸‰ì  ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë‹¨ìˆœí•œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ë¶ˆí•„ìš”í•œ ë„êµ¬ í˜¸ì¶œì„ ìì œí•˜ë¼."

    # íš¨ìœ¨ì„± ìƒíƒœì— ë”°ë¥¸ ì§€ì¹¨ ì£¼ì…
    last_eff = state.get("last_efficiency", 100.0)
    if last_eff < 40.0:
        base_instruction += f"\n\n[Efficiency Alert] ìµœê·¼ ì‘ì—…ì˜ íš¨ìœ¨ì„± ì ìˆ˜ê°€ {last_eff:.1f}ë¡œ ë§¤ìš° ë‚®ë‹¤. ì´ëŠ” ë¹„íš¨ìœ¨ì ì¸ ì ‘ê·¼ ë°©ì‹ ë•Œë¬¸ì¼ ìˆ˜ ìˆë‹¤. ì´ë²ˆ ê³„íš ìˆ˜ë¦½ ì‹œì—ëŠ” ë” ì‹ ì¤‘í•˜ê³  ìƒì„¸í•œ(Detailed) ë‹¨ê³„ë¥¼ êµ¬ì„±í•˜ì—¬ ì‹¤íŒ¨ ë¹„ìš©ì„ ì¤„ì—¬ë¼."

    # ì§€ì†ì ì¸ ì €íš¨ìœ¨ ê°ì§€ ë° Optimizer ê°•ì œ (Self-Healing)
    eff_history = state.get("efficiency_history", [])
    if len(eff_history) >= 3 and all(e < 40.0 for e in eff_history[-3:]):
        logger.warning("ğŸ“‰ Persistent low efficiency detected. Forcing optimization.")
        base_instruction += "\n\n[CRITICAL ALERT] ìµœê·¼ 3íšŒ ì—°ì† ì‘ì—… íš¨ìœ¨ì„±ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (< 40). ì¦‰ì‹œ 'optimizer' ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…í•˜ì—¬ ì›ì¸ì„ ì§„ë‹¨í•˜ê³  í•´ê²°ì±…ì„ ë§ˆë ¨í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ ì‘ì—…ì€ ì¤‘ë‹¨í•˜ì‹­ì‹œì˜¤."

    config = types.GenerateContentConfig(
        system_instruction=base_instruction + "\n\n[Thought Tree Rules]\nì‚¬ê³  ê³¼ì •ì„ ë…¼ë¦¬ì ì¸ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ì„¸ë¶„í™”í•˜ë¼. ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ë¶„ì„, íŒë‹¨, ê²°ë¡ ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë¼.\n\n[Self-Consistency Rules]\nìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ê¸° ì „, ë°˜ë“œì‹œ 'internal_critique' ë‹¨ê³„ì—ì„œ ìì‹ ì˜ ë…¼ë¦¬ì  ëª¨ìˆœì´ë‚˜ ìœ„í—˜ ìš”ì†Œë¥¼ ë¹„íŒì ìœ¼ë¡œ ì¬ê²€í† í•˜ë¼.",
        temperature=0.0,
        response_mime_type="application/json",
        response_schema={
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING", "description": "ì „ì²´ ì‚¬ê³  ìš”ì•½"},
                "internal_critique": {"type": "STRING", "description": "ìì‹ ì˜ ì¶”ë¡  ê³¼ì •ì— ëŒ€í•œ ë¹„íŒì  ì¬ê²€í† "},
                "thought_tree": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "STRING"},
                            "parent_id": {"type": "STRING", "nullable": True},
                            "text": {"type": "STRING"},
                            "type": {"type": "STRING", "enum": ["analysis", "reasoning", "decision"]},
                            "priority": {"type": "INTEGER", "description": "1~5 (ë‚®ìŒ~ë†’ìŒ)"},
                            "certainty": {"type": "NUMBER", "description": "0.0~1.0 (í™•ì‹ ë„)"}
                        },
                        "required": ["id", "text", "type", "priority", "certainty"]
                    }
                },
                "next_node": {
                    "type": "STRING", 
                    "enum": ["planner", "researcher", "analyst", "swarm", "optimizer", "__end__"]
                },
                "requires_user_input": {
                    "type": "BOOLEAN",
                    "description": "ì¤‘ìš”í•œ ê²°ì •ì— ëŒ€í•´ ì‚¬ìš©ìì˜ ìŠ¹ì¸ì´ë‚˜ ì˜ê²¬ì´ í•„ìš”í•œ ê²½ìš° true"
                },
                "question_to_user": {
                    "type": "STRING",
                    "description": "ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë‚´ìš©"
                },
                "ui_mode": {
                    "type": "STRING",
                    "enum": ["coding", "research", "analyst", "debugging", "standard"],
                    "description": "í˜„ì¬ ì‘ì—… ë§¥ë½ì— ê°€ì¥ ì í•©í•œ UI ë ˆì´ì•„ì›ƒ ëª¨ë“œ"
                },
                "user_intent_projection": {
                    "type": "OBJECT",
                    "properties": {
                        "big_picture": {"type": "STRING", "description": "ì‚¬ìš©ìê°€ ë‹¬ì„±í•˜ë ¤ëŠ” ìµœì¢…ì ì¸ ëª©í‘œ"},
                        "intent_nodes": {
                            "type": "ARRAY",
                            "items": {
                                "type": "OBJECT",
                                "properties": {
                                    "id": {"type": "STRING"},
                                    "label": {"type": "STRING"},
                                    "status": {"type": "STRING", "enum": ["pending", "in_progress", "done"]},
                                    "parent_id": {"type": "STRING", "nullable": True}
                                },
                                "required": ["id", "label", "status"]
                            }
                        }
                    }
                },
                "parallel_tasks": {
                    "type": "ARRAY",
                    "items": {"type": "STRING"},
                    "description": "next_nodeê°€ 'swarm'ì¼ ë•Œ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  í•˜ìœ„ ì‘ì—… ë¦¬ìŠ¤íŠ¸"
                },
                "response_to_user": {"type": "STRING", "description": "ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ë‹µí•  ë‚´ìš©"}
            },
            "required": ["thought", "internal_critique", "thought_tree", "next_node"]
        }
    )

    # 2. Gemini í˜¸ì¶œì„ í†µí•œ ì˜ë„ ë¶„ì„ ë° ë¼ìš°íŒ… ê²°ì •
    # ìµœê·¼ API í˜¸ì¶œ ë¹ˆë„ ë° ì—ë„ˆì§€ ìˆ˜ì¤€ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ (Adaptive Throttling & Energy Awareness)
    call_count = state.get("api_call_count", 0)
    
    if call_count > 10 or energy < 30:
        model_id = "gemini-2.5-flash-lite"
        reason = "High API usage" if call_count > 10 else "Low Energy"
        logger.warning(f"âš ï¸ {reason} ({call_count}/{energy}). Throttling to {model_id}")
    else:
        # ì„¤ì •ëœ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        from gortex.core.config import GortexConfig
        model_id = GortexConfig().get("default_model", "gemini-1.5-flash")

    response = auth.generate(
        model_id=model_id,
        contents=state["messages"],
        config=config 
    )


    # JSON ì‘ë‹µ íŒŒì‹±
    try:
        res_data = response.parsed if hasattr(response, 'parsed') else json.loads(response.text)
        
        logger.info(f"Manager Thought: {res_data.get('thought')}")
        logger.info(f"Critique: {res_data.get('internal_critique')}")
        
        # ì—ë„ˆì§€ ì†Œëª¨ ê¸°ë¡ (ë‹¨ìˆœí™”: ë§¤ í„´ 5% ê°ì†Œ)
        new_energy = max(0, energy - 5)
        
        target_node = res_data.get("next_node", "__end__")
        
        # [Peer Review Economy] í¬ë ˆë”§ ê¸°ë°˜ ëª¨ë¸ í• ë‹¹ ë° ë¹„ìš© ì°¨ê°
        assigned_model = "gemini-1.5-flash"
        credits = state.get("token_credits", {}).copy()
        
        if target_node in ["planner", "coder", "analyst"]:
            level = state.get("agent_economy", {}).get(target_node, {}).get("level", "Novice")
            balance = credits.get(target_node, 100.0)
            
            # ë¹„ìš© ì •ì˜: PRO ëª¨ë¸ = 50.0 credits
            if level == "Master" and energy >= 30 and balance >= 50.0:
                assigned_model = "gemini-1.5-pro"
                credits[target_node] = balance - 50.0 # ë¹„ìš© ì°¨ê°
                logger.info(f"ğŸ’ Master agent '{target_node}' purchased PRO model. Remaining: {credits[target_node]}")
            elif level == "Master" and balance < 50.0:
                logger.info(f"ğŸ’¸ Insufficient credits for '{target_node}'. Falling back to FLASH.")
            elif energy < 30:
                logger.info(f"ğŸ”‹ Low energy. Forcing FLASH model for '{target_node}'.")
        
        updates = {
            "thought": res_data.get("thought"),
            "internal_critique": res_data.get("internal_critique"),
            "thought_tree": res_data.get("thought_tree"),
            "next_node": target_node,
            "assigned_model": assigned_model,
            "agent_energy": new_energy,
            "ui_mode": res_data.get("ui_mode", "standard"),
            "token_credits": credits,
            "knowledge_lineage": knowledge_lineage,
            "user_intent_projection": res_data.get("user_intent_projection")
        }
        
        if res_data.get("parallel_tasks"):
            updates["plan"] = res_data["parallel_tasks"] # Swarmì„ ìœ„í•œ ì„ì‹œ ê³„íš ì£¼ì…
            logger.info(f"ğŸ“¦ Parallel tasks detected: {len(res_data['parallel_tasks'])} items.")

        
        # ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´ ì¶”ê°€
        if res_data.get("response_to_user"):
            updates["messages"] = [("ai", res_data["response_to_user"])]
            
        return updates

    except Exception as e:
        logger.error(f"Error parsing manager response: {e}")
        return {"next_node": "__end__", "messages": [("ai", "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")]}
