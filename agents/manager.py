import logging
import json
import os
import time
from typing import Dict, List, Any
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.log_vectorizer import SemanticLogSearch
from gortex.utils.translator import SynapticTranslator
from gortex.utils.vector_store import LongTermMemory
from gortex.utils.efficiency_monitor import EfficiencyMonitor

logger = logging.getLogger("GortexManager")

def manager_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ì¤‘ì•™ ê´€ì œì†Œ(Manager) ë…¸ë“œ.
    ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    (Ollama/Gemini í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)
    """
    backend = LLMFactory.get_default_backend()
    log_search = SemanticLogSearch()
    translator = SynapticTranslator()
    ltm = LongTermMemory()
    evo_mem = EvolutionaryMemory()
    monitor = EfficiencyMonitor()
    start_time = time.time()
    
    # 1. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ (ë‹¤êµ­ì–´ ì§€ì›)
    last_msg_obj = state["messages"][-1]
    raw_input = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
    lang_info = translator.detect_and_translate(raw_input)
    
    # ë‚´ë¶€ ì²˜ë¦¬ëŠ” í•œêµ­ì–´ ë§¥ë½ì„ í¬í•¨í•œ ì›ë¬¸ í™œìš©
    internal_input = lang_info.get("translated_text", raw_input) if not lang_info.get("is_korean") else raw_input

    # ì—ë„ˆì§€ ìƒíƒœ ì¡°ê¸° íšë“ (ì „ì—­ ì°¸ì¡°ìš©)
    energy = state.get("agent_energy", 100)

    # [Persona Lab] ìƒí™©ë³„ í˜ë¥´ì†Œë‚˜ ì„ íƒ ì „ëµ
    recommended_personas = ["Innovation", "Stability"]
    if any(k in internal_input.lower() for k in ["ë³´ì•ˆ", "security", "ì·¨ì•½ì ", "auth"]):
        recommended_personas.append("Security Expert")
    if any(k in internal_input.lower() for k in ["ui", "ux", "ë””ìì¸", "dashboard", "í™”ë©´"]):
        recommended_personas.append("UX Specialist")
    
    persona_context = f"í˜„ì¬ ìš”ì²­ì˜ ì„±ê²©ì— ë”°ë¼ ë‹¤ìŒ í˜ë¥´ì†Œë‚˜ ì¤‘ 2ê°œ ì´ìƒì„ ì„ íƒí•˜ì—¬ í† ë¡ ì„ êµ¬ì„±í•˜ë¼: {', '.join(recommended_personas)}"

    # 2. ì¥ê¸° ê¸°ì–µ ì†Œí™˜ (Recall)
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒ¤ë”©ëœ ì§€ì‹ ì†Œí™˜
    namespace = os.path.basename(state.get("working_dir", "global"))
    recalled_items = ltm.recall(internal_input, namespace=namespace)
    
    # ë§Œì•½ í”„ë¡œì íŠ¸ ì „ìš© ì§€ì‹ì´ ë¶€ì¡±í•˜ë©´ ê¸€ë¡œë²Œ ìƒ¤ë“œì—ì„œë„ ì¶”ê°€ ê²€ìƒ‰
    if len(recalled_items) < 2:
        recalled_items += ltm.recall(internal_input, namespace="global", limit=2)
    
    ltm_context = ""
    knowledge_lineage = []
    
    if recalled_items:
        texts = [item["content"] for item in recalled_items]
        ltm_context = "\n[RECALLED LONG-TERM KNOWLEDGE]\n" + "\n".join([f"- {t}" for t in texts])

        if any("ìµœì‹ " in k or "ì‹ ê·œ" in k for k in texts):
            ltm_context += "\n(ì°¸ê³ : ìœ„ ì •ë³´ì—ëŠ” ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê³„íš ìˆ˜ë¦½ì— ì ê·¹ ë°˜ì˜í•˜ì‹­ì‹œì˜¤.)"
        
        # ì§€ì‹ ê³„ë³´ ë°ì´í„° êµ¬ì„±
        for item in recalled_items:
            knowledge_lineage.append({
                "source": item["metadata"].get("source", "Unknown"),
                "score": item["score"],
                "content_preview": item["content"][:50] + "..."
            })

    # 3. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (CBR)
    past_cases = log_search.search_similar_cases(internal_input)
    
    case_context = ""
    if past_cases:
        case_context = "\n[PAST SIMILAR CASES (FOR REFERENCE)]\n"
        for i, case in enumerate(past_cases):
            case_context += f"Case {i+1}: {case.get('agent')} encountered {case.get('event')}. Payload: {json.dumps(case.get('payload'))}\n"

    # 4. í•™ìŠµëœ ë§¤í¬ë¡œ í™•ì¸
    macros = evo_mem.get_macros()
    macro_context = ""
    if macros:
        macro_context = "\n[Learned Macros (User-Defined Skills)]\n"
        for m in macros:
            macro_context += f"- Command: '{m['name']}' -> Steps: {m['steps']}\n"

    # 5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì™¸ë¶€ í…œí”Œë¦¿ ë¡œë“œ)
    from gortex.utils.prompt_loader import loader
    base_instruction = loader.get_prompt(
        "manager", 
        ltm_context=ltm_context, 
        case_context=case_context, 
        macro_context=macro_context,
        persona_context=persona_context
    )

    # ìê°€ ì§„í™” ì—”ì§„ì—ì„œ í•™ìŠµëœ ê·œì¹™ì´ ìˆë‹¤ë©´ ì£¼ì…
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES (MUST FOLLOW)]\n{constraints_str}"

    # [Tech Radar Adoption] ì‹ ê¸°ìˆ  ë„ì… í›„ë³´ í™•ì¸
    if os.path.exists("tech_radar.json"):
        try:
            with open("tech_radar.json", "r") as f:
                radar_data = json.load(f)
                candidates = radar_data.get("adoption_candidates", [])
                if candidates:
                    candidates_str = "\n".join([f"- {c['tech']} -> {c['target_file']}: {c['reason']}" for c in candidates[:3]])
                    base_instruction += f"\n\n[OPPORTUNITY: Tech Radar Adoption]\nìƒˆë¡œìš´ ê¸°ìˆ  ë„ì… ê¸°íšŒê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì‘ì—…ì´ ë°”ì˜ì§€ ì•Šë‹¤ë©´, ì´ë¥¼ ë°˜ì˜í•œ ë¦¬íŒ©í† ë§ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.\n{candidates_str}"
        except Exception as e:
            logger.warning(f"Failed to read tech radar: {e}")

    # ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ(Improvement Task)ì´ ìˆëŠ”ì§€ í™•ì¸
    system_improvement_msg = ""
    
    # [Auto-Refactor Loop] ì—ë„ˆì§€ê°€ ì¶©ë¶„í•  ë•Œ ëŠ¥ë™ì  ê¸°ìˆ  ë¶€ì±„ í•´ì†Œ ì‹œë„
    if energy > 80 and not any("refactor" in msg.content.lower() for msg in reversed(state["messages"]) if hasattr(msg, 'content')):
        from gortex.agents.analyst import AnalystAgent
        refactor_target = AnalystAgent().suggest_refactor_target()
        if refactor_target:
            file = refactor_target.get('file', 'Unknown')
            issue = refactor_target.get('issue', 'Technical debt detected')
            strategy = refactor_target.get('refactor_strategy', 'Modularization required')
            base_instruction += f"\n\n[AUTO-REFACTOR OPPORTUNITY]\ní˜„ì¬ ì‹œìŠ¤í…œ ì—ë„ˆì§€ê°€ ì¶©ë¶„í•˜ì—¬ ê¸°ìˆ  ë¶€ì±„ í•´ì†Œë¥¼ ì œì•ˆí•œë‹¤.\nëŒ€ìƒ íŒŒì¼: {file}\në¬¸ì œ: {issue}\nì „ëµ: {strategy}\nì´ ì‘ì—…ì„ ìµœìš°ì„ ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê³„íšì„ ìˆ˜ë¦½í•˜ë¼."

    for msg in reversed(state["messages"]):
        content = msg.content if hasattr(msg, 'content') else str(msg)
        if "ìµœì í™” ì „ë¬¸ê°€ì˜ ì œì•ˆ:" in content:
            system_improvement_msg = content
            base_instruction += f"\n\n[SYSTEM OPTIMIZATION REQUEST (HIGH PRIORITY)]\n{system_improvement_msg}"
            base_instruction += "\nê²°ì •: í˜„ì¬ ì‹œìŠ¤í…œ ìµœì í™” ìš”ì²­ì´ ìˆìœ¼ë¯€ë¡œ, ë¬´ì¡°ê±´ 'next_node'ë¥¼ 'planner'ë¡œ ì§€ì •í•˜ë¼."
            break


    # ì—ë„ˆì§€ ìƒíƒœì— ë”°ë¥¸ ì§€ì¹¨ ì£¼ì…
    if energy < 50:
        base_instruction += f"\n\n[Energy Alert] í˜„ì¬ ë„ˆì˜ ì—ë„ˆì§€ê°€ {energy}%ë¡œ ë‚®ë‹¤. ê°€ê¸‰ì  ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë‹¨ìˆœí•œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ë¶ˆí•„ìš”í•œ ë„êµ¬ í˜¸ì¶œì„ ìì œí•˜ë¼."

    # íš¨ìœ¨ì„± ìƒíƒœì— ë”°ë¥¸ ì§€ì¹¨ ì£¼ì…
    last_eff = state.get("last_efficiency", 100.0)
    if last_eff < 40.0:
        base_instruction += f"\n\n[Efficiency Alert] ìµœê·¼ ì‘ì—…ì˜ íš¨ìœ¨ì„± ì ìˆ˜ê°€ {last_eff:.1f}ë¡œ ë§¤ìš° ë‚®ë‹¤. ì´ëŠ” ë¹„íš¨ìœ¨ì ì¸ ì ‘ê·¼ ë°©ì‹ ë•Œë¬¸ì¼ ìˆ˜ ìˆë‹¤. ì´ë²ˆ ê³„íš ìˆ˜ë¦½ ì‹œì—ëŠ” ë” ì‹ ì¤‘í•˜ê³  ìƒì„¸í•œ(Detailed) ë‹¨ê³„ë¥¼ êµ¬ì„±í•˜ì—¬ ì‹¤íŒ¨ ë¹„ìš©ì„ ì¤„ì—¬ë¼."

    # ì§€ì†ì ì¸ ì €íš¨ìœ¨ ê°ì§€ ë° Optimizer ê°•ì œ (Self-Healing)
    eff_history = state.get("efficiency_history", [])
    if len(eff_history) >= 3 and all(e < 40.0 for e in eff_history[-3:]):
        logger.warning("ğŸ“‰ Persistent low efficiency detected. Forcing optimization.")
        base_instruction += "\n\n[CRITICAL ALERT] ìµœê·¼ 3íšŒ ì—°ì† ì‘ì—… íš¨ìœ¨ì„±ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (< 40). ì¦‰ì‹œ 'optimizer' ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…í•˜ì—¬ ì›ì¸ì„ ì§„ë‹¨í•˜ê³  í•´ê²°ì±…ì„ ë§ˆë ¨í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ ì‘ì—…ì€ ì¤‘ë‹¨í•˜ì‹­ì‹œì˜¤."

    # ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜ (Nativeìš©)
    schema = {
        "type": "OBJECT",
        "properties": {
            "thought": {"type": "STRING"},
            "internal_critique": {"type": "STRING"},
            "thought_tree": {
                "type": "ARRAY",
                "items": {
                    "type": "OBJECT",
                    "properties": {
                        "id": {"type": "STRING"},
                        "text": {"type": "STRING"},
                        "type": {"type": "STRING"},
                        "priority": {"type": "INTEGER"},
                        "certainty": {"type": "NUMBER"}
                    },
                    "required": ["id", "text", "type", "priority", "certainty"]
                }
            },
            "next_node": {"type": "STRING"},
            "response_to_user": {"type": "STRING"},
            "ui_mode": {"type": "STRING"},
            "assigned_persona": {"type": "STRING"}
        },
        "required": ["thought", "internal_critique", "thought_tree", "next_node"]
    }

    # ë°±ì—”ë“œ ëŠ¥ë ¥ì— ë”°ë¥¸ ì„¤ì • ë¶„ê¸°
    config = {"temperature": 0.0}
    if not backend.supports_structured_output():
        base_instruction += "\n\n[IMPORTANT: OUTPUT FORMAT]\nYou must respond in JSON format ONLY. Required fields: thought, internal_critique, thought_tree (list of {id, text, type, priority, certainty}), next_node, response_to_user."
    else:
        from google.genai import types
        config = types.GenerateContentConfig(
            system_instruction=base_instruction,
            temperature=0.0,
            response_mime_type="application/json",
            response_schema=schema
        )

    # ëª¨ë¸ ê²°ì • (Routing Intelligence)
    call_count = state.get("api_call_count", 0)
    scores = monitor.calculate_model_scores()
    logger.info(f"Model Scores: {scores}")
    
    # ê¸°ë³¸ ëª¨ë¸ í›„ë³´êµ°
    from gortex.core.config import GortexConfig
    config_obj = GortexConfig()
    cloud_model = config_obj.get("default_model", "gemini-1.5-flash")
    local_model = "ollama/llama3" # ê°€ì¹­ (ì¶”í›„ ì„¤ì •í™”)

    # ì§€ëŠ¥í˜• ì„ íƒ ë¡œì§
    if energy < 30 or scores.get(local_model, 0) > 70:
        # ì—ë„ˆì§€ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë¡œì»¬ ëª¨ë¸ ì„±ëŠ¥ì´ ì¶©ë¶„íˆ ê²€ì¦ëœ ê²½ìš°
        model_id = local_model
        logger.info(f"ğŸ¤– Intelligent Routing: Selecting Local Model ({model_id}) for efficiency.")
    elif call_count > 10:
        model_id = "gemini-2.5-flash-lite"
        logger.warning(f"âš ï¸ High API usage ({call_count}). Throttling to lite model.")
    else:
        model_id = cloud_model

    # [Exception] ì§„í™”ë‚˜ ë³µì¡í•œ ë¶„ì„ì€ ê°€ê¸‰ì  ê°•ë ¥í•œ ëª¨ë¸ ê°•ì œ
    if any(k in internal_input.lower() for k in ["ì§„í™”", "evolve", "architecture", "refactor"]):
        model_id = "gemini-1.5-pro"
        logger.info(f"ğŸ’ Critical task detected. Forcing PRO model.")

    # ë©”ì‹œì§€ êµ¬ì„±
    formatted_messages = [{"role": "system", "content": base_instruction}]
    for m in state["messages"]:
        role = m[0] if isinstance(m, tuple) else "user"
        content = m[1] if isinstance(m, tuple) else (m.content if hasattr(m, 'content') else str(m))
        formatted_messages.append({"role": role, "content": content})

    # LLM í˜¸ì¶œ
    success = False
    tokens = 0
    try:
        response_text = backend.generate(model=model_id, messages=formatted_messages, config=config)
        success = True
        tokens = len(base_instruction) // 4 + len(response_text) // 4
        
        # JSON íŒŒì‹±
        import re
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ë¡œì§ ìˆ˜í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        new_energy = max(0, energy - 5)
        target_node = res_data.get("next_node", "__end__")
        
        latency_ms = int((time.time() - start_time) * 1000)
        monitor.record_interaction("manager", model_id, success, tokens, latency_ms, metadata={"next_node": target_node})

        updates = {
            "thought": res_data.get("thought"),
            "internal_critique": res_data.get("internal_critique"),
            "thought_tree": res_data.get("thought_tree"),
            "next_node": target_node,
            "agent_energy": new_energy,
            "ui_mode": res_data.get("ui_mode", "standard"),
            "assigned_persona": res_data.get("assigned_persona", "standard")
        }
        
        if res_data.get("response_to_user"):
            updates["messages"] = [("ai", res_data["response_to_user"])]
            
        return updates

    except Exception as e:
        logger.error(f"Error in manager node: {e}")
        latency_ms = int((time.time() - start_time) * 1000)
        monitor.record_interaction("manager", model_id, False, 0, latency_ms, metadata={"error": str(e)})
        return {"next_node": "__end__", "messages": [("ai", f"âŒ ìš”ì²­ ë¶„ì„ ì‹¤íŒ¨: {e}")]}