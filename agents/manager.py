import logging
from typing import Dict, List, Any
from google.genai import types
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.utils.log_vectorizer import SemanticLogSearch
from gortex.utils.translator import SynapticTranslator
from gortex.utils.vector_store import LongTermMemory

logger = logging.getLogger("GortexManager")

def manager_node(state: GortexState) -> Dict[str, Any]:
    """
    Gortex ì‹œìŠ¤í…œì˜ ì¤‘ì•™ ê´€ì œì†Œ(Manager) ë…¸ë“œ.
    ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    """
    auth = GortexAuth()
    log_search = SemanticLogSearch()
    translator = SynapticTranslator()
    ltm = LongTermMemory()
    
    # 1. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ (ë‹¤êµ­ì–´ ì§€ì›)
    last_msg_obj = state["messages"][-1]
    raw_input = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
    lang_info = translator.detect_and_translate(raw_input)
    
    # ë‚´ë¶€ ì²˜ë¦¬ëŠ” í•œêµ­ì–´ ë§¥ë½ì„ í¬í•¨í•œ ì›ë¬¸ í™œìš©
    internal_input = lang_info.get("translated_text", raw_input) if not lang_info.get("is_korean") else raw_input

    # 2. ì¥ê¸° ê¸°ì–µ ì†Œí™˜ (Recall)
    long_term_knowledge = ltm.recall(internal_input)
    ltm_context = ""
    if long_term_knowledge:
        ltm_context = "\n[RECALLED LONG-TERM KNOWLEDGE]\n" + "\n".join([f"- {k}" for k in long_term_knowledge])

    # 3. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (CBR)
    past_cases = log_search.search_similar_cases(internal_input)
    
    case_context = ""
    if past_cases:
        case_context = "\n[PAST SIMILAR CASES (FOR REFERENCE)]\n"
        for i, case in enumerate(past_cases):
            case_context += f"Case {i+1}: {case.get('agent')} encountered {case.get('event')}. Payload: {json.dumps(case.get('payload'))}\n"

    # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    base_instruction = f"""ë„ˆëŠ” Gortex v1.0 ì‹œìŠ¤í…œì˜ ìˆ˜ì„ ë§¤ë‹ˆì €(Manager)ë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ ê°€ì¥ ì í•©í•œ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ë°°ë¶„í•˜ë¼.
{ltm_context}
{case_context}

[Speculative Reasoning Rules]
ì‚¬ìš©ìì˜ ìš”ì²­ì´ ë³µì¡í•˜ê±°ë‚˜ í•´ê²° ë°©ë²•ì´ ì—¬ëŸ¬ ê°€ì§€ì¸ ê²½ìš°, 'swarm' ë…¸ë“œë¥¼ í†µí•´ ë³‘ë ¬ ê²€í† í•˜ë¼.

[Agent Factory Rules]
ë§Œì•½ í˜„ì¬ ê°€ìš©í•œ ì—ì´ì „íŠ¸(planner, researcher, analyst)ë¡œ ì²˜ë¦¬í•˜ê¸°ì— ì§€ë‚˜ì¹˜ê²Œ ì „ë¬¸í™”ëœ ì˜ì—­(ì˜ˆ: ì–‘ìì—­í•™ ë¶„ì„, íŠ¹ì • ê²Œì„ ì—”ì§„ íŠœë‹ ë“±)ì´ ë°˜ë³µì ìœ¼ë¡œ ìš”ì²­ëœë‹¤ë©´, ìƒˆë¡œìš´ ì „ë¬¸ ì—ì´ì „íŠ¸ì˜ ìƒì„±ì„ ê²°ì •í•˜ë¼. 
ì´ ê²½ìš° 'thought'ì— ì‚¬ìœ ë¥¼ ì ê³  'next_node'ë¥¼ 'planner'ë¡œ ì§€ì •í•˜ì—¬ ì‹ ê·œ ì—ì´ì „íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ê²Œ í•˜ë¼.

ì—ì´ì „íŠ¸ ì—­í• :
- planner: ì½”ë“œ ì‘ì„±, ë²„ê·¸ ìˆ˜ì •, ì—ì´ì „íŠ¸ ìê°€ ìƒì„±(Agent Factory) ë“± ëª¨ë“  ê°œë°œ ê´€ë ¨ ì‘ì—….
- researcher: ìµœì‹  ì •ë³´ ê²€ìƒ‰, ê¸°ìˆ  ì¡°ì‚¬.
- analyst: ë°ì´í„° ë¶„ì„, í”¼ë“œë°± ë¶„ì„.
- swarm: ë³‘ë ¬ ì¶”ë¡  ë° ë¶„ì‚° ì²˜ë¦¬.
"""

    # ìê°€ ì§„í™” ì—”ì§„ì—ì„œ í•™ìŠµëœ ê·œì¹™ì´ ìˆë‹¤ë©´ ì£¼ì…
    if state.get("active_constraints"):
        constraints_str = "\n".join([f"- {c}" for c in state["active_constraints"]])
        base_instruction += f"\n\n[USER-SPECIFIC EVOLVED RULES (MUST FOLLOW)]\n{constraints_str}"

    # ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ(Improvement Task)ì´ ìˆëŠ”ì§€ í™•ì¸
    system_improvement_msg = ""
    for msg in reversed(state["messages"]):
        content = msg.content if hasattr(msg, 'content') else str(msg)
        if "ìµœì í™” ì „ë¬¸ê°€ì˜ ì œì•ˆ:" in content:
            system_improvement_msg = content
            base_instruction += f"\n\n[SYSTEM OPTIMIZATION REQUEST (HIGH PRIORITY)]\n{system_improvement_msg}"
            base_instruction += "\nê²°ì •: í˜„ì¬ ì‹œìŠ¤í…œ ìµœì í™” ìš”ì²­ì´ ìˆìœ¼ë¯€ë¡œ, ë¬´ì¡°ê±´ 'next_node'ë¥¼ 'planner'ë¡œ ì§€ì •í•˜ë¼."
            break


    config = types.GenerateContentConfig(
        system_instruction=base_instruction + "\n\n[Thought Tree Rules]\nì‚¬ê³  ê³¼ì •ì„ ë…¼ë¦¬ì ì¸ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ì„¸ë¶„í™”í•˜ë¼. ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ë¶„ì„, íŒë‹¨, ê²°ë¡ ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë¼.\n\n[Self-Consistency Rules]\nìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ê¸° ì „, ë°˜ë“œì‹œ 'internal_critique' ë‹¨ê³„ì—ì„œ ìì‹ ì˜ ë…¼ë¦¬ì  ëª¨ìˆœì´ë‚˜ ìœ„í—˜ ìš”ì†Œë¥¼ ë¹„íŒì ìœ¼ë¡œ ì¬ê²€í† í•˜ë¼.",
        temperature=0.0,
        response_mime_type="application/json",
        response_schema={
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING", "description": "ì „ì²´ ì‚¬ê³  ìš”ì•½"},
                "internal_critique": {"type": "STRING", "description": "ìì‹ ì˜ ì¶”ë¡  ê³¼ì •ì— ëŒ€í•œ ë¹„íŒì  ì¬ê²€í† "},
                "thought_tree": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "id": {"type": "STRING"},
                            "parent_id": {"type": "STRING", "nullable": True},
                            "text": {"type": "STRING"},
                            "type": {"type": "STRING", "enum": ["analysis", "reasoning", "decision"]},
                            "priority": {"type": "INTEGER", "description": "1~5 (ë‚®ìŒ~ë†’ìŒ)"},
                            "certainty": {"type": "NUMBER", "description": "0.0~1.0 (í™•ì‹ ë„)"}
                        },
                        "required": ["id", "text", "type", "priority", "certainty"]
                    }
                },
                "next_node": {
                    "type": "STRING", 
                    "enum": ["planner", "researcher", "analyst", "swarm", "__end__"]
                },
                "parallel_tasks": {
                    "type": "ARRAY",
                    "items": {"type": "STRING"},
                    "description": "next_nodeê°€ 'swarm'ì¼ ë•Œ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  í•˜ìœ„ ì‘ì—… ë¦¬ìŠ¤íŠ¸"
                },
                "response_to_user": {"type": "STRING", "description": "ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ë‹µí•  ë‚´ìš©"}
            },
            "required": ["thought", "internal_critique", "thought_tree", "next_node"]
        }
    )

    # 2. Gemini í˜¸ì¶œì„ í†µí•œ ì˜ë„ ë¶„ì„ ë° ë¼ìš°íŒ… ê²°ì •
    # ìµœê·¼ API í˜¸ì¶œ ë¹ˆë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ (Adaptive Throttling)
    call_count = state.get("api_call_count", 0)
    if call_count > 10:
        model_id = "gemini-2.5-flash-lite"
        logger.warning(f"âš ï¸ High API usage ({call_count}). Throttling to {model_id}")
    else:
        # ì„¤ì •ëœ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        from gortex.core.config import GortexConfig
        model_id = GortexConfig().get("default_model", "gemini-1.5-flash")

    response = auth.generate(
        model_id=model_id,
        contents=state["messages"],
        config=config
    )


    # JSON ì‘ë‹µ íŒŒì‹±
    try:
        res_data = response.parsed if hasattr(response, 'parsed') else json.loads(response.text)
        
        logger.info(f"Manager Thought: {res_data.get('thought')}")
        logger.info(f"Critique: {res_data.get('internal_critique')}")
        
        updates = {
            "thought": res_data.get("thought"),
            "internal_critique": res_data.get("internal_critique"),
            "thought_tree": res_data.get("thought_tree"),
            "next_node": res_data.get("next_node", "__end__")
        }
        
        if res_data.get("parallel_tasks"):
            updates["plan"] = res_data["parallel_tasks"] # Swarmì„ ìœ„í•œ ì„ì‹œ ê³„íš ì£¼ì…
            logger.info(f"ğŸ“¦ Parallel tasks detected: {len(res_data['parallel_tasks'])} items.")

        
        # ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´ ì¶”ê°€
        if res_data.get("response_to_user"):
            updates["messages"] = [("ai", res_data["response_to_user"])]
            
        return updates

    except Exception as e:
        logger.error(f"Error parsing manager response: {e}")
        return {"next_node": "__end__", "messages": [("ai", "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")]}
