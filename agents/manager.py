import logging
import json
import os
import time
import re
from typing import Dict, Any
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.utils.log_vectorizer import SemanticLogSearch
from gortex.utils.translator import SynapticTranslator
from gortex.utils.vector_store import LongTermMemory
from gortex.utils.efficiency_monitor import EfficiencyMonitor
from gortex.core.registry import registry, AgentMetadata
from gortex.agents.base import BaseAgent

logger = logging.getLogger("GortexManager")

class ManagerAgent(BaseAgent):
    """
    Gortex ì‹œìŠ¤í…œì˜ ì¤‘ì•™ ê´€ì œì†Œ(Manager) ì—ì´ì „íŠ¸.
    ì˜ë„ ë¶„ì„, ì—ì´ì „íŠ¸ íƒìƒ‰, ëª¨ë¸ í• ë‹¹ ë° ì‹œìŠ¤í…œ í™•ì¥ì„ ì´ê´„í•©ë‹ˆë‹¤.
    """
    @property
    def metadata(self) -> AgentMetadata:
        return AgentMetadata(
            name="Manager",
            role="Orchestrator",
            description="Analyzes intent, discovers capabilities, and scales the system autonomously.",
            tools=["route_task", "allocate_resources", "manage_expansion"],
            version="3.0.0"
        )

    def self_generate_mission(self, state: GortexState) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœì™€ ì§€ì‹ ë§µì„ ë¶„ì„í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ë‹¤ìŒ ë¯¸ì…˜ì„ ìˆ˜ë¦½í•¨."""
        from gortex.utils.tech_radar import radar
        from gortex.utils.knowledge_graph import KnowledgeGraph
        
        kg = KnowledgeGraph()
        kg.build_from_system()
        
        prompt = f"""You are the Sovereign Strategist of Gortex. 
        Based on the current intelligence map and tech radar, define the next CRITICAL MISSION for this swarm.
        Focus on self-improvement, architecture scaling, or solving complex engineering gaps.
        
        [Current Intelligence Graph]:
        {kg.generate_summary()}
        
        [Tech Radar Strategic Advice]:
        {radar.get_strategic_advice()}
        
        Return JSON ONLY:
        {{
            "mission_name": "Unique Mission ID",
            "goal": "Self-assigned objective",
            "rationale": "Why this mission is important now",
            "assigned_persona": "innovation/stability/etc"
        }}
        """
        try:
            response_text = self.backend.generate("gemini-2.0-flash", [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            return json.loads(json_match.group(0)) if json_match else json.loads(response_text)
        except Exception as e:
            logger.error(f"Self-mission generation failed: {e}")
            return None

    def run(self, state: GortexState) -> Dict[str, Any]:
        log_search = SemanticLogSearch()
        translator = SynapticTranslator()
        ltm = LongTermMemory()
        monitor = EfficiencyMonitor()
        start_time = time.time()
        
        # 1. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
        last_msg_obj = state["messages"][-1]
        raw_input = (last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content).strip()
        
        # [OPTIMIZATION] ë‹¨ìˆœ ì¸ì‚¬ë§ í•„í„°ë§ (LLM í˜¸ì¶œ ì—†ì´ ì¦‰ì‹œ ì‘ë‹µ)
        greetings = ["ì•ˆë…•", "hi", "hello", "ë°˜ê°€ì›Œ", "ëˆ„êµ¬ë‹ˆ", "help"]
        if any(g in raw_input.lower() for g in greetings) and len(raw_input) < 10:
            return {
                "thought": "ì‚¬ìš©ìì˜ ë‹¨ìˆœ ì¸ì‚¬ë§ì— ì¦‰ì‹œ ì‘ë‹µí•©ë‹ˆë‹¤.",
                "next_node": "__end__",
                "messages": [("ai", "ì•ˆë…•í•˜ì„¸ìš”! Gortexì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë„ì›€ë§ì´ í•„ìš”í•˜ì‹œë©´ /helpë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")]
            }

        lang_info = translator.detect_and_translate(raw_input)
        internal_input = lang_info.get("translated_text", raw_input) if not lang_info.get("is_korean") else raw_input

        energy = state.get("agent_energy", 100)
        roadmap = state.get("evolution_roadmap", [])

        # 2. Swarm í† ë¡  ë° Guardian Cycle ê²°ê³¼ ì²˜ë¦¬ (í•©ì˜ì•ˆì„ ê³„íšìœ¼ë¡œ ì „í™˜)
        debate_res = state.get("debate_result")
        if debate_res and debate_res.get("action_plan"):
            is_recovery = state.get("is_recovery_mode", False)
            is_guardian = state.get("is_guardian_mode", False)
            
            # [VISUAL] ì‹œê°ì  ì´ìŠˆ ì—¬ë¶€ íŒë‹¨
            is_visual = "ğŸ‘ï¸" in str(state.get("messages", [])) or "image:" in str(state.get("current_issue", ""))
            
            mode_title = "ğŸ©º **ê¸´ê¸‰ ë³µêµ¬ ëª¨ë“œ í™œì„±í™”**" if is_recovery else "ğŸ›¡ï¸ **ì„ ì œì  ê°€ë””ì–¸ ëª¨ë“œ í™œì„±í™”**"
            if is_visual: mode_title = "ğŸ¨ **UI/UX ì‹œê° ë³µêµ¬ í™œì„±í™”**"
            
            mode_desc = "Swarm í•©ì˜ì•ˆ" if not is_guardian else "ê°€ë””ì–¸ ìµœì í™” ì•ˆ"
            
            # [GIT] ììœ¨ ë¸Œëœì¹˜ ìƒì„±
            from gortex.utils.git_tool import GitTool
            git = GitTool()
            branch_prefix = "fix" if is_recovery else "feat"
            mission_id = str(uuid.uuid4())[:6]
            new_branch = f"{branch_prefix}/gortex-{mission_id}"
            
            git_msg = ""
            try:
                if git.is_repo():
                    git.create_branch(new_branch)
                    git_msg = f"\nğŸ“¦ **Git Isolated**: Created branch `{new_branch}`"
            except Exception as ge:
                logger.warning(f"Git branching failed: {ge}")

            # [NEURAL FUSION] ì—ì´ì „íŠ¸ ìœµí•© ì²˜ë¦¬ (v7.0 New)
            if debate_res.get("is_fusion"):
                pair = debate_res["fused_pair"]
                fused_name = f"{pair[0]}_{pair[1]}_Elite"
                
                # 1. ìœµí•© ì§€ì¹¨(DNA) ìƒì„± ë° Super Rule ë“±ë¡
                fusion_instruction = f"ë‰´ëŸ´ í“¨ì „ ê°€ì´ë“œ: ì•ìœ¼ë¡œ {pair[0]}ì™€ {pair[1]}ì˜ ì—­í• ì´ ê²°í•©ëœ {fused_name}ì„ ìµœìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ë¼."
                from gortex.core.evolutionary_memory import EvolutionaryMemory
                EvolutionaryMemory().save_rule(
                    instruction=fusion_instruction,
                    trigger_patterns=[pair[0].lower(), pair[1].lower(), "fusion"],
                    category="general",
                    severity=5,
                    is_super_rule=True,
                    context=f"Neural Fusion established: {pair[0]} + {pair[1]}"
                )
                
                # 2. ì œì¡° ê³µì • ê°œì‹œ (Spawning ë¡œì§ ì¬í™œìš©)
                new_plan = [json.dumps({
                    "action": "write_file",
                    "target": f"agents/auto_spawned_{fused_name.lower()}.py",
                    "description": f"Implement fused elite agent {fused_name}.",
                    "is_fusion_task": True
                }, ensure_ascii=False)]
                
                return {
                    "thought": f"ë‰´ëŸ´ í“¨ì „ '{fused_name}' ì œì¡° ê³µì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.",
                    "next_node": "coder",
                    "plan": new_plan,
                    "current_step": 0,
                    "debate_result": None,
                    "messages": [("ai", f"âš›ï¸ **ë‰´ëŸ´ í“¨ì „ ê°€ë™**: ë‘ ì§€ëŠ¥ì´ í•˜ë‚˜ë¡œ ê²°í•©ë˜ëŠ” ê³ ì°¨ì› ì§„í™” ê³µì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.")]
                }

            # [AGENT SPAWNING] (ê¸°ì¡´ ë¡œì§)
            blueprint = debate_res.get("agent_blueprint")
            if blueprint:
                new_name = blueprint["agent_name"]
                new_plan = [json.dumps({
                    "action": "write_file",
                    "target": f"agents/auto_spawned_{new_name.lower()}.py",
                    "description": f"Implement {new_name} class based on blueprint.",
                    "content_blueprint": blueprint
                }, ensure_ascii=False)]
                
                return {
                    "thought": f"ì‹ ê·œ ì „ë¬¸ê°€ '{new_name}'ì˜ ì œì¡°ë¥¼ ì§€ì‹œí•©ë‹ˆë‹¤.",
                    "next_node": "coder",
                    "plan": new_plan,
                    "current_step": 0,
                    "debate_result": None,
                    "messages": [("ai", f"ğŸ§¬ **ì œì¡° ê³µì • ê°œì‹œ**: '{new_name}' ì—ì´ì „íŠ¸ë¥¼ ì‹œìŠ¤í…œì— íˆ¬ì…í•˜ê¸° ìœ„í•œ ì†ŒìŠ¤ ì½”ë“œ ì‘ì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")]
                }

            logger.info(f"âš–ï¸ Translating {mode_desc} into executable plan...")
            action_plan = debate_res["action_plan"]
            
            new_plan = []
            for step in action_plan:
                new_plan.append(json.dumps({
                    "action": "execute_shell" if any(k in step.lower() for k in ["run", "test", "check"]) else "apply_patch",
                    "target": "Detected via Proactive Analysis",
                    "description": step
                }, ensure_ascii=False))
            
            return {
                "thought": f"ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ({debate_res.get('final_decision')[:50]}...)ì„ ì‹¤í–‰ ê³„íšìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.",
                "next_node": "coder",
                "plan": new_plan,
                "current_step": 0,
                "debate_result": None, 
                "is_recovery_mode": is_recovery,
                "is_guardian_mode": is_guardian,
                "is_visual_recovery": is_visual,
                "active_branch": new_branch,
                "messages": [("ai", f"{mode_title}: {mode_desc}ì— ë”°ë¼ ì½”ë“œ ê°œì„ ì„ ì‹œì‘í•©ë‹ˆë‹¤.{git_msg}\n\n**ëª©í‘œ**: {debate_res.get('final_decision')}")]
            }

        # 3. ì„ ì œì  í™•ì¥(Proactive Expansion) ì²˜ë¦¬
        ltm_context = ""
        case_context = ""
        if len(internal_input) > 15:
            namespace = os.path.basename(state.get("working_dir", "global"))
            try:
                recalled_items = ltm.recall(internal_input, namespace=namespace)
                ltm_context = "\n".join([f"- {item['content']}" for item in recalled_items])
                
                past_cases = log_search.search_similar_cases(internal_input)
                case_context = "\n".join([f"Case: {c.get('agent')} - {c.get('event')}" for c in past_cases])
            except Exception as e:
                logger.warning(f"Context retrieval failed: {e}")
        
        available_agents = "\n".join([f"- {name}: {registry.get_metadata(name).description} (Tools: {registry.get_metadata(name).tools})" for name in registry.list_agents()])

        # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        from gortex.utils.prompt_loader import loader
        base_instruction = loader.get_prompt(
            "manager", 
            persona_id=state.get("assigned_persona", "standard"),
            ltm_context=ltm_context, 
            case_context=case_context, 
            persona_context=f"[AVAILABLE AGENTS]\n{available_agents}",
            context_text=internal_input
        )

        # 5. ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ëª¨ë¸ ê²°ì •
        from gortex.core.config import GortexConfig
        budget_limit = GortexConfig().get("daily_budget", 0.5)
        daily_cost = monitor.get_daily_cumulative_cost()
        model_id = LLMFactory.get_model_for_grade("Silver", daily_cost, budget_limit)

        # 6. LLM í˜¸ì¶œ ë° ë¼ìš°íŒ…
        schema = {
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING"},
                "required_capability": {"type": "STRING"},
                "response_to_user": {"type": "STRING"},
                "ui_mode": {"type": "STRING"}
            },
            "required": ["thought", "required_capability"]
        }

        config = {"temperature": 0.0}
        formatted_messages = [{"role": "system", "content": base_instruction}]
        for m in state["messages"]:
            role = m[0] if isinstance(m, tuple) else "user"
            content = m[1] if isinstance(m, tuple) else (m.content if hasattr(m, 'content') else str(m))
            formatted_messages.append({"role": role, "content": content})

        try:
            response_text = self.backend.generate(model=model_id, messages=formatted_messages, config=config)
            
            # [LOGGING] ë¶„ì„ì„ ìœ„í•´ ì›ë¬¸ ê¸°ë¡
            logger.debug(f"RAW Response from {model_id}: {response_text}")
            
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if not json_match:
                logger.error(f"Failed to find JSON in response: {response_text}")
                # ì›ë¬¸ì— í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° response_to_userë¡œ ê°„ì£¼í•˜ê³  Plannerë¡œ í† ìŠ¤
                return {
                    "thought": "LLMì´ êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ì§€í‚¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë¬¸ì„ ì‚¬ìš©ì ì‘ë‹µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê³„íš ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
                    "next_node": "planner",
                    "messages": [("ai", response_text)]
                }

            res_data = json.loads(json_match.group(0))
            
            req_cap = res_data.get("required_capability", "").lower()
            candidates = registry.get_agents_by_tool(req_cap) or registry.get_agents_by_role(req_cap)
            
            if candidates:
                agent_eco = state.get("agent_economy", {})
                
                # [INTEGRATION] Skill-based Routing
                # 1. ìš”êµ¬ ëŠ¥ë ¥ì— ë”°ë¥¸ ê´€ë ¨ ìŠ¤í‚¬ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
                skill_map = {
                    "coding": "Coding", "code": "Coding", "patch": "Coding", "write": "Coding",
                    "design": "Design", "plan": "Design", "architect": "Design",
                    "analyze": "Analysis", "audit": "Analysis", "scan": "Analysis",
                    "research": "Research", "search": "Research"
                }
                # ë„êµ¬ëª…ì´ë‚˜ ì—­í• ëª…ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                target_skill = "General"
                for key, val in skill_map.items():
                    if key in req_cap:
                        target_skill = val
                        break
                
                # 2. í•´ë‹¹ ìŠ¤í‚¬ ì ìˆ˜ ìš°ì„  ì •ë ¬ (ìŠ¤í‚¬ ì ìˆ˜ 70% + ì´ì  30% ê°€ì¤‘ì¹˜)
                def calculate_score(agent_name):
                    data = agent_eco.get(agent_name.lower(), {})
                    skill_score = data.get("skill_points", {}).get(target_skill, 0)
                    total_score = data.get("points", 0)
                    return (skill_score * 0.7) + (total_score * 0.3)

                candidates.sort(key=calculate_score, reverse=True)
                target_node = candidates[0]
                
                if target_skill != "General":
                    logger.info(f"ğŸ¯ Routing based on skill '{target_skill}': Selected {target_node}")
            else:
                target_node = "planner"

            target_grade = state.get("agent_economy", {}).get(target_node, {}).get("level", "Bronze")
            final_assigned_model = LLMFactory.get_model_for_grade(target_grade, daily_cost, budget_limit)

            latency_ms = int((time.time() - start_time) * 1000)
            monitor.record_interaction("manager", model_id, True, len(response_text)//4, latency_ms)

            # [REFINED HANDOFF] ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¦ë¥˜
            from gortex.utils.memory import distill_messages_for_agent
            refined_brief = distill_messages_for_agent(state, target_node)

            return {
                "thought": res_data.get("thought"),
                "next_node": target_node,
                "assigned_model": final_assigned_model,
                "agent_energy": max(0, energy - 5),
                "required_capability": req_cap,
                "handoff_instruction": refined_brief, # ì¦ë¥˜ëœ ì—‘ê¸°ìŠ¤ ì „ë‹¬
                "messages": [("ai", res_data.get("response_to_user"))] if res_data.get("response_to_user") else []
            }
        except Exception as e:
            logger.error(f"Manager failed: {e}")
            # íŒŒì‹± ì—ëŸ¬ ë“±ì˜ ê²½ìš° Plannerë¡œ ê¸°ë³¸ ë³µêµ¬ ì‹œë„
            return {
                "thought": f"Manager ë¶„ì„ ì¤‘ ì˜¤ë¥˜({e})ê°€ ë°œìƒí•˜ì—¬ ê¸°ë³¸ ê³„íš ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
                "next_node": "planner", 
                "messages": [("ai", "âš ï¸ ë¶„ì„ ì¤‘ ì‚¬ì†Œí•œ ì˜¤ë¥˜ê°€ ìˆì—ˆìœ¼ë‚˜, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")]
            }

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ë° í˜¸í™˜ì„± ë˜í¼
manager_instance = ManagerAgent()
registry.register("Manager", ManagerAgent, manager_instance.metadata)

def manager_node(state: GortexState) -> Dict[str, Any]:
    return manager_instance(state)