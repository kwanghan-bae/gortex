import logging
import json
import os
import time
import re
from typing import Dict, List, Any, Optional
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.core.evolutionary_memory import EvolutionaryMemory
from gortex.utils.log_vectorizer import SemanticLogSearch
from gortex.utils.translator import SynapticTranslator
from gortex.utils.vector_store import LongTermMemory
from gortex.utils.efficiency_monitor import EfficiencyMonitor
from gortex.core.registry import registry, AgentMetadata
from gortex.agents.base import BaseAgent

logger = logging.getLogger("GortexManager")

class ManagerAgent(BaseAgent):
    """
    Gortex ì‹œìŠ¤í…œì˜ ì¤‘ì•™ ê´€ì œì†Œ(Manager) ì—ì´ì „íŠ¸.
    ì˜ë„ ë¶„ì„, ì—ì´ì „íŠ¸ íƒìƒ‰, ëª¨ë¸ í• ë‹¹ ë° ì‹œìŠ¤í…œ í™•ì¥ì„ ì´ê´„í•©ë‹ˆë‹¤.
    """
    @property
    def metadata(self) -> AgentMetadata:
        return AgentMetadata(
            name="Manager",
            role="Orchestrator",
            description="Analyzes intent, discovers capabilities, and scales the system autonomously.",
            tools=["route_task", "allocate_resources", "manage_expansion"],
            version="3.0.0"
        )

    def run(self, state: GortexState) -> Dict[str, Any]:
        log_search = SemanticLogSearch()
        translator = SynapticTranslator()
        ltm = LongTermMemory()
        monitor = EfficiencyMonitor()
        start_time = time.time()
        
        # 1. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
        last_msg_obj = state["messages"][-1]
        raw_input = last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content
        lang_info = translator.detect_and_translate(raw_input)
        internal_input = lang_info.get("translated_text", raw_input) if not lang_info.get("is_korean") else raw_input

        energy = state.get("agent_energy", 100)
        roadmap = state.get("evolution_roadmap", [])

        # 2. ì„ ì œì  í™•ì¥(Proactive Expansion) ì²˜ë¦¬
        # TrendScout ë“±ìœ¼ë¡œë¶€í„° ì—ì´ì „íŠ¸ í™•ì¥ ì œì•ˆì´ ì˜¨ ê²½ìš°
        agent_proposals = state.get("agent_proposals", [])
        if agent_proposals:
            logger.info(f"âš¡ Proactive expansion proposal detected: {agent_proposals[0]['agent_name']}")
            # Analystì—ê²Œ ë„˜ê²¨ íƒ€ë‹¹ì„± ê²€í† (identify_capability_gapê³¼ ìœ ì‚¬í•œ íë¦„) í›„ Coderì—ê²Œ ì œì¡° ì§€ì‹œ
            return {
                "thought": f"TrendScoutì˜ ì‹ ê·œ ì—ì´ì „íŠ¸ '{agent_proposals[0]['agent_name']}' ì˜ì… ì œì•ˆì„ ë¶„ì„í•©ë‹ˆë‹¤.",
                "next_node": "analyst",
                "required_capability": "capability_gap_analysis",
                "handoff_instruction": f"ë‹¤ìŒ ì—ì´ì „íŠ¸ ì œì•ˆì˜ íƒ€ë‹¹ì„±ì„ ê²€í† í•˜ë¼: {json.dumps(agent_proposals[0], ensure_ascii=False)}",
                "messages": [("ai", f"ğŸš€ **ì‹œìŠ¤í…œ í™•ì¥ ê°ì§€**: '{agent_proposals[0]['agent_name']}' ì „ë¬¸ê°€ ì˜ì…ì„ ìœ„í•œ íƒ€ë‹¹ì„± ê²€í† ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")]
            }

        # 3. ë§¥ë½ ì •ë³´ ìˆ˜ì§‘
        namespace = os.path.basename(state.get("working_dir", "global"))
        recalled_items = ltm.recall(internal_input, namespace=namespace)
        ltm_context = "\n".join([f"- {item['content']}" for item in recalled_items])
        
        past_cases = log_search.search_similar_cases(internal_input)
        case_context = "\n".join([f"Case: {c.get('agent')} - {c.get('event')}" for c in past_cases])
        
        available_agents = "\n".join([f"- {name}: {registry.get_metadata(name).description} (Tools: {registry.get_metadata(name).tools})" for name in registry.list_agents()])

        # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        from gortex.utils.prompt_loader import loader
        base_instruction = loader.get_prompt(
            "manager", 
            persona_id=state.get("assigned_persona", "standard"),
            ltm_context=ltm_context, 
            case_context=case_context, 
            persona_context=f"[AVAILABLE AGENTS]\n{available_agents}",
            context_text=internal_input
        )

        # 5. ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ëª¨ë¸ ê²°ì •
        from gortex.core.config import GortexConfig
        budget_limit = GortexConfig().get("daily_budget", 0.5)
        daily_cost = monitor.get_daily_cumulative_cost()
        model_id = LLMFactory.get_model_for_grade("Silver", daily_cost, budget_limit)

        # 6. LLM í˜¸ì¶œ ë° ë¼ìš°íŒ…
        schema = {
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING"},
                "required_capability": {"type": "STRING"},
                "response_to_user": {"type": "STRING"},
                "ui_mode": {"type": "STRING"}
            },
            "required": ["thought", "required_capability"]
        }

        config = {"temperature": 0.0}
        formatted_messages = [{"role": "system", "content": base_instruction}]
        for m in state["messages"]:
            role = m[0] if isinstance(m, tuple) else "user"
            content = m[1] if isinstance(m, tuple) else (m.content if hasattr(m, 'content') else str(m))
            formatted_messages.append({"role": role, "content": content})

        try:
            response_text = self.backend.generate(model=model_id, messages=formatted_messages, config=config)
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            res_data = json.loads(json_match.group(0)) if json_match else json.loads(response_text)
            
            req_cap = res_data.get("required_capability", "").lower()
            candidates = registry.get_agents_by_tool(req_cap) or registry.get_agents_by_role(req_cap)
            
            if candidates:
                agent_eco = state.get("agent_economy", {})
                
                # [INTEGRATION] Skill-based Routing
                # 1. ìš”êµ¬ ëŠ¥ë ¥ì— ë”°ë¥¸ ê´€ë ¨ ìŠ¤í‚¬ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
                skill_map = {
                    "coding": "Coding", "code": "Coding", "patch": "Coding", "write": "Coding",
                    "design": "Design", "plan": "Design", "architect": "Design",
                    "analyze": "Analysis", "audit": "Analysis", "scan": "Analysis",
                    "research": "Research", "search": "Research"
                }
                # ë„êµ¬ëª…ì´ë‚˜ ì—­í• ëª…ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                target_skill = "General"
                for key, val in skill_map.items():
                    if key in req_cap:
                        target_skill = val
                        break
                
                # 2. í•´ë‹¹ ìŠ¤í‚¬ ì ìˆ˜ ìš°ì„  ì •ë ¬ (ìŠ¤í‚¬ ì ìˆ˜ 70% + ì´ì  30% ê°€ì¤‘ì¹˜)
                def calculate_score(agent_name):
                    data = agent_eco.get(agent_name.lower(), {})
                    skill_score = data.get("skill_points", {}).get(target_skill, 0)
                    total_score = data.get("points", 0)
                    return (skill_score * 0.7) + (total_score * 0.3)

                candidates.sort(key=calculate_score, reverse=True)
                target_node = candidates[0]
                
                if target_skill != "General":
                    logger.info(f"ğŸ¯ Routing based on skill '{target_skill}': Selected {target_node}")
            else:
                target_node = "planner"

            target_grade = state.get("agent_economy", {}).get(target_node, {}).get("level", "Bronze")
            final_assigned_model = LLMFactory.get_model_for_grade(target_grade, daily_cost, budget_limit)

            latency_ms = int((time.time() - start_time) * 1000)
            monitor.record_interaction("manager", model_id, True, len(response_text)//4, latency_ms)

            return {
                "thought": res_data.get("thought"),
                "next_node": target_node,
                "assigned_model": final_assigned_model,
                "agent_energy": max(0, energy - 5),
                "required_capability": req_cap,
                "messages": [("ai", res_data.get("response_to_user"))] if res_data.get("response_to_user") else []
            }
        except Exception as e:
            logger.error(f"Manager failed: {e}")
            return {"next_node": "__end__", "messages": [("ai", f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")]}

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ë° í˜¸í™˜ì„± ë˜í¼
manager_instance = ManagerAgent()
registry.register("Manager", ManagerAgent, manager_instance.metadata)

def manager_node(state: GortexState) -> Dict[str, Any]:
    return manager_instance(state)