import logging
import json
import os
import time
import re
from typing import Dict, Any
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.utils.log_vectorizer import SemanticLogSearch
from gortex.utils.translator import SynapticTranslator
from gortex.utils.vector_store import LongTermMemory
from gortex.utils.efficiency_monitor import EfficiencyMonitor
from gortex.core.registry import registry, AgentMetadata
from gortex.agents.base import BaseAgent

logger = logging.getLogger("GortexManager")

class ManagerAgent(BaseAgent):
    """
    Gortex ì‹œìŠ¤í…œì˜ ì¤‘ì•™ ê´€ì œì†Œ(Manager) ì—ì´ì „íŠ¸.
    ì˜ë„ ë¶„ì„, ì—ì´ì „íŠ¸ íƒìƒ‰, ëª¨ë¸ í• ë‹¹ ë° ì‹œìŠ¤í…œ í™•ì¥ì„ ì´ê´„í•©ë‹ˆë‹¤.
    """
    @property
    def metadata(self) -> AgentMetadata:
        return AgentMetadata(
            name="Manager",
            role="Orchestrator",
            description="Analyzes intent, discovers capabilities, and scales the system autonomously.",
            tools=["route_task", "allocate_resources", "manage_expansion"],
            version="3.0.0"
        )

    def run(self, state: GortexState) -> Dict[str, Any]:
        log_search = SemanticLogSearch()
        translator = SynapticTranslator()
        ltm = LongTermMemory()
        monitor = EfficiencyMonitor()
        start_time = time.time()
        
        # 1. ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
        last_msg_obj = state["messages"][-1]
        raw_input = (last_msg_obj[1] if isinstance(last_msg_obj, tuple) else last_msg_obj.content).strip()
        
        # [OPTIMIZATION] ë‹¨ìˆœ ì¸ì‚¬ë§ í•„í„°ë§ (LLM í˜¸ì¶œ ì—†ì´ ì¦‰ì‹œ ì‘ë‹µ)
        greetings = ["ì•ˆë…•", "hi", "hello", "ë°˜ê°€ì›Œ", "ëˆ„êµ¬ë‹ˆ", "help"]
        if any(g in raw_input.lower() for g in greetings) and len(raw_input) < 10:
            return {
                "thought": "ì‚¬ìš©ìì˜ ë‹¨ìˆœ ì¸ì‚¬ë§ì— ì¦‰ì‹œ ì‘ë‹µí•©ë‹ˆë‹¤.",
                "next_node": "__end__",
                "messages": [("ai", "ì•ˆë…•í•˜ì„¸ìš”! Gortexì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë„ì›€ë§ì´ í•„ìš”í•˜ì‹œë©´ /helpë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")]
            }

        lang_info = translator.detect_and_translate(raw_input)
        internal_input = lang_info.get("translated_text", raw_input) if not lang_info.get("is_korean") else raw_input

        energy = state.get("agent_energy", 100)
        roadmap = state.get("evolution_roadmap", [])

        # 2. Swarm í† ë¡  ë° Guardian Cycle ê²°ê³¼ ì²˜ë¦¬ (í•©ì˜ì•ˆì„ ê³„íšìœ¼ë¡œ ì „í™˜)
        debate_res = state.get("debate_result")
        if debate_res and debate_res.get("action_plan"):
            is_recovery = state.get("is_recovery_mode", False)
            is_guardian = state.get("is_guardian_mode", False)
            
            mode_title = "ğŸ©º **ê¸´ê¸‰ ë³µêµ¬ ëª¨ë“œ í™œì„±í™”**" if is_recovery else "ğŸ›¡ï¸ **ì„ ì œì  ê°€ë””ì–¸ ëª¨ë“œ í™œì„±í™”**"
            mode_desc = "Swarm í•©ì˜ì•ˆ" if not is_guardian else "ê°€ë””ì–¸ ìµœì í™” ì•ˆ"
            
            logger.info(f"âš–ï¸ Translating {mode_desc} into executable plan...")
            action_plan = debate_res["action_plan"]
            
            new_plan = []
            for step in action_plan:
                new_plan.append(json.dumps({
                    "action": "execute_shell" if any(k in step.lower() for k in ["run", "test", "check"]) else "apply_patch",
                    "target": "Detected via Proactive Analysis",
                    "description": step
                }, ensure_ascii=False))
            
            return {
                "thought": f"ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ({debate_res.get('final_decision')[:50]}...)ì„ ì‹¤í–‰ ê³„íšìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.",
                "next_node": "coder",
                "plan": new_plan,
                "current_step": 0,
                "debate_result": None, 
                "is_recovery_mode": is_recovery,
                "is_guardian_mode": is_guardian,
                "messages": [("ai", f"{mode_title}: {mode_desc}ì— ë”°ë¼ ì½”ë“œ ê°œì„ ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n\n**ëª©í‘œ**: {debate_res.get('final_decision')}")]
            }

        # 3. ì„ ì œì  í™•ì¥(Proactive Expansion) ì²˜ë¦¬
        ltm_context = ""
        case_context = ""
        if len(internal_input) > 15:
            namespace = os.path.basename(state.get("working_dir", "global"))
            try:
                recalled_items = ltm.recall(internal_input, namespace=namespace)
                ltm_context = "\n".join([f"- {item['content']}" for item in recalled_items])
                
                past_cases = log_search.search_similar_cases(internal_input)
                case_context = "\n".join([f"Case: {c.get('agent')} - {c.get('event')}" for c in past_cases])
            except Exception as e:
                logger.warning(f"Context retrieval failed: {e}")
        
        available_agents = "\n".join([f"- {name}: {registry.get_metadata(name).description} (Tools: {registry.get_metadata(name).tools})" for name in registry.list_agents()])

        # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        from gortex.utils.prompt_loader import loader
        base_instruction = loader.get_prompt(
            "manager", 
            persona_id=state.get("assigned_persona", "standard"),
            ltm_context=ltm_context, 
            case_context=case_context, 
            persona_context=f"[AVAILABLE AGENTS]\n{available_agents}",
            context_text=internal_input
        )

        # 5. ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ëª¨ë¸ ê²°ì •
        from gortex.core.config import GortexConfig
        budget_limit = GortexConfig().get("daily_budget", 0.5)
        daily_cost = monitor.get_daily_cumulative_cost()
        model_id = LLMFactory.get_model_for_grade("Silver", daily_cost, budget_limit)

        # 6. LLM í˜¸ì¶œ ë° ë¼ìš°íŒ…
        schema = {
            "type": "OBJECT",
            "properties": {
                "thought": {"type": "STRING"},
                "required_capability": {"type": "STRING"},
                "response_to_user": {"type": "STRING"},
                "ui_mode": {"type": "STRING"}
            },
            "required": ["thought", "required_capability"]
        }

        config = {"temperature": 0.0}
        formatted_messages = [{"role": "system", "content": base_instruction}]
        for m in state["messages"]:
            role = m[0] if isinstance(m, tuple) else "user"
            content = m[1] if isinstance(m, tuple) else (m.content if hasattr(m, 'content') else str(m))
            formatted_messages.append({"role": role, "content": content})

        try:
            response_text = self.backend.generate(model=model_id, messages=formatted_messages, config=config)
            
            # [LOGGING] ë¶„ì„ì„ ìœ„í•´ ì›ë¬¸ ê¸°ë¡
            logger.debug(f"RAW Response from {model_id}: {response_text}")
            
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if not json_match:
                logger.error(f"Failed to find JSON in response: {response_text}")
                # ì›ë¬¸ì— í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° response_to_userë¡œ ê°„ì£¼í•˜ê³  Plannerë¡œ í† ìŠ¤
                return {
                    "thought": "LLMì´ êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ì§€í‚¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë¬¸ì„ ì‚¬ìš©ì ì‘ë‹µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê³„íš ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
                    "next_node": "planner",
                    "messages": [("ai", response_text)]
                }

            res_data = json.loads(json_match.group(0))
            
            req_cap = res_data.get("required_capability", "").lower()
            candidates = registry.get_agents_by_tool(req_cap) or registry.get_agents_by_role(req_cap)
            
            if candidates:
                agent_eco = state.get("agent_economy", {})
                
                # [INTEGRATION] Skill-based Routing
                # 1. ìš”êµ¬ ëŠ¥ë ¥ì— ë”°ë¥¸ ê´€ë ¨ ìŠ¤í‚¬ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
                skill_map = {
                    "coding": "Coding", "code": "Coding", "patch": "Coding", "write": "Coding",
                    "design": "Design", "plan": "Design", "architect": "Design",
                    "analyze": "Analysis", "audit": "Analysis", "scan": "Analysis",
                    "research": "Research", "search": "Research"
                }
                # ë„êµ¬ëª…ì´ë‚˜ ì—­í• ëª…ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                target_skill = "General"
                for key, val in skill_map.items():
                    if key in req_cap:
                        target_skill = val
                        break
                
                # 2. í•´ë‹¹ ìŠ¤í‚¬ ì ìˆ˜ ìš°ì„  ì •ë ¬ (ìŠ¤í‚¬ ì ìˆ˜ 70% + ì´ì  30% ê°€ì¤‘ì¹˜)
                def calculate_score(agent_name):
                    data = agent_eco.get(agent_name.lower(), {})
                    skill_score = data.get("skill_points", {}).get(target_skill, 0)
                    total_score = data.get("points", 0)
                    return (skill_score * 0.7) + (total_score * 0.3)

                candidates.sort(key=calculate_score, reverse=True)
                target_node = candidates[0]
                
                if target_skill != "General":
                    logger.info(f"ğŸ¯ Routing based on skill '{target_skill}': Selected {target_node}")
            else:
                target_node = "planner"

            target_grade = state.get("agent_economy", {}).get(target_node, {}).get("level", "Bronze")
            final_assigned_model = LLMFactory.get_model_for_grade(target_grade, daily_cost, budget_limit)

            latency_ms = int((time.time() - start_time) * 1000)
            monitor.record_interaction("manager", model_id, True, len(response_text)//4, latency_ms)

            return {
                "thought": res_data.get("thought"),
                "next_node": target_node,
                "assigned_model": final_assigned_model,
                "agent_energy": max(0, energy - 5),
                "required_capability": req_cap,
                "messages": [("ai", res_data.get("response_to_user"))] if res_data.get("response_to_user") else []
            }
        except Exception as e:
            logger.error(f"Manager failed: {e}")
            # íŒŒì‹± ì—ëŸ¬ ë“±ì˜ ê²½ìš° Plannerë¡œ ê¸°ë³¸ ë³µêµ¬ ì‹œë„
            return {
                "thought": f"Manager ë¶„ì„ ì¤‘ ì˜¤ë¥˜({e})ê°€ ë°œìƒí•˜ì—¬ ê¸°ë³¸ ê³„íš ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
                "next_node": "planner", 
                "messages": [("ai", "âš ï¸ ë¶„ì„ ì¤‘ ì‚¬ì†Œí•œ ì˜¤ë¥˜ê°€ ìˆì—ˆìœ¼ë‚˜, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")]
            }

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ë° í˜¸í™˜ì„± ë˜í¼
manager_instance = ManagerAgent()
registry.register("Manager", ManagerAgent, manager_instance.metadata)

def manager_node(state: GortexState) -> Dict[str, Any]:
    return manager_instance(state)