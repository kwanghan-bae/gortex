import json
import os
import logging
import re
from datetime import datetime
from typing import Dict, Any, List
from gortex.core.state import GortexState
from gortex.core.llm.factory import LLMFactory
from gortex.utils.tools import read_file, write_file, execute_shell
from gortex.utils.efficiency_monitor import EfficiencyMonitor

logger = logging.getLogger("GortexEvolution")

class EvolutionNode:
    """
    Gortexì˜ ìê°€ ì§„í™” ì—”ì§„.
    ì‹œìŠ¤í…œì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ìŠ¤ìŠ¤ë¡œ ë¦¬íŒ©í† ë§í•˜ê³  ì‹ ê¸°ìˆ ì„ ë„ì…í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        self.backend = LLMFactory.get_default_backend()
        self.monitor = EfficiencyMonitor()

    def _get_radar_candidates(self) -> List[Dict[str, Any]]:
        """Tech Radarì—ì„œ ë„ì… í›„ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if os.path.exists("tech_radar.json"):
            try:
                with open("tech_radar.json", "r", encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("adoption_candidates", [])
            except:
                pass
        return []

    def prepare_fine_tuning_job(self, dataset_path: str = "logs/datasets/evolution.jsonl") -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ëœ ì§„í™” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Fine-tuning ì‘ì—…(Job)ì„ íŒ¨í‚¤ì§•í•©ë‹ˆë‹¤.
        ë°ì´í„° ê²€ì¦, ë³€í™˜, ì„¤ì • íŒŒì¼ ìƒì„±ì„ í¬í•¨í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(dataset_path):
            return {"status": "failed", "reason": f"Dataset not found: {dataset_path}"}
            
        try:
            # 1. Load and Validate Data
            valid_data = []
            with open(dataset_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        entry = json.loads(line)
                        if "messages" in entry and isinstance(entry["messages"], list):
                            valid_data.append(entry)
                    except: continue
            
            if not valid_data:
                return {"status": "failed", "reason": "No valid data found in dataset"}
                
            # 2. Create Job Directory
            job_id = datetime.now().strftime("%Y%m%d_%H%M%S")
            job_dir = f"training_jobs/job_{job_id}"
            os.makedirs(job_dir, exist_ok=True)
            
            # 3. Save Processed Dataset (ShareGPT/Chat format)
            # ì—¬ê¸°ì„œëŠ” JSONLì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, í•˜ë‚˜ì˜ JSON ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            output_dataset = os.path.join(job_dir, "dataset.json")
            with open(output_dataset, "w", encoding="utf-8") as f:
                json.dump(valid_data, f, indent=2, ensure_ascii=False)
                
            # 4. Copy/Template Config
            config_template = "config/training.yaml"
            job_config = os.path.join(job_dir, "config.yaml")
            
            if os.path.exists(config_template):
                with open(config_template, "r", encoding="utf-8") as f:
                    config_content = f.read()
            else:
                # Fallback config
                config_content = "model: unsloth/llama-3-8b-bnb-4bit\nlora_r: 16\n"
            
            with open(job_config, "w", encoding="utf-8") as f:
                f.write(f"# Job ID: {job_id}\n# Source: {dataset_path}\n\n{config_content}")
                
            # 5. Create Metadata
            meta = {
                "job_id": job_id,
                "created_at": datetime.now().isoformat(),
                "data_count": len(valid_data),
                "source_file": dataset_path,
                "status": "ready"
            }
            with open(os.path.join(job_dir, "meta.json"), "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2)
                
            logger.info(f"ğŸ“¦ Fine-tuning job prepared: {job_dir} ({len(valid_data)} items)")
            return {
                "status": "success", 
                "job_dir": job_dir, 
                "item_count": len(valid_data)
            }
            
        except Exception as e:
            logger.error(f"Failed to prepare fine-tuning job: {e}")
            return {"status": "error", "reason": str(e)}

    def heal_architecture(self, state: GortexState, violations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë°œê²¬ëœ ì•„í‚¤í…ì²˜ ìœ„ë°˜ ì‚¬í•­(Layer Violation ë“±)ì„ ìë™ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤."""
        if not violations:
            return {"thought": "ìˆ˜ì •í•  ì•„í‚¤í…ì²˜ ìœ„ë°˜ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.", "next_node": "manager"}

        # ê°€ì¥ ì‹¬ê°í•œ ìœ„ë°˜ ë˜ëŠ” ì²« ë²ˆì§¸ ìœ„ë°˜ ì„ íƒ
        v = violations[0]
        source_mod = v["source"]
        target_mod = v["target"]
        reason = v["reason"]
        
        # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        source_file = source_mod.replace(".", "/") + ".py"
        if not os.path.exists(source_file):
            # gortex prefix ì œê±° ì‹œë„
            source_file = source_file.replace("gortex/", "")
            
        if not os.path.exists(source_file):
            return {"thought": f"ìˆ˜ì • ëŒ€ìƒ íŒŒì¼ {source_file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "next_node": "manager"}

        original_code = read_file(source_file)
        
        prompt = f"""ë„ˆëŠ” Gortexì˜ ì•„í‚¤í…ì²˜ ìˆ˜í˜¸ìë‹¤. 
ë‹¤ìŒ ì•„í‚¤í…ì²˜ ìœ„ë°˜ ì‚¬í•­ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì½”ë“œë¥¼ ë¦¬íŒ©í† ë§í•˜ë¼.

[ìœ„ë°˜ ë‚´ìš©] {reason}
[ìœ„ë°˜ ê²½ë¡œ] {source_mod} -> {target_mod}
[ìˆ˜ì • íŒŒì¼] {source_file}

ì£¼ë¡œ ìƒìœ„ ë ˆì´ì–´ì˜ ê¸°ëŠ¥ì„ í•˜ìœ„ ë ˆì´ì–´ì—ì„œ ì§ì ‘ ì°¸ì¡°í•  ë•Œ ë°œìƒí•œë‹¤.
í•´ê²° ì „ëµ: 
1. ìƒìœ„ ë ˆì´ì–´ì˜ ê¸°ëŠ¥ì„ ì¶”ìƒí™”(Interface/Base Class)í•˜ì—¬ í•˜ìœ„ ë ˆì´ì–´ë¡œ ì˜®ê¸´ë‹¤.
2. ë˜ëŠ” í•˜ìœ„ ë ˆì´ì–´ì—ì„œ ìƒìœ„ ë ˆì´ì–´ ì°¸ì¡°ë¥¼ ì œê±°í•˜ê³  ì½œë°±ì´ë‚˜ DIë¥¼ ì‚¬ìš©í•œë‹¤.

ìˆ˜ì •ëœ ì „ì²´ ì½”ë“œë¥¼ ë°˜í™˜í•˜ë¼. ì½”ë“œ ì™¸ì˜ ì„¤ëª…ì€ ë°°ì œí•˜ê³  ì˜¤ì§ ì½”ë“œë§Œ ì¶œë ¥í•˜ë¼.
"""
        logger.info(f"ğŸ›¡ï¸ Healing architecture in {source_file}...")
        assigned_model = "gemini-1.5-pro" # ê³ ìˆ˜ì¤€ ì•„í‚¤í…ì²˜ íŒë‹¨ì€ PRO ì‚¬ìš©
        
        start_time = time.time()
        try:
            new_code = self.backend.generate(assigned_model, [{"role": "user", "content": prompt}])
            new_code = re.sub(r'```python\n|```', '', new_code).strip()
            
            # [Simulation Step]
            if not self.simulate_evolution(state, source_file, new_code):
                return {
                    "thought": "ì•„í‚¤í…ì²˜ ì¹˜ìœ  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê±´ê°•ë„ê°€ í•˜ë½í•˜ì—¬ ì¤‘ë‹¨ë¨.",
                    "messages": [("system", "âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê±´ê°•ë„ í•˜ë½ì´ ì˜ˆìƒë˜ì–´ ë¦¬íŒ©í† ë§ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")],
                    "next_node": "manager"
                }

            write_file(source_file, new_code)
            check_res = execute_shell(f"./scripts/pre_commit.sh --selective {source_file}")
            
            latency_ms = int((time.time() - start_time) * 1000)
            success = "Ready to commit" in check_res
            
            self.monitor.record_interaction("arch_healing", assigned_model, success, len(new_code)//4, latency_ms, metadata={"violation": reason})

            if success:
                return {
                    "thought": f"ì•„í‚¤í…ì²˜ ì¹˜ìœ  ì„±ê³µ: {source_file}ì˜ ë ˆì´ì–´ ìœ„ë°˜ í•´ì†Œ.",
                    "messages": [("ai", f"ğŸ›¡ï¸ **ì•„í‚¤í…ì²˜ ìê°€ ì¹˜ìœ  ì™„ë£Œ**\n- ëŒ€ìƒ: {source_file}\n- ê²°ê³¼: ë ˆì´ì–´ ìœ„ë°˜ ì‚¬í•­ì´ í•´ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")],
                    "next_node": "manager"
                }
            else:
                write_file(source_file, original_code)
                return {
                    "thought": f"ì•„í‚¤í…ì²˜ ì¹˜ìœ  ì‹¤íŒ¨: {check_res}", 
                    "messages": [("system", f"âš ï¸ ì•„í‚¤í…ì²˜ ì¹˜ìœ  ì‹¤íŒ¨: {source_file} ë¦¬íŒ©í† ë§ ì¤‘ ê²€ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë¡¤ë°±ë˜ì—ˆìŠµë‹ˆë‹¤.")],
                    "next_node": "manager"
                }
        except Exception as e:
            logger.error(f"Arch healing error: {e}")
            return {
                "thought": f"ì•„í‚¤í…ì²˜ ì¹˜ìœ  ì¤‘ ì˜¤ë¥˜: {e}", 
                "messages": [("system", f"âŒ ì•„í‚¤í…ì²˜ ì¹˜ìœ  ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")],
                "next_node": "manager"
            }

    def evolve_subsystem(self, state: GortexState) -> Dict[str, Any]:
        """ì„œë¸Œì‹œìŠ¤í…œ ì „ì²´ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ê°œì„  (ë‹¤ì¤‘ íŒŒì¼)"""
        candidates = self._get_radar_candidates()
        if not candidates:
            return {"thought": "ì§„í™” í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", "next_node": "manager"}

        target = next((c for c in candidates if c.get("effort") == "High"), candidates[0])
        target_file = target.get("target_file")
        
        # 1. ì˜í–¥ ë²”ìœ„ ë¶„ì„
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        impact = indexer.get_impact_radius(target_file)
        
        related_files = [target_file] + impact.get("direct", [])
        files_context = ""
        for f in related_files:
            if os.path.exists(f):
                files_context += f"\n--- FILE: {f} ---\n{read_file(f)}\n"

        prompt = f"""ë„ˆëŠ” Gortexì˜ ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ë‹¤. 
ë‹¤ìŒ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ê¸°ìˆ  '{target.get('tech')}'ë¥¼ ì¼ê´€ì„± ìˆê²Œ ì ìš©í•˜ë¼.

[ëŒ€ìƒ íŒŒì¼ë“¤]
{', '.join(related_files)}

[íŒŒì¼ ë‚´ìš©ë“¤]
{files_context}

ê° íŒŒì¼ë³„ ìˆ˜ì •ëœ ì „ì²´ ì½”ë“œë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ë¼:
{{
    "files": [
        {{ "path": "file1.py", "content": "..." }},
        {{ "path": "file2.py", "content": "..." }}
    ]
}}
"""
        logger.info(f"ğŸš€ Evolving subsystem: {target_file} and related {len(impact.get('direct', []))} files...")
        assigned_model = "gemini-1.5-pro"
        
        try:
            response_text = self.backend.generate(assigned_model, [{"role": "user", "content": prompt}], {"response_mime_type": "application/json"})
            import json
            res_data = json.loads(response_text)
            
            modified_files = []
            for f_data in res_data.get("files", []):
                path = f_data["path"]
                content = f_data["content"]
                write_file(path, content)
                modified_files.append(path)
            
            # ì¼ê´„ ê²€ì¦
            check_res = execute_shell(f"./scripts/pre_commit.sh --selective {' '.join(modified_files)}")
            if "Ready to commit" in check_res:
                return {
                    "thought": f"ì„œë¸Œì‹œìŠ¤í…œ ì§„í™” ì„±ê³µ: {len(modified_files)}ê°œ íŒŒì¼ ìˆ˜ì • ì™„ë£Œ.",
                    "messages": [("ai", f"ğŸ›ï¸ **ì„œë¸Œì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì§„í™” ì™„ë£Œ**\n- ëŒ€ìƒ: {target_file} ë° ê´€ë ¨ ëª¨ë“ˆ\n- ìˆ˜ì • íŒŒì¼: {', '.join(modified_files)}")],
                    "next_node": "analyst",
                    "awaiting_review": True,
                    "review_target": f"Subsystem ({target_file})"
                }
            else:
                # ë¡¤ë°± (ë‹¨ìˆœí™”: ì—¬ê¸°ì„  ìƒëµí•˜ë‚˜ ì‹¤ì œë¡œëŠ” ë°±ì—… ë³µêµ¬ í•„ìš”)
                return {"thought": "ì„œë¸Œì‹œìŠ¤í…œ ì§„í™” ê²€ì¦ ì‹¤íŒ¨", "next_node": "manager"}
        except Exception as e:
            return {"thought": f"ì„œë¸Œì‹œìŠ¤í…œ ì§„í™” ì¤‘ ì˜¤ë¥˜: {e}", "next_node": "manager"}

    def simulate_evolution(self, state: GortexState, target_file: str, new_code: str) -> bool:
        """ì½”ë“œ ìˆ˜ì •ì´ ì‹¤ì œë¡œ ê±´ê°•ë„ ì ìˆ˜ë¥¼ í–¥ìƒì‹œí‚¤ëŠ”ì§€ ê°€ìƒ ì‹œë®¬ë ˆì´ì…˜"""
        original_code = read_file(target_file)
        from gortex.utils.indexer import SynapticIndexer
        indexer = SynapticIndexer()
        
        # 1. ìˆ˜ì • ì „ ì ìˆ˜ ì¸¡ì •
        before_stats = indexer.calculate_health_score()
        
        # 2. ì„ì‹œ íŒŒì¼ ì“°ê¸° ë° ì¬ì¸ë±ì‹±
        write_file(target_file, new_code)
        indexer.scan_project()
        after_stats = indexer.calculate_health_score()
        
        # 3. ì ìˆ˜ ë¹„êµ
        improved = after_stats["score"] >= before_stats["score"]
        
        if not improved:
            logger.warning(f"ğŸ“‰ Simulation rejected: Health score would drop from {before_stats['score']} to {after_stats['score']}")
            write_file(target_file, original_code) # ì›ë³µ
            indexer.scan_project() # ì¸ë±ìŠ¤ ë³µêµ¬
        else:
            logger.info(f"ğŸ“ˆ Simulation passed: Health score {before_stats['score']} -> {after_stats['score']}")
            
        return improved

    def evolve_system(self, state: GortexState) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì§„í™” ë¡œì§ ì‹¤í–‰"""
        candidates = self._get_radar_candidates()
        if not candidates:
            return {"thought": "ë„ì…í•  ë§Œí•œ ì‹ ê·œ ê¸°ìˆ  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", "next_node": "manager"}

        # ê°€ì¥ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ í›„ë³´ ì„ íƒ (ì—¬ê¸°ì„  ì²« ë²ˆì§¸)
        target = candidates[0]
        target_file = target.get("target_file")
        tech_name = target.get("tech")
        reason = target.get("reason")

        if not target_file or not os.path.exists(target_file):
            return {"thought": f"ëŒ€ìƒ íŒŒì¼ {target_file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "next_node": "manager"}

        original_code = read_file(target_file)
        
        prompt = f"""ë„ˆëŠ” Gortexì˜ ì§„í™” ì„¤ê³„ìë‹¤. 
ë‹¤ìŒ ê¸°ìˆ /íŒ¨í„´ì„ í”„ë¡œì íŠ¸ì— ë„ì…í•˜ì—¬ ì½”ë“œë¥¼ ê°œì„ í•˜ë¼.

[ëŒ€ìƒ ê¸°ìˆ ] {tech_name}
[ë„ì… ì´ìœ ] {reason}
[ëŒ€ìƒ íŒŒì¼] {target_file}

[í˜„ì¬ ì½”ë“œ]
{original_code}

ê°œì„ ëœ ì „ì²´ ì½”ë“œë¥¼ ë°˜í™˜í•˜ë¼. ì½”ë“œ ì™¸ì˜ ì„¤ëª…ì€ ë°°ì œí•˜ê³  ì˜¤ì§ ì½”ë“œë§Œ ì¶œë ¥í•˜ë¼.
"""
        logger.info(f"ğŸ§¬ Evolving {target_file} with {tech_name}...")
        
        assigned_model = state.get("assigned_model", "gemini-1.5-pro") # ì§„í™”ëŠ” ì •êµí•´ì•¼ í•˜ë¯€ë¡œ PRO ê¶Œì¥
        
        start_time = time.time()
        try:
            new_code = self.backend.generate(assigned_model, [{"role": "user", "content": prompt}])
            new_code = re.sub(r'```python\n|```', '', new_code).strip()
            
            # [Simulation Step]
            if not self.simulate_evolution(state, target_file, new_code):
                return {
                    "thought": "ì‹œìŠ¤í…œ ì§„í™” ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê±´ê°•ë„ê°€ í•˜ë½í•˜ì—¬ ì¤‘ë‹¨ë¨.",
                    "messages": [("system", "âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê±´ê°•ë„ í•˜ë½ì´ ì˜ˆìƒë˜ì–´ ë¦¬íŒ©í† ë§ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")],
                    "next_node": "manager"
                }

            # 1. íŒŒì¼ ì“°ê¸°
            write_file(target_file, new_code)
            
            # 2. ê²€ì¦ (í…ŒìŠ¤íŠ¸ ì‹¤í–‰)
            check_res = execute_shell(f"./scripts/pre_commit.sh --selective {target_file}")
            
            latency_ms = int((time.time() - start_time) * 1000)
            success = "Ready to commit" in check_res
            
            # RLHF-lite: ì‹¤ì‹œê°„ í”¼ë“œë°± ë£¨í”„ ì ìš©
            self.monitor.record_interaction("evolution", assigned_model, success, len(new_code)//4, latency_ms, metadata={"tech": tech_name, "file": target_file})
            if not success:
                self.monitor.apply_immediate_feedback(assigned_model, False, weight=2.0) # ì§„í™” ì‹¤íŒ¨ëŠ” í° í˜ë„í‹°

            if success:
                logger.info(f"âœ… Evolution successful: {target_file} updated with {tech_name}")
                return {
                    "thought": f"ì‹œìŠ¤í…œ ì§„í™” ì„±ê³µ: {target_file}ì— {tech_name}ì„(ë¥¼) ì ìš©í–ˆìŠµë‹ˆë‹¤. êµì°¨ ë¦¬ë·°ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.",
                    "messages": [("ai", f"ğŸ§¬ **ì‹œìŠ¤í…œ ìê°€ ì§„í™” ì‹œë„ ì™„ë£Œ**\n- ê¸°ìˆ : {tech_name}\n- ëŒ€ìƒ: {target_file}\n- ìƒíƒœ: ê²€ì¦ í†µê³¼, êµì°¨ ë¦¬ë·° ì¤‘...")],
                    "next_node": "analyst", # Analystì—ê²Œ ë„˜ê²¨ ë¦¬ë·° ë°›ìŒ
                    "awaiting_review": True,
                    "review_target": target_file
                }
            else:
                # ì‹¤íŒ¨ ì‹œ ë¡¤ë°± (write_fileì˜ ë°±ì—… ê¸°ëŠ¥ì„ í™œìš©í•˜ê±°ë‚˜ ì§ì ‘ ë³µêµ¬)
                logger.warning(f"âŒ Evolution failed validation. Rolling back {target_file}...")
                write_file(target_file, original_code)
                return {
                    "thought": f"ì‹œìŠ¤í…œ ì§„í™” ì‹¤íŒ¨ (ê²€ì¦ ë‹¨ê³„): {check_res}",
                    "messages": [("system", f"âš ï¸ ì§„í™” ì‹œë„ ì‹¤íŒ¨: {tech_name} ì ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ë¡¤ë°±ë˜ì—ˆìŠµë‹ˆë‹¤.")],
                    "next_node": "manager"
                }

        except Exception as e:
            logger.error(f"Evolution process error: {e}")
            return {"thought": f"ì§„í™” ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", "next_node": "manager"}

import time

def evolution_node(state: GortexState) -> Dict[str, Any]:
    """Evolution ë…¸ë“œ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
    node = EvolutionNode()
    
    # 1. ì•„í‚¤í…ì²˜ ìœ„ë°˜ ì‚¬í•­ í™•ì¸ (Analyst ê¸°ëŠ¥ í™œìš©)
    from gortex.agents.analyst import AnalystAgent
    analyst = AnalystAgent()
    violations = analyst.audit_architecture()
    
    if violations:
        return node.heal_architecture(state, violations)
        
    # 2. ì„œë¸Œì‹œìŠ¤í…œ ë‹¨ìœ„ ì§„í™” (High Effort í›„ë³´ê°€ ìˆëŠ” ê²½ìš°)
    candidates = node._get_radar_candidates()
    if any(c.get("effort") == "High" for c in candidates):
        return node.evolve_subsystem(state)

    # 3. ì¼ë°˜ ì‹œìŠ¤í…œ ì§„í™”
    return node.evolve_system(state)
