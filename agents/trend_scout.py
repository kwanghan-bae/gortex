import json
import os
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from gortex.core.auth import GortexAuth
from gortex.core.state import GortexState
from gortex.agents.researcher import ResearcherAgent
from gortex.utils.vector_store import LongTermMemory

logger = logging.getLogger("GortexTrendScout")

class TrendScoutAgent:
    """
    ì¸í„°ë„· íŠ¸ë Œë“œ(ì‹ ê·œ LLM, ì—ì´ì „íŠ¸ ê¸°ë²•)ë¥¼ ê²€ìƒ‰í•˜ê³  tech_radar.jsonì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì—ì´ì „íŠ¸.
    """
    def __init__(self, radar_path: str = "tech_radar.json"):
        self.radar_path = radar_path
        self.auth = GortexAuth()
        self.researcher = ResearcherAgent()
        self.ltm = LongTermMemory()
        self.radar_data = self._load_radar()

    def _load_radar(self) -> Dict[str, Any]:
        if os.path.exists(self.radar_path):
            try:
                with open(self.radar_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load tech radar: {e}")
                return {}
        return {}

    def _save_radar(self):
        try:
            with open(self.radar_path, 'w', encoding='utf-8') as f:
                json.dump(self.radar_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save tech radar: {e}")

    async def check_vulnerabilities(self) -> List[str]:
        """requirements.txtë¥¼ ë¶„ì„í•˜ì—¬ ì•Œë ¤ì§„ ë³´ì•ˆ ì·¨ì•½ì  ì ê²€"""
        req_path = "requirements.txt"
        if not os.path.exists(req_path):
            return ["requirements.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë³´ì•ˆ ì ê²€ì„ ê±´ë„ˆëœë‹ˆë‹¤."]

        logger.info("ğŸ” Scanning for security vulnerabilities in dependencies...")
        try:
            with open(req_path, "r", encoding='utf-8') as f:
                packages = [line.strip() for line in f if line.strip() and not line.startswith("#")]
            
            if not packages:
                return ["ì ê²€í•  íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."]

            findings = []
            for pkg in packages[:10]: # í† í° ë° ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ ìƒìœ„ 10ê°œ íŒ¨í‚¤ì§€ ìš°ì„  ì ê²€
                query = f"security vulnerability {pkg} python cve 2024 2025"
                result = await self.researcher.search_and_summarize(query)
                findings.append(f"Package: {pkg}\n{result}")

            analysis_prompt = f"""
            ë‹¤ìŒì€ í”„ë¡œì íŠ¸ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ë“¤ì— ëŒ€í•œ ë³´ì•ˆ ê²€ìƒ‰ ê²°ê³¼ì´ë‹¤.
            ì‹¬ê°í•œ ì·¨ì•½ì (Critical/High)ì´ ë°œê²¬ë˜ì—ˆëŠ”ì§€ ë¶„ì„í•˜ê³ , ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡ì„ ì œì•ˆí•˜ë¼.
            
            [Search Results]
            {""}
            
            ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì„ ë”°ë¼ë¼:
            {{
                "vulnerabilities_found": true/false,
                "risky_packages": [{{ "name": "íŒ¨í‚¤ì§€ëª…", "cve": "CVE ID", "severity": "High/Medium", "recommendation": "ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë“±" }}]
            }}
            """
            
            response = self.auth.generate("gemini-1.5-flash", [("user", analysis_prompt)], None)
            res_data = json.loads(response.text)
            
            notifications = []
            if res_data.get("vulnerabilities_found"):
                for p in res_data.get("risky_packages", []):
                    msg = f"âš ï¸ [ë³´ì•ˆ ìœ„í—˜] {p['name']}: {p['recommendation']} ({p['severity']})"
                    notifications.append(msg)
                    # tech_radarì— ë³´ì•ˆ ì •ë³´ ê¸°ë¡
                    if "security_alerts" not in self.radar_data:
                        self.radar_data["security_alerts"] = []
                    self.radar_data["security_alerts"].append({
                        "package": p["name"],
                        "detected_at": datetime.now().isoformat(),
                        "details": p
                    })
                self._save_radar()
                return notifications
            return ["âœ… ì£¼ìš” íŒ¨í‚¤ì§€ ë³´ì•ˆ ì ê²€ ê²°ê³¼, ì•Œë ¤ì§„ ì‹¬ê°í•œ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."]
            
        except Exception as e:
            logger.error(f"Vulnerability scan failed: {e}")
            return [f"ë³´ì•ˆ ì ê²€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"]

    def should_scan(self, interval_hours: int = 24) -> bool:
        """ë§ˆì§€ë§‰ ìŠ¤ìº”ìœ¼ë¡œë¶€í„° ì§€ì •ëœ ì‹œê°„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸"""
        last_scan_str = self.radar_data.get("last_scan")
        if not last_scan_str:
            return True
        
        try:
            last_scan = datetime.fromisoformat(last_scan_str)
            return datetime.now() > last_scan + timedelta(hours=interval_hours)
        except ValueError:
            return True

    async def scan_trends(self) -> List[str]:
        """ì›¹ ê²€ìƒ‰ì„ í†µí•´ íŠ¸ë Œë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„"""
        logger.info("ğŸš€ Scouting for new tech trends and LLM models...")
        
        # 1. ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •
        queries = [
            "latest free LLM API 2025",
            "Gemini API updates and new models",
            "new autonomous agent patterns and best practices python"
        ]
        
        findings = []
        for q in queries:
            result = await self.researcher.search_and_summarize(q)
            findings.append(result)

        # 2. ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½ (LLM)
        analysis_prompt = f"""
        ë‹¤ìŒì€ ìµœì‹  AI íŠ¸ë Œë“œ ë° ëª¨ë¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë“¤ì´ë‹¤.
        Gortex ì‹œìŠ¤í…œì„ ê°•í™”í•  ìˆ˜ ìˆëŠ” ì‹ ê·œ ëª¨ë¸ ì†Œì‹ì´ë‚˜ ì—ì´ì „íŠ¸ ì„¤ê³„ ê¸°ë²•ì´ ìˆëŠ”ì§€ ë¶„ì„í•˜ë¼.
        
        [Search Results]
        {"".join(findings)}
        
        ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'models'ì™€ 'patterns' ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ë¼.
        {{
            "models": [{{ "name": "ëª¨ë¸ëª…", "status": "new/updated", "note": "ì„¤ëª…" }}],
            "patterns": [{{ "topic": "ì£¼ì œ", "summary": "ì„¤ëª…" }}]
        }}
        """
        
        response = self.auth.generate("gemini-1.5-flash", [("user", analysis_prompt)], None)
        
        try:
            # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ (ì •ê·œì‹ ë˜ëŠ” ê°„ë‹¨í•œ íŒŒì‹±)
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                extracted = json.loads(json_match.group())
                
                # 3. tech_radar.json ì—…ë°ì´íŠ¸
                self.radar_data["last_scan"] = datetime.now().isoformat()
                self.radar_data["models"] = extracted.get("models", [])
                self.radar_data["patterns"] = extracted.get("patterns", [])
                self._save_radar()
                
                # [Knowledge Base Integration] ìµœì‹  íŠ¸ë Œë“œë¥¼ ì¥ê¸° ê¸°ì–µ ì €ì¥ì†Œì— í†µí•©
                for m in extracted.get("models", []):
                    knowledge_text = f"ìµœì‹  ëª¨ë¸ ì •ë³´: {m['name']}ëŠ” {m['status']} ìƒíƒœì´ë©°, íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤: {m.get('note')}"
                    self.ltm.memorize(knowledge_text, {"source": "TrendScout", "type": "model", "topic": m['name']})
                
                for p in extracted.get("patterns", []):
                    knowledge_text = f"ì‹ ê·œ ì—ì´ì „íŠ¸ íŒ¨í„´: {p['topic']} - {p.get('summary')}"
                    self.ltm.memorize(knowledge_text, {"source": "TrendScout", "type": "pattern", "topic": p['topic']})

                # ì•Œë¦¼ìš© ìš”ì•½ ë©”ì‹œì§€ ìƒì„±
                notifications = []
                for m in extracted.get("models", []):
                    notifications.append(f"âœ¨ ì‹ ê·œ ëª¨ë¸ ë°œê²¬: {m['name']} ({m['status']})")
                return notifications
        except Exception as e:
            logger.error(f"Trend analysis parsing failed: {e}")
            
        return ["íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìœ¼ë‚˜, ìŠ¤ìº”ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."]

    async def analyze_adoption_opportunity(self, file_list: List[str]) -> List[str]:
        """ì‹ ê¸°ìˆ  ë„ì… ê¸°íšŒ ë¶„ì„"""
        if not self.radar_data.get("models") and not self.radar_data.get("patterns"):
            return []
            
        logger.info("ğŸ•µï¸ Analyzing code adoption opportunities...")
        
        # í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡° ìš”ì•½ (í† í° ì ˆì•½)
        file_summary = "\n".join(file_list[:50]) # ìµœëŒ€ 50ê°œ íŒŒì¼ëª…ë§Œ
        
        radar_summary = json.dumps({
            "models": self.radar_data.get("models", []),
            "patterns": self.radar_data.get("patterns", [])
        }, ensure_ascii=False)
        
        prompt = f"""
        ë‹¤ìŒì€ í˜„ì¬ í”„ë¡œì íŠ¸ì˜ íŒŒì¼ êµ¬ì¡°ì™€ Tech Radarì—ì„œ ë°œê²¬ëœ ì‹ ê¸°ìˆ  ëª©ë¡ì´ë‹¤.
        í”„ë¡œì íŠ¸ì— ë„ì…í•  ë§Œí•œ ê¸°ìˆ ì´ë‚˜ íŒ¨í„´ì´ ìˆëŠ”ì§€ ë¶„ì„í•˜ê³ , ì ìš© ëŒ€ìƒ íŒŒì¼ê³¼ ì´ìœ ë¥¼ ì œì•ˆí•˜ë¼.
        
        [Project Files]
        {file_summary}
        
        [Tech Radar]
        {radar_summary}
        
        ê²°ê³¼ëŠ” JSONìœ¼ë¡œ:
        {{
            "candidates": [
                {{ "tech": "ì´ë¦„", "target_file": "ê²½ë¡œ", "reason": "ì´ìœ ", "effort": "High/Medium/Low" }}
            ]
        }}
        """
        try:
            response = self.auth.generate("gemini-1.5-flash", [("user", prompt)], {"response_mime_type": "application/json"})
            res_data = json.loads(response.text)
            candidates = res_data.get("candidates", [])
            
            if candidates:
                self.radar_data["adoption_candidates"] = candidates
                self._save_radar()
                return [f"ğŸ’¡ ê¸°ìˆ  ë„ì… ì œì•ˆ: {c['tech']} -> {c['target_file']} ({c['reason']})" for c in candidates]
        except Exception as e:
            logger.error(f"Adoption analysis failed: {e}")
            
        return []

import asyncio
import re

def trend_scout_node(state: GortexState) -> Dict[str, Any]:
    """TrendScout ë…¸ë“œ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
    scout = TrendScoutAgent()
    
    interval = int(os.getenv("TREND_SCAN_INTERVAL_HOURS", "24"))
    
    if scout.should_scan(interval):
        # ë¹„ë™ê¸° ì‹¤í–‰ (Researcherì™€ ë™ì¼í•œ íŒ¨í„´)
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        file_list = list(state.get("file_cache", {}).keys())

        if loop.is_running():
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                # íŠ¸ë Œë“œ ìŠ¤ìº”ê³¼ ë³´ì•ˆ ì ê²€ ë³‘ë ¬ ì‹¤í–‰
                f1 = executor.submit(lambda: asyncio.run(scout.scan_trends()))
                f2 = executor.submit(lambda: asyncio.run(scout.check_vulnerabilities()))
                notifications = f1.result() + f2.result()
                
                # ë„ì… ê¸°íšŒ ë¶„ì„ì€ ìœ„ ê²°ê³¼ ë°˜ì˜ í›„ ìˆœì°¨ ì‹¤í–‰
                f3 = executor.submit(lambda: asyncio.run(scout.analyze_adoption_opportunity(file_list)))
                notifications += f3.result()
        else:
            n1 = loop.run_until_complete(scout.scan_trends())
            n2 = loop.run_until_complete(scout.check_vulnerabilities())
            n3 = loop.run_until_complete(scout.analyze_adoption_opportunity(file_list))
            notifications = n1 + n2 + n3
            
        return {
            "messages": [("ai", "\n".join(notifications))],
            "next_node": "manager"
        }
    
    return {
        "next_node": "manager"
    }
